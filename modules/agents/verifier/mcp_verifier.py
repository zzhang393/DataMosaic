"""åŸºäºMCP (Model Context Protocol) çš„éªŒè¯å™¨å®ç°"""
import logging
import asyncio
from typing import List, Dict, Any, Optional
from ..mcp.base import MCPClient
from ..mcp.data_quality_server import DataQualityMCPServer
from ...memory import TableSnapshot, Violation, ConstraintType, ViolationSeverity, SuggestedFix
from ...core.ids import IdGenerator
from ...core.io import IOManager
from enum import Enum

from .mcp_router import MCPRouter


class ViolationCategory(Enum):
    """è¿è§„åˆ†ç±» - åŒºåˆ†å¤„ç†æ–¹å¼"""
    TOOL_FIXABLE = "tool_fixable"  # å¯ä»¥é€šè¿‡å·¥å…·ä¿®å¤
    REQUIRES_REEXTRACTION = "requires_reextraction"  # éœ€è¦é‡æ–°ä»æ–‡æ¡£æå–


class MCPBasedVerifier:
    """åŸºäºModel Context Protocolçš„éªŒè¯å™¨"""
    
    def __init__(self, enable_smart_routing: bool = True):
        """
        åˆå§‹åŒ–MCPéªŒè¯å™¨
        
        Args:
            enable_smart_routing: æ˜¯å¦å¯ç”¨æ™ºèƒ½MCPè·¯ç”±ï¼ˆé»˜è®¤Trueï¼‰
        """
        self.verifier_id = "MCPBasedVerifier.v2.1"
        self.logger = logging.getLogger('doc2db.verifier')
        
        self.mcp_client = MCPClient("doc2db-verifier-client")
        self.data_quality_server = DataQualityMCPServer()
        
        self.mcp_client.connect_server(self.data_quality_server)
        
        self.constraint_type_mapping = {
            ConstraintType.TYPE.value: "TYPE",
            ConstraintType.VALUE.value: "VALUE",
            ConstraintType.STRUCTURE.value: "STRUCTURE",
            ConstraintType.LOGIC.value: "LOGIC",
            ConstraintType.TEMPORAL.value: "TEMPORAL",
            ConstraintType.FORMAT.value: "FORMAT",
            ConstraintType.REFERENCE.value: "REFERENCE",
            ConstraintType.AGGREGATION.value: "AGGREGATION",
        }
        
        self.enable_smart_routing = enable_smart_routing
        self.router = MCPRouter() if enable_smart_routing else None
        
        routing_status = "å¯ç”¨æ™ºèƒ½è·¯ç”±" if enable_smart_routing else "ä½¿ç”¨ä¼ ç»Ÿé¡ºåºéªŒè¯"
        self.logger.info(f"åˆå§‹åŒ–MCPéªŒè¯å™¨ v2.1ï¼Œæ”¯æŒ {len(self.constraint_type_mapping)} ç§çº¦æŸç±»å‹ï¼Œ{routing_status}")
    
    def verify_table(self, table, context=None) -> List[Violation]:
        """
        ç›´æ¥éªŒè¯Tableå¯¹è±¡ (æ–°æ¥å£)
        
        Args:
            table: Tableå¯¹è±¡
            context: å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            å‘ç°çš„è¿è§„åˆ—è¡¨
        """
        try:
            return table.verify(self, context)
        except Exception as e:
            self.logger.error(f"éªŒè¯Tableå¯¹è±¡å¤±è´¥: {e}")
            return []
    
    def verify(self, snapshot: TableSnapshot, schema: Dict[str, Any], 
               table_name: str, context=None) -> List[Violation]:
        """ä½¿ç”¨MCPéªŒè¯è¡¨æ ¼å¿«ç…§ä¸­çš„æ•°æ®"""
        self.logger.info(f"å¼€å§‹MCPéªŒè¯è¡¨æ ¼ {table_name}")
        
        if isinstance(schema, str):
            try:
                import json
                schema = json.loads(schema)
                self.logger.info(f"Schemaä»å­—ç¬¦ä¸²è§£æä¸ºå­—å…¸")
            except Exception as e:
                self.logger.error(f"Schemaè§£æå¤±è´¥: {e}")
                return []
        elif not isinstance(schema, dict):
            self.logger.error(f"Schemaç±»å‹ä¸æ­£ç¡®: {type(schema)}")
            return []
        
        try:
            try:
                loop = asyncio.get_running_loop()
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self._async_verify(snapshot, schema, table_name, context))
                    return future.result()
            except RuntimeError:
                return asyncio.run(self._async_verify(snapshot, schema, table_name, context))
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥éªŒè¯å¤±è´¥: {e}")
            return []
    
    async def _async_verify(self, snapshot: TableSnapshot, schema: Dict[str, Any], 
                           table_name: str, context=None) -> List[Violation]:
        """å¼‚æ­¥éªŒè¯å®ç°"""
        all_violations = []
        routing_info = None
        
        constraint_types_to_check = list(self.constraint_type_mapping.values())  # é»˜è®¤å…¨éƒ¨
        
        if self.enable_smart_routing and self.router:
            try:
                selected_mcp_names, routing_info = self.router.select_mcps(snapshot, schema, table_name)
                
                mcp_to_constraint_map = {
                    "TypeMCP": "TYPE",
                    "ValueMCP": "VALUE", 
                    "StructureMCP": "STRUCTURE",
                    "FormatMCP": "FORMAT",
                    "LogicMCP": "LOGIC",
                    "TemporalMCP": "TEMPORAL",
                    "ReferenceMCP": "REFERENCE",
                    "AggregationMCP": "AGGREGATION"
                }
                
                constraint_types_to_check = [
                    mcp_to_constraint_map[mcp_name] 
                    for mcp_name in selected_mcp_names 
                    if mcp_name in mcp_to_constraint_map
                ]
                
                self.logger.info(f"ğŸ§  æ™ºèƒ½è·¯ç”±é€‰æ‹©äº† {len(constraint_types_to_check)} ä¸ªçº¦æŸç±»å‹: {constraint_types_to_check}")
                
            except Exception as e:
                self.logger.warning(f"æ™ºèƒ½è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨å…¨éƒ¨MCP: {e}")
                constraint_types_to_check = list(self.constraint_type_mapping.values())
        
        verification_data = {
            "table_name": table_name,
            "rows": [
                {
                    "tuple_id": row.tuple_id,
                    "cells": {
                        attr: cell.value
                        for attr, cell in row.cells.items()
                    }
                }
                for row in snapshot.rows
            ]
        }
        
        mcp_success = False
        try:
            response = await self.mcp_client.call_tool(
                server_name="doc2db-data-quality",
                tool_name="batch_verify",
                arguments={
                    "snapshots": [verification_data],
                    "schema": schema,
                    "constraint_types": constraint_types_to_check  #  ä½¿ç”¨æ™ºèƒ½è·¯ç”±é€‰æ‹©çš„çº¦æŸç±»å‹
                }
            )
            
            if response and response.get("status") == "success":
                mcp_success = True
                total_violations = response.get("total_violations", 0)
                self.logger.info(f"MCPæ‰¹é‡éªŒè¯æˆåŠŸï¼Œå¤„ç†äº† {response.get('processed_snapshots', 0)} ä¸ªå¿«ç…§ï¼Œå‘ç° {total_violations} ä¸ªè¿è§„")
                
                violations_data = response.get("violations", [])
                
                routing_info = {
                    'violations_by_mcp': response.get("violations_by_type", {}),
                    'constraint_types_checked': response.get("constraint_types", []),
                    'total_violations': total_violations
                }
                
                for violation_data in violations_data:
                    all_violations.append(self._convert_mcp_violation_to_object(violation_data, table_name))
                    
            elif response and response.get("status") == "error":
                self.logger.error(f"MCPéªŒè¯è¿”å›é”™è¯¯: {response.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
        except Exception as e:
            self.logger.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
        
        if not mcp_success:
            self.logger.info("MCPæœåŠ¡å™¨éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨å…·ä½“MCPç»„ä»¶ä½œä¸ºåå¤‡")
            all_violations = await self._fallback_verify(snapshot, schema, table_name, context)
        
        unique_violations = {}
        for violation in all_violations:
            if hasattr(violation, 'id') and violation.id not in unique_violations:
                unique_violations[violation.id] = violation
        
        final_violations = list(unique_violations.values())
        
        categorized_violations = self._categorize_violations(final_violations, table_name)
        
        for violation, category in zip(final_violations, categorized_violations):
            if category == ViolationCategory.REQUIRES_REEXTRACTION:
                violation.processing_category = "requires_reextraction"
            elif category == ViolationCategory.TOOL_FIXABLE:
                violation.processing_category = "tool_fixable"
            else:
                violation.processing_category = str(category)
        
        verification_info = f"MCPéªŒè¯å®Œæˆï¼Œå‘ç° {len(final_violations)} ä¸ªè¿è§„"
        self.logger.info(verification_info)
        
        if hasattr(context, 'step_outputs'):
            error_violations = [v for v in final_violations if getattr(v, 'severity', 'warn') == 'error']
            warn_violations = [v for v in final_violations if getattr(v, 'severity', 'warn') == 'warn']
            
            details = {
                'table_name': table_name,
                'violations_found': len(final_violations),
                'error_violations': len(error_violations),
                'warn_violations': len(warn_violations),
                'mcp_server': self.data_quality_server.name,
                'constraint_types_checked': constraint_types_to_check,
                'violations_by_type': {},
                'output_text': f"âœ… MCPéªŒè¯å®Œæˆï¼Œå‘ç° {len(final_violations)} ä¸ªè´¨é‡é—®é¢˜"
            }
            
            for violation in final_violations:
                constraint_type = getattr(violation, 'constraint_type', 'unknown')
                if constraint_type not in details['violations_by_type']:
                    details['violations_by_type'][constraint_type] = 0
                details['violations_by_type'][constraint_type] += 1
            
            if routing_info and 'violations_by_mcp' in routing_info:
                details['mcp_results'] = []
                mcp_name_map = {
                    'TYPE': 'TypeMCP',
                    'VALUE': 'ValueMCP',
                    'STRUCTURE': 'StructureMCP',
                    'FORMAT': 'FormatMCP',
                    'LOGIC': 'LogicMCP',
                    'TEMPORAL': 'TemporalMCP',
                    'REFERENCE': 'ReferenceMCP',
                    'AGGREGATION': 'AggregationMCP'
                }
                
                for constraint_type, violation_count in routing_info['violations_by_mcp'].items():
                    mcp_name = mcp_name_map.get(constraint_type, f"{constraint_type}MCP")
                    details['mcp_results'].append({
                        'mcp_name': mcp_name,
                        'constraint_type': constraint_type,
                        'violations_found': violation_count,
                        'status': 'found_issues' if violation_count > 0 else 'passed'
                    })
                
                for constraint_type in constraint_types_to_check:
                    if constraint_type not in routing_info['violations_by_mcp']:
                        mcp_name = mcp_name_map.get(constraint_type, f"{constraint_type}MCP")
                        details['mcp_results'].append({
                            'mcp_name': mcp_name,
                            'constraint_type': constraint_type,
                            'violations_found': 0,
                            'status': 'passed'
                        })
                
                details['smart_routing_enabled'] = True
                details['mcps_skipped'] = routing_info.get('optional_mcps_skipped', [])
            else:
                details['smart_routing_enabled'] = False
            
            context.step_outputs.append({
                'step': f'verifier_{table_name}_completed',
                'step_name': f'verifier_{table_name}_completed',
                'status': 'completed', 
                'description': f"è¡¨ {table_name} éªŒè¯å®Œæˆï¼Œå‘ç° {len(final_violations)} ä¸ªè´¨é‡é—®é¢˜",
                'details': details,
                'timestamp': IOManager.get_timestamp()
            })
            
        
        return final_violations
    
    async def _fallback_verify(self, snapshot: TableSnapshot, schema: Dict[str, Any], 
                              table_name: str, context=None) -> List[Violation]:
        """å›é€€éªŒè¯æ–¹æ³• - ç›´æ¥ä½¿ç”¨å…·ä½“çš„MCPç»„ä»¶"""
        self.logger.info("ç›´æ¥ä½¿ç”¨å…·ä½“MCPç»„ä»¶è¿›è¡ŒéªŒè¯")
        
        violations = []
        routing_info = None
        
        try:
            from ..mcp import (
                TypeMCP, ValueMCP, StructureMCP, FormatMCP,
                LogicMCP, TemporalMCP, ReferenceMCP, AggregationMCP
            )
            
            all_mcps = {
                "TypeMCP": TypeMCP(),
                "ValueMCP": ValueMCP(), 
                "StructureMCP": StructureMCP(),
                "FormatMCP": FormatMCP(),
                "LogicMCP": LogicMCP(),
                "TemporalMCP": TemporalMCP(),
                "ReferenceMCP": ReferenceMCP(),
                "AggregationMCP": AggregationMCP()
            }
            
            if self.enable_smart_routing and self.router:
                selected_mcp_names, routing_info = self.router.select_mcps(snapshot, schema, table_name)
                
                mcps_to_run = [(name, all_mcps[name]) for name in selected_mcp_names if name in all_mcps]
            else:
                mcps_to_run = list(all_mcps.items())
            
            for mcp_name, mcp in mcps_to_run:
                try:
                    self.logger.info(f"ä½¿ç”¨ {mcp_name} éªŒè¯...")
                    
                    mcp_violations = mcp.verify(snapshot, schema, table_name)
                    violations.extend(mcp_violations)
                except Exception as e:
                    self.logger.error(f"{mcp_name} éªŒè¯å¤±è´¥: {e}")
            
        except ImportError as e:
            self.logger.error(f"æ— æ³•å¯¼å…¥MCPç»„ä»¶: {e}")
            violations = []
        
        if violations:
            categorized_violations = self._categorize_violations(violations, table_name)
            
            for violation, category in zip(violations, categorized_violations):
                violation.processing_category = category
        
        self.logger.info(f"MCPéªŒè¯å®Œæˆï¼Œå…±å‘ç° {len(violations)} ä¸ªè¿è§„")
        
        if routing_info and hasattr(context, 'step_outputs'):
            context.step_outputs.append({
                'step': f'mcp_routing_{table_name}',
                'status': 'completed',
                'description': f'è¡¨ {table_name} çš„MCPè·¯ç”±å†³ç­–',
                'details': routing_info,
                'timestamp': IOManager.get_timestamp()
            })
        
        return violations
    
    def _convert_mcp_violation_to_object(self, violation_data: Dict[str, Any], table_name: str) -> Violation:
        """å°†MCPè¿”å›çš„è¿è§„æ•°æ®è½¬æ¢ä¸ºViolationå¯¹è±¡"""
        
        tuple_id = violation_data.get('tuple_id', '')
        attr = violation_data.get('attr', '')
        constraint_type = violation_data.get('constraint_type', 'UNKNOWN')
        
        violation_id = IdGenerator.generate_violation_id(
            table_name, tuple_id, attr, constraint_type
        )
        
        suggested_fix = None
        if violation_data.get('suggested_fix'):
            suggested_fix = SuggestedFix(
                value=violation_data['suggested_fix']
            )
        
        severity = violation_data.get('severity', ViolationSeverity.WARN.value)
        if severity not in [s.value for s in ViolationSeverity]:
            severity = ViolationSeverity.WARN.value
        
        violation = Violation(
            id=violation_id,
            table=table_name,
            tuple_id=tuple_id,
            attr=attr,
            constraint_type=constraint_type,
            description=violation_data.get('description', f"MCPæ£€æµ‹åˆ°çš„{constraint_type}çº¦æŸè¿è§„"),
            severity=severity,
            suggested_fix=suggested_fix,
            detector_id=self.verifier_id,
            timestamp=""
        )
        
        if 'current_value' in violation_data:
            violation.current_value = violation_data['current_value']
        
        return violation
    
    def get_supported_constraints(self) -> List[str]:
        """è·å–æ”¯æŒçš„çº¦æŸç±»å‹åˆ—è¡¨"""
        return list(self.constraint_type_mapping.keys())
    
    async def get_mcp_resources(self) -> List[Dict[str, Any]]:
        """è·å–MCPæœåŠ¡å™¨èµ„æº"""
        try:
            resources = await self.mcp_client.list_server_resources("doc2db-data-quality")
            return [resource.to_dict() for resource in resources]
        except Exception as e:
            self.logger.error(f"è·å–MCPèµ„æºå¤±è´¥: {e}")
            return []
    
    async def get_constraint_types_info(self) -> Optional[Dict[str, Any]]:
        """è·å–çº¦æŸç±»å‹ä¿¡æ¯"""
        try:
            resource_data = await self.mcp_client.get_resource("doc2db-data-quality", "constraint://types")
            return resource_data
        except Exception as e:
            self.logger.error(f"è·å–çº¦æŸç±»å‹ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _categorize_violations(self, violations: List[Violation], table_name: str) -> List[ViolationCategory]:
        """å¯¹è¿è§„è¿›è¡Œåˆ†ç±»ï¼ŒåŒºåˆ†å¤„ç†æ–¹å¼"""
        categories = []
        
        for violation in violations:
            category = self._determine_violation_category(violation)
            categories.append(category)
            
            category_name = "å·¥å…·ä¿®å¤" if category == ViolationCategory.TOOL_FIXABLE else "é‡æ–°æå–"
        
        tool_fixable_count = sum(1 for cat in categories if cat == ViolationCategory.TOOL_FIXABLE)
        reextraction_count = sum(1 for cat in categories if cat == ViolationCategory.REQUIRES_REEXTRACTION)
        
        
        return categories
    
    def _determine_violation_category(self, violation: Violation) -> ViolationCategory:
        """åˆ¤æ–­å•ä¸ªè¿è§„çš„å¤„ç†æ–¹å¼"""
        constraint_type = violation.constraint_type
        description = violation.description.lower() if violation.description else ""
        
        is_missing_value = any(keyword in description for keyword in ["ç¼ºå¤±", "missing", "ç©ºå€¼", "null"])
        if is_missing_value:
            pass  # Auto-fixed empty block
        
        if hasattr(violation, 'business_rule_id') and violation.business_rule_id:
            if violation.business_rule_id in ['agg_1', 'agg_2']:
                return ViolationCategory.REQUIRES_REEXTRACTION
        
        
        tool_fixable_conditions = [
            constraint_type == ConstraintType.TYPE.value and any(keyword in description for keyword in [
                "ç±»å‹", "type", "è½¬æ¢", "convert", "æ ¼å¼", "format"
            ]),
            
            constraint_type == ConstraintType.FORMAT.value,
            
            constraint_type == ConstraintType.VALUE.value and any(keyword in description for keyword in [
                "èŒƒå›´", "range", "é•¿åº¦", "length", "å¤§å°", "size", "è¶…å‡º", "exceed"
            ]),
            
            hasattr(violation, 'suggested_fix') and violation.suggested_fix is not None,
            
            constraint_type == ConstraintType.LOGIC.value and any(format_keyword in description for format_keyword in [
                "æ ¼å¼", "å•ä½", "æ ‡è¯†", "format", "unit", "identifier", "ç»Ÿä¸€", "standardize", "ç©ºæ ¼", "spacing"
            ]),
            
            constraint_type == ConstraintType.STRUCTURE.value and any(keyword in description for keyword in [
                "é‡å¤", "duplicate", "å†²çª", "conflict", "å”¯ä¸€", "unique"
            ]),
        ]
        
        reextraction_conditions = [
            constraint_type == ConstraintType.STRUCTURE.value and not any(keyword in description for keyword in [
                "é‡å¤", "duplicate", "å†²çª", "conflict", "å”¯ä¸€", "unique"
            ]),
            
            constraint_type == ConstraintType.LOGIC.value and any(keyword in description for keyword in [
                "å…³ç³»", "relation", "ä¾èµ–", "dependency", "ä¸€è‡´æ€§", "consistency", "è®¡ç®—é”™è¯¯", "æ•°å€¼å¼‚å¸¸", "ä¸šåŠ¡è§„åˆ™å†²çª"
            ]) and not any(format_keyword in description for format_keyword in [
                "æ ¼å¼", "å•ä½", "æ ‡è¯†", "format", "unit", "identifier", "ç»Ÿä¸€", "standardize"
            ]),
            
            constraint_type == ConstraintType.REFERENCE.value,
            
            "ç¼ºå¤±" in description or "missing" in description or "ç©ºå€¼" in description or "null" in description,
            
            violation.severity == ViolationSeverity.ERROR.value and not (
                hasattr(violation, 'suggested_fix') and violation.suggested_fix is not None
            ),
        ]
        
        if any(tool_fixable_conditions):
            category = ViolationCategory.TOOL_FIXABLE
            if is_missing_value:
                pass  # Auto-fixed empty block
            return category
        elif any(reextraction_conditions):
            category = ViolationCategory.REQUIRES_REEXTRACTION
            if is_missing_value:
                pass  # Auto-fixed empty block
            return category
        else:
            if violation.severity == ViolationSeverity.ERROR.value:
                category = ViolationCategory.REQUIRES_REEXTRACTION  # ä¸¥é‡é”™è¯¯å€¾å‘äºé‡æ–°æå–
                if is_missing_value:
                    pass  # Auto-fixed empty block
                return category
            else:
                category = ViolationCategory.TOOL_FIXABLE
                if is_missing_value:
                    pass  # Auto-fixed empty block
                return category
    
    def verify_multi_table(self, all_snapshots: Dict[str, TableSnapshot], 
                          schema: Dict[str, Any], context=None) -> List[Violation]:
        """
        å¤šè¡¨éªŒè¯ - éªŒè¯è¡¨é—´å…³ç³»å’Œè·¨è¡¨çº¦æŸ
        
        Args:
            all_snapshots: æ‰€æœ‰è¡¨çš„å¿«ç…§å­—å…¸ {table_name: TableSnapshot}
            schema: å®Œæ•´çš„schemaå®šä¹‰ï¼ˆåŒ…å«relationsï¼‰
            context: å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            è·¨è¡¨è¿è§„åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹å¤šè¡¨éªŒè¯ï¼Œå…± {len(all_snapshots)} ä¸ªè¡¨")
        
        try:
            try:
                loop = asyncio.get_running_loop()
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self._async_verify_multi_table(all_snapshots, schema, context))
                    return future.result()
            except RuntimeError:
                return asyncio.run(self._async_verify_multi_table(all_snapshots, schema, context))
        except Exception as e:
            self.logger.error(f"å¤šè¡¨éªŒè¯å¤±è´¥: {e}")
            return []
    
    async def _async_verify_multi_table(self, all_snapshots: Dict[str, TableSnapshot],
                                       schema: Dict[str, Any], context=None) -> List[Violation]:
        """å¼‚æ­¥å¤šè¡¨éªŒè¯å®ç°"""
        all_violations = []
        
        relation_verification_details = []
        same_field_verification_details = []
        
        relations = schema.get('relations', [])
        if relations:
            self.logger.info(f"ğŸ“‹ [å¤šè¡¨éªŒè¯-Relations] å‘ç° {len(relations)} ä¸ªrelationå®šä¹‰")
            
            for rel_idx, relation in enumerate(relations):
                if not isinstance(relation, dict):
                    continue
                
                rel_type = relation.get('type', 'unknown')
                from_def = relation.get('from', {})
                to_def = relation.get('to', {})
                
                from_table = from_def.get('table')
                from_field = from_def.get('field')
                to_table = to_def.get('table')
                to_field = to_def.get('field')
                
                if not all([from_table, from_field, to_table, to_field]):
                    self.logger.warning(f"Relation {rel_idx} å®šä¹‰ä¸å®Œæ•´ï¼Œè·³è¿‡")
                    continue
                
                self.logger.info(f"  ğŸ”— éªŒè¯å…³ç³» [{rel_type}]: {from_table}.{from_field} â†’ {to_table}.{to_field}")
                
                relation_violations = await self._verify_single_relation(
                    relation, all_snapshots, schema, context
                )
                
                relation_detail = {
                    'relation_type': rel_type,
                    'from_table': from_table,
                    'from_field': from_field,
                    'to_table': to_table,
                    'to_field': to_field,
                    'violations_count': len(relation_violations),
                    'status': 'passed' if not relation_violations else 'violations_found'
                }
                relation_verification_details.append(relation_detail)
                
                if relation_violations:
                    self.logger.warning(f"    âš ï¸ å‘ç° {len(relation_violations)} ä¸ªè¿è§„")
                    all_violations.extend(relation_violations)
                else:
                    self.logger.info(f"    âœ… å…³ç³»å®Œæ•´")
        else:
            self.logger.info("Schemaä¸­æœªå®šä¹‰relations")
            self.logger.info("â„¹ï¸ [å¤šè¡¨éªŒè¯-Relations] Schemaä¸­æœªå®šä¹‰relations")
        
        self.logger.info(f"ğŸ” [å¤šè¡¨éªŒè¯-ç›¸åŒå­—æ®µ] å¼€å§‹æ£€æŸ¥è·¨è¡¨çš„ç›¸åŒå­—æ®µ...")
        same_field_violations = await self._verify_same_fields_across_tables(
            all_snapshots, schema, context
        )
        
        if same_field_violations:
            self.logger.warning(f"  âš ï¸ å‘ç° {len(same_field_violations)} ä¸ªç›¸åŒå­—æ®µè¿è§„")
            all_violations.extend(same_field_violations)
        else:
            self.logger.info(f"  âœ… ç›¸åŒå­—æ®µéªŒè¯é€šè¿‡")
        
        self.logger.info(f"ğŸ” [å¤šè¡¨éªŒè¯-ä¸šåŠ¡è§„åˆ™] å¼€å§‹æ£€æŸ¥schemaå®šä¹‰çš„å¤šè¡¨ä¸šåŠ¡è§„åˆ™...")
        business_rule_violations = await self._verify_multi_table_business_rules(
            all_snapshots, schema, context
        )
        
        if business_rule_violations:
            self.logger.warning(f"  âš ï¸ å‘ç° {len(business_rule_violations)} ä¸ªä¸šåŠ¡è§„åˆ™è¿è§„")
            all_violations.extend(business_rule_violations)
        else:
            self.logger.info(f"  âœ… ä¸šåŠ¡è§„åˆ™éªŒè¯é€šè¿‡")
        
        self.logger.info(f"ğŸ“Š [å¤šè¡¨éªŒè¯] å®Œæˆï¼Œå…±å‘ç° {len(all_violations)} ä¸ªè·¨è¡¨è¿è§„")
        self.logger.info(f"    - RelationséªŒè¯: {len(all_violations) - len(same_field_violations)} ä¸ª")
        self.logger.info(f"    - ç›¸åŒå­—æ®µéªŒè¯: {len(same_field_violations)} ä¸ª")
        
        if context and hasattr(context, 'step_outputs'):
            from ...core.io import IOManager
            
            multi_table_detail_step = {
                'step': 'multi_table_verification_detailed',
                'step_name': 'multi_table_verification_detailed',
                'status': 'completed',
                'description': f'å¤šè¡¨éªŒè¯è¯¦ç»†ä¿¡æ¯',
                'details': {
                    'total_tables': len(all_snapshots),
                    'tables_verified': list(all_snapshots.keys()),
                    'relations_verified': len(relations) if relations else 0,
                    'relation_verification_details': relation_verification_details,
                    'same_field_checks': len(same_field_verification_details),
                    'total_violations': len(all_violations),
                    'relation_violations': len(all_violations) - len(same_field_violations),
                    'same_field_violations': len(same_field_violations),
                    'verification_summary': f"éªŒè¯äº† {len(relations) if relations else 0} ä¸ªè¡¨å…³ç³»ï¼Œæ£€æŸ¥äº†è·¨è¡¨å­—æ®µä¸€è‡´æ€§"
                },
                'timestamp': IOManager.get_timestamp()
            }
            context.step_outputs.append(multi_table_detail_step)
        
        return all_violations
    
    async def _verify_single_relation(self, relation: Dict[str, Any],
                                     all_snapshots: Dict[str, TableSnapshot],
                                     schema: Dict[str, Any], context=None) -> List[Violation]:
        """éªŒè¯å•ä¸ªrelationçš„å®Œæ•´æ€§"""
        violations = []
        
        from_def = relation.get('from', {})
        to_def = relation.get('to', {})
        
        from_table = from_def.get('table')
        from_field = from_def.get('field')
        to_table = to_def.get('table')
        to_field = to_def.get('field')
        rel_type = relation.get('type', 'unknown')
        
        if from_table not in all_snapshots:
            self.logger.warning(f"æºè¡¨ {from_table} ä¸å­˜åœ¨ï¼Œè·³è¿‡relationéªŒè¯")
            return violations
        
        if to_table not in all_snapshots:
            self.logger.warning(f"ç›®æ ‡è¡¨ {to_table} ä¸å­˜åœ¨ï¼Œè·³è¿‡relationéªŒè¯")
            return violations
        
        from_snapshot = all_snapshots[from_table]
        to_snapshot = all_snapshots[to_table]
        
        to_values = set()
        for row in to_snapshot.rows:
            if to_field in row.cells:
                cell = row.cells[to_field]
                if cell.value is not None:
                    to_values.add(str(cell.value))
        
        for row in from_snapshot.rows:
            if from_field not in row.cells:
                continue
            
            cell = row.cells[from_field]
            if cell.value is None:
                field_nullable = self._is_field_nullable(schema, from_table, from_field)
                if not field_nullable:
                    violation = Violation(
                        id=IdGenerator.generate_violation_id(
                            from_table, row.tuple_id, from_field, "REFERENCE"
                        ),
                        table=from_table,
                        tuple_id=row.tuple_id,
                        attr=from_field,
                        constraint_type=ConstraintType.REFERENCE.value,
                        description=f"å¤–é”®å­—æ®µ {from_field} ä¸ºç©ºï¼Œä½†è¯¥å­—æ®µä¸å…è®¸ä¸ºç©ºï¼ˆå¼•ç”¨ {to_table}.{to_field}ï¼‰",
                        severity=ViolationSeverity.ERROR.value,
                        suggested_fix=None,
                        detector_id=self.verifier_id,
                        timestamp=""
                    )
                    violation.current_value = None
                    violations.append(violation)
                continue
            
            fk_value = str(cell.value)
            if fk_value not in to_values:
                violation = Violation(
                    id=IdGenerator.generate_violation_id(
                        from_table, row.tuple_id, from_field, "REFERENCE"
                    ),
                    table=from_table,
                    tuple_id=row.tuple_id,
                    attr=from_field,
                    constraint_type=ConstraintType.REFERENCE.value,
                    description=f"å¤–é”®å€¼ '{fk_value}' åœ¨ç›®æ ‡è¡¨ {to_table}.{to_field} ä¸­ä¸å­˜åœ¨ï¼ˆ{rel_type} å…³ç³»ï¼‰",
                    severity=ViolationSeverity.ERROR.value,
                    suggested_fix=None,
                    detector_id=self.verifier_id,
                    timestamp=""
                )
                violation.current_value = fk_value
                violations.append(violation)
        
        return violations
    
    def _is_field_nullable(self, schema: Dict[str, Any], table_name: str, field_name: str) -> bool:
        """æ£€æŸ¥å­—æ®µæ˜¯å¦å…è®¸ä¸ºç©º"""
        tables = schema.get('tables', [])
        for table in tables:
            if table.get('name') != table_name:
                continue
            
            fields = table.get('fields', table.get('attributes', []))
            for field in fields:
                if field.get('name') == field_name:
                    constraints = field.get('constraints', {})
                    return constraints.get('nullable', True)
        
        return True
    
    async def _verify_same_fields_across_tables(self, all_snapshots: Dict[str, TableSnapshot],
                                               schema: Dict[str, Any], context=None) -> List[Violation]:
        """éªŒè¯ä¸åŒè¡¨ä¸­çš„ç›¸åŒå­—æ®µï¼ˆå­—æ®µåç›¸åŒï¼‰
        
        éªŒè¯å†…å®¹åŒ…æ‹¬ï¼š
        1. ç›¸åŒå­—æ®µçš„æ•°æ®ç±»å‹ä¸€è‡´æ€§
        2. ç›¸åŒå­—æ®µçš„å€¼åŸŸä¸€è‡´æ€§ï¼ˆå¦‚æœå­—æ®µå€¼è¡¨ç¤ºç›¸åŒçš„å®ä½“/æ¦‚å¿µï¼‰
        3. ç›¸åŒå­—æ®µçš„æ ¼å¼ä¸€è‡´æ€§
        
        Args:
            all_snapshots: æ‰€æœ‰è¡¨çš„å¿«ç…§
            schema: å®Œæ•´schema
            context: å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            è·¨è¡¨ç›¸åŒå­—æ®µè¿è§„åˆ—è¡¨
        """
        violations = []
        
        table_fields = {}  # {table_name: {field_name: field_info}}
        
        for table_name, snapshot in all_snapshots.items():
            if not snapshot or not snapshot.rows:
                continue
            
            if snapshot.rows:
                first_row = snapshot.rows[0]
                field_names = list(first_row.cells.keys())
                
                field_defs = self._get_table_field_definitions(schema, table_name)
                
                table_fields[table_name] = {
                    'field_names': field_names,
                    'field_defs': field_defs,
                    'snapshot': snapshot
                }
        
        field_to_tables = {}  # {field_name: [table_names]}
        for table_name, info in table_fields.items():
            for field_name in info['field_names']:
                if field_name not in field_to_tables:
                    field_to_tables[field_name] = []
                field_to_tables[field_name].append(table_name)
        
        common_fields = {field: tables for field, tables in field_to_tables.items() if len(tables) > 1}
        
        if not common_fields:
            self.logger.info("æœªå‘ç°è·¨è¡¨çš„ç›¸åŒå­—æ®µ")
            return violations
        
        self.logger.info(f"  ğŸ“Œ å‘ç° {len(common_fields)} ä¸ªè·¨è¡¨ç›¸åŒå­—æ®µ:")
        for field, tables in common_fields.items():
            self.logger.info(f"     - {field}: {', '.join(tables)}")
        
        for field_name, table_names in common_fields.items():
            self.logger.info(f"  ğŸ” éªŒè¯ç›¸åŒå­—æ®µ: {field_name}")
            
            field_values_by_table = {}
            field_types_by_table = {}
            
            for table_name in table_names:
                snapshot = table_fields[table_name]['snapshot']
                values = []
                types = set()
                
                for row in snapshot.rows:
                    if field_name in row.cells:
                        cell = row.cells[field_name]
                        if cell.value is not None:
                            values.append(cell.value)
                            types.add(type(cell.value).__name__)
                
                field_values_by_table[table_name] = values
                field_types_by_table[table_name] = types
            
            type_violations = self._check_field_type_consistency(
                field_name, table_names, field_types_by_table, all_snapshots
            )
            violations.extend(type_violations)
            
            domain_violations = self._check_field_domain_consistency(
                field_name, table_names, field_values_by_table, all_snapshots, schema
            )
            violations.extend(domain_violations)
            
            format_violations = self._check_field_format_consistency(
                field_name, table_names, field_values_by_table, all_snapshots
            )
            violations.extend(format_violations)
        
        return violations
    
    def _get_table_field_definitions(self, schema: Dict[str, Any], table_name: str) -> Dict[str, Any]:
        """ä»schemaä¸­è·å–è¡¨çš„å­—æ®µå®šä¹‰"""
        tables = schema.get('tables', [])
        for table in tables:
            if table.get('name') == table_name:
                fields = table.get('fields', table.get('attributes', []))
                return {field.get('name'): field for field in fields if 'name' in field}
        return {}
    
    def _check_field_type_consistency(self, field_name: str, table_names: List[str],
                                     field_types_by_table: Dict[str, set],
                                     all_snapshots: Dict[str, TableSnapshot]) -> List[Violation]:
        """æ£€æŸ¥ç›¸åŒå­—æ®µåœ¨ä¸åŒè¡¨ä¸­çš„ç±»å‹ä¸€è‡´æ€§"""
        violations = []
        
        all_types = set()
        for types in field_types_by_table.values():
            all_types.update(types)
        
        if len(all_types) > 1:
            self.logger.warning(f"    âš ï¸ ç±»å‹ä¸ä¸€è‡´: {all_types}")
            
            type_counts = {}
            for table_name, types in field_types_by_table.items():
                for t in types:
                    type_counts[t] = type_counts.get(t, 0) + 1
            
            expected_type = max(type_counts.items(), key=lambda x: x[1])[0] if type_counts else None
            
            for table_name, types in field_types_by_table.items():
                if expected_type and expected_type not in types:
                    actual_type = list(types)[0] if types else "unknown"
                    violation = Violation(
                        id=IdGenerator.generate_violation_id(
                            table_name, "CROSS_TABLE", field_name, "TYPE"
                        ),
                        table=table_name,
                        tuple_id="CROSS_TABLE",
                        attr=field_name,
                        constraint_type=ConstraintType.TYPE.value,
                        description=f"è·¨è¡¨å­—æ®µç±»å‹ä¸ä¸€è‡´: å­—æ®µ {field_name} åœ¨è¡¨ {table_name} ä¸­ç±»å‹ä¸º {actual_type}ï¼Œä½†åœ¨å…¶ä»–è¡¨ä¸­ä¸º {expected_type}",
                        severity=ViolationSeverity.WARN.value,
                        suggested_fix=None,
                        detector_id=self.verifier_id,
                        timestamp=""
                    )
                    violations.append(violation)
        
        return violations
    
    def _check_field_domain_consistency(self, field_name: str, table_names: List[str],
                                       field_values_by_table: Dict[str, List],
                                       all_snapshots: Dict[str, TableSnapshot],
                                       schema: Dict[str, Any]) -> List[Violation]:
        """æ£€æŸ¥ç›¸åŒå­—æ®µåœ¨ä¸åŒè¡¨ä¸­çš„å€¼åŸŸä¸€è‡´æ€§
        
        å¦‚æœå­—æ®µåœ¨schemaä¸­å®šä¹‰äº†æšä¸¾å€¼ï¼Œæ£€æŸ¥å®é™…å€¼æ˜¯å¦éƒ½åœ¨æšä¸¾èŒƒå›´å†…
        
        æ³¨æ„ï¼šç›¸ä¼¼å®ä½“æ£€æµ‹ç°åœ¨ç”±ConsistencyMCPç»„ä»¶è´Ÿè´£
        """
        violations = []
        
        all_unique_values = set()
        for values in field_values_by_table.values():
            all_unique_values.update(str(v) for v in values)
        
        if len(all_unique_values) < 20 and len(all_unique_values) > 0:
            value_sets = {table: set(str(v) for v in values) 
                         for table, values in field_values_by_table.items()}
            
            common_values = set.intersection(*value_sets.values()) if value_sets else set()
            
            for table_name, values in value_sets.items():
                unique_to_table = values - common_values
                if unique_to_table and len(unique_to_table) / len(values) > 0.3:  # è¶…è¿‡30%æ˜¯ç‰¹æœ‰å€¼
                    self.logger.info(f"    â„¹ï¸  è¡¨ {table_name} æœ‰ç‰¹æœ‰å€¼: {unique_to_table}")
        
        return violations
    
    def _check_field_format_consistency(self, field_name: str, table_names: List[str],
                                       field_values_by_table: Dict[str, List],
                                       all_snapshots: Dict[str, TableSnapshot]) -> List[Violation]:
        """æ£€æŸ¥ç›¸åŒå­—æ®µåœ¨ä¸åŒè¡¨ä¸­çš„æ ¼å¼ä¸€è‡´æ€§ï¼ˆé’ˆå¯¹å­—ç¬¦ä¸²å‹å­—æ®µï¼‰"""
        violations = []
        
        format_patterns = {}  # {table_name: set_of_patterns}
        
        for table_name, values in field_values_by_table.items():
            patterns = set()
            for value in values:
                if isinstance(value, str):
                    pattern = self._detect_string_pattern(value)
                    patterns.add(pattern)
            format_patterns[table_name] = patterns
        
        all_patterns = set()
        for patterns in format_patterns.values():
            all_patterns.update(patterns)
        
        if len(all_patterns) > 3:  # è¶…è¿‡3ç§ä¸åŒæ¨¡å¼
            self.logger.info(f"    âš ï¸ æ ¼å¼æ¨¡å¼è¾ƒå¤š: {all_patterns}")
        
        return violations
    
    async def _verify_multi_table_business_rules(self, all_snapshots: Dict[str, TableSnapshot],
                                                 schema: Dict[str, Any], context=None) -> List[Violation]:
        """éªŒè¯schemaä¸­å®šä¹‰çš„å¤šè¡¨ä¸šåŠ¡è§„åˆ™ï¼ˆåŒ…æ‹¬é€»è¾‘è§„åˆ™å’Œèšåˆè§„åˆ™ï¼‰"""
        violations = []
        
        try:
            from ..mcp import LogicMCP
            
            logic_mcp = LogicMCP()
            if hasattr(logic_mcp.verifier, 'verify_multi_table_business_rules'):
                logic_violations = logic_mcp.verifier.verify_multi_table_business_rules(
                    all_snapshots, schema, context
                )
                violations.extend(logic_violations)
        except Exception as e:
            self.logger.error(f"å¤šè¡¨ä¸šåŠ¡é€»è¾‘è§„åˆ™éªŒè¯å¤±è´¥: {e}")
        
        try:
            from ..mcp import AggregationMCP
            
            aggregation_mcp = AggregationMCP()
            if hasattr(aggregation_mcp.verifier, 'verify_multi_table_aggregation_rules'):
                aggregation_violations = aggregation_mcp.verifier.verify_multi_table_aggregation_rules(
                    all_snapshots, schema, context
                )
                violations.extend(aggregation_violations)
        except Exception as e:
            self.logger.error(f"å¤šè¡¨èšåˆè§„åˆ™éªŒè¯å¤±è´¥: {e}")
        
        return violations
    
    def _detect_string_pattern(self, value: str) -> str:
        """æ£€æµ‹å­—ç¬¦ä¸²çš„æ ¼å¼æ¨¡å¼"""
        import re
        
        if not value:
            return "empty"
        
        if re.match(r'^\d{4}-\d{2}-\d{2}$', value):
            return "date_iso"
        elif re.match(r'^\d{2}/\d{2}/\d{4}$', value):
            return "date_us"
        elif re.match(r'^\d+$', value):
            return "numeric"
        elif re.match(r'^\d+\.\d+$', value):
            return "decimal"
        elif re.match(r'^[A-Z][a-z]+$', value):
            return "capitalized_word"
        elif re.match(r'^[A-Z]+$', value):
            return "uppercase"
        elif re.match(r'^[a-z]+$', value):
            return "lowercase"
        else:
            return "mixed"