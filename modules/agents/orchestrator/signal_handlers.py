"""ä¿¡å·å¤„ç†å™¨æ¨¡å—"""
import asyncio
import time
import os
from typing import List, Optional, Any
import logging

from .context import Doc2DBContext, Doc2DBProcessingState
from ...memory import TableSnapshot, Violation, Fix
from ...signals.core import SignalType
from ...signals.handlers import BaseSignalHandler
from .utils import DocumentUtils


class OrchestratorSignalHandler(BaseSignalHandler):
    """åè°ƒå™¨ä¿¡å·å¤„ç†å™¨"""
    
    def __init__(self, orchestrator):
        super().__init__('unified_orchestrator')
        self.orchestrator = orchestrator
        self.logger = logging.getLogger('orchestrator.signal_handler')
        
        self.add_supported_signal(SignalType.EXTRACTION_COMPLETE)
        self.add_supported_signal(SignalType.VERIFICATION_COMPLETE)
        self.add_supported_signal(SignalType.FIXING_COMPLETE)
        self.add_supported_signal(SignalType.EXTRACTION_ERROR)
        self.add_supported_signal(SignalType.VERIFICATION_ERROR)
        self.add_supported_signal(SignalType.FIXING_ERROR)
        self.add_supported_signal(SignalType.WARM_START_REQUEST)
        
    async def _process_signal(self, signal) -> Optional[Any]:
        """å¤„ç†åè°ƒå™¨ç›¸å…³ä¿¡å·"""
        if signal.signal_type == SignalType.EXTRACTION_COMPLETE:
            return await self.orchestrator.handle_extraction_complete(signal)
        elif signal.signal_type == SignalType.VERIFICATION_COMPLETE:
            return await self.orchestrator.handle_verification_complete(signal)
        elif signal.signal_type == SignalType.FIXING_COMPLETE:
            return await self.orchestrator.handle_fixing_complete(signal)
        elif signal.signal_type == SignalType.WARM_START_REQUEST:
            return await self.orchestrator.handle_warm_start_request(signal)
        elif signal.signal_type in [SignalType.EXTRACTION_ERROR, SignalType.VERIFICATION_ERROR, SignalType.FIXING_ERROR]:
            return await self.orchestrator.handle_component_error(signal)
        
        return None


class SignalHandlerMixin:
    """ä¿¡å·å¤„ç†å™¨æ··å…¥ç±»ï¼Œæä¾›ä¿¡å·å¤„ç†æ–¹æ³•"""
    
    async def handle_extraction_complete(self, signal):
        """å¤„ç†æå–å®Œæˆä¿¡å·ï¼ˆæ”¯æŒç‰‡æ®µå¤„ç†å’Œbatchå¤„ç†ï¼‰"""
        data = signal.data
        table_name = data.get('table_name')
        snapshots = data.get('snapshots', [])
        context_data = data.get('context', {})
        segment_info = data.get('segment_info')
        batch_info = data.get('batch_info')
        if self.current_context:
            signal_run_id = context_data.get('run_id') or signal.correlation_id.split('_')[0] if hasattr(signal, 'correlation_id') else None
            current_run_id = self.current_context.run_id
            current_state = getattr(self.current_context, 'current_state', None)
            
            if signal_run_id and signal_run_id != current_run_id:
                self.logger.warning(f'â›” æ‹’ç»ä¸åŒ¹é…çš„ä¿¡å·ï¼šä¿¡å·run_id={signal_run_id}, å½“å‰run_id={current_run_id}')
                return
            
            if current_state and str(current_state).endswith('COMPLETED'):
                self.logger.warning(f'â›” ä»»åŠ¡å·²å®Œæˆï¼ˆçŠ¶æ€ï¼š{current_state}ï¼‰ï¼Œå¿½ç•¥æ¥è‡ªè¡¨ {table_name} çš„æå–å®Œæˆä¿¡å·')
                return
        
        self.logger.info(f' æ”¶åˆ°æå–å®Œæˆä¿¡å· - è¡¨: {table_name}, batch_info: {batch_info is not None}, segment_info: {segment_info is not None}')
        
        is_warm_start = context_data.get('warm_start', False) or (hasattr(self.current_context, 'warm_start_attempted') and table_name in self.current_context.warm_start_attempted)
        
        if is_warm_start and self.current_context and hasattr(self.current_context, 'step_outputs'):
            from ...core.io import IOManager
            for step in self.current_context.step_outputs:
                if step.get('step') == f'warm_start_extraction_{table_name}' and step.get('status') == 'in_progress':
                    step['status'] = 'completed'
                    step['description'] = f'Warm Start å®Œæˆ: é‡æ–°æå–äº† {len(snapshots[0].rows) if snapshots and len(snapshots) > 0 else 0} è¡Œæ•°æ®'
                    step['details']['rows_extracted'] = len(snapshots[0].rows) if snapshots and len(snapshots) > 0 else 0
                    step['timestamp_completed'] = IOManager.get_timestamp()
                    self.logger.info(f' å·²æ›´æ–° warm start æ­¥éª¤çŠ¶æ€ä¸ºå®Œæˆ')
                    break
            
            self.logger.info(f' Warm startæå–å®Œæˆ: {table_name}, ç­‰å¾…éªŒè¯å®Œæˆ...')
        
        if self.current_context and hasattr(self.current_context, 'warm_start_in_progress') and table_name in self.current_context.warm_start_in_progress:
            self.current_context.warm_start_in_progress.remove(table_name)
        
        if batch_info:
            await self._handle_batch_extraction_complete(
                table_name, snapshots, batch_info, context_data
            )
            return
        
        if segment_info:
            await self._handle_segment_extraction_complete(
                table_name, snapshots, segment_info, context_data
            )
            return
        if self.current_context and snapshots:
            for snapshot in snapshots:
                self.logger.info(f'ğŸ“¥ æ”¶åˆ°æå–å®Œæˆä¿¡å· - è¡¨: {table_name}, æ–°æ•°æ®: {len(snapshot.rows)}è¡Œ')
                self.logger.info(f' å½“å‰context.all_snapshotsä¸­çš„è¡¨: {list(self.current_context.all_snapshots.keys())}')
                
                if table_name in self.current_context.all_snapshots:
                    existing_snapshot = self.current_context.all_snapshots[table_name]
                    self.logger.info(f' è¡¨ {table_name} å·²å­˜åœ¨snapshot ({len(existing_snapshot.rows)}è¡Œ)ï¼Œæ–°æ•°æ® ({len(snapshot.rows)}è¡Œ)')
                    
                    existing_desc = getattr(existing_snapshot, 'stage_description', '')
                    is_batch_merged = 'åˆ†batchæå–å®Œæˆ' in existing_desc or 'åˆ†ç‰‡æ®µæå–å®Œæˆ' in existing_desc
                    
                    if not is_batch_merged and hasattr(existing_snapshot, 'is_batch_merged'):
                        is_batch_merged = existing_snapshot.is_batch_merged
                    if not is_batch_merged and hasattr(self.current_context, 'batch_merged_tables') and table_name in self.current_context.batch_merged_tables:
                        is_batch_merged = True
                    
                    is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False) if context_data else False
                    
                    if is_batch_merged:
                        self.logger.warning(f'â›” æ£€æµ‹åˆ°batch/segmentåˆå¹¶ç»“æœï¼Œè·³è¿‡è¦†ç›–ï¼ä¿æŒç°æœ‰ {len(existing_snapshot.rows)} è¡Œæ•°æ®')
                        
                        if is_warm_start and len(snapshot.rows) > 0:
                            self.logger.info(f' Warm startæ¨¡å¼ï¼šæ‰§è¡Œcellçº§åˆ«ä¿®å¤åˆå¹¶')
                            existing_snapshot = self._merge_warm_start_cells(existing_snapshot, snapshot, table_name)
                        
                        snapshot = existing_snapshot
                    else:
                        is_warm_start = context_data.get('warm_start', False) if context_data else False
                        warm_start_failed = False
                        
                        if is_warm_start:
                            if len(snapshot.rows) == 0:
                                self.logger.warning(f' Warm starté‡æå–è¿”å›ç©ºæ•°æ®ï¼Œä¿ç•™åŸæœ‰æ•°æ®å¹¶ç›´æ¥æ ‡è®°è¡¨ä¸ºå®Œæˆ')
                                snapshot = existing_snapshot
                                warm_start_failed = True
                            elif len(snapshot.rows) < len(existing_snapshot.rows) * 0.5:
                                self.logger.warning(f' Warm starté‡æå–æ•°æ®é‡æ˜¾è‘—å‡å°‘ ({len(snapshot.rows)} < {len(existing_snapshot.rows)}*0.5)ï¼Œä¿ç•™åŸæœ‰æ•°æ®å¹¶ç›´æ¥æ ‡è®°è¡¨ä¸ºå®Œæˆ')
                                snapshot = existing_snapshot
                                warm_start_failed = True
                            else:
                                self.logger.info(f' Warm starté‡æå–æˆåŠŸï¼Œä½¿ç”¨æ–°æ•°æ® ({len(snapshot.rows)}è¡Œ)')
                            
                            if warm_start_failed:
                                self.logger.info(f'ğŸ Warm startå¤±è´¥ï¼Œç›´æ¥æ ‡è®°è¡¨ {table_name} ä¸ºå®Œæˆ')
                                self._set_table_force_completed(table_name)
                                
                                if hasattr(self.current_context, 'warm_start_tracking') and table_name in self.current_context.warm_start_tracking:
                                    self.current_context.warm_start_tracking.remove(table_name)
                                    self.logger.info(f' å·²ä»warm startè¿½è¸ªä¸­ç§»é™¤: {table_name}')
                        else:
                            if hasattr(existing_snapshot, 'rows') and hasattr(snapshot, 'rows'):
                                existing_snapshot.rows.extend(snapshot.rows)
                                snapshot = existing_snapshot
                
                self.current_context.all_snapshots[table_name] = snapshot
                if 'åˆ†batchæå–å®Œæˆ' not in getattr(snapshot, 'stage_description', '') and 'åˆ†ç‰‡æ®µæå–å®Œæˆ' not in getattr(snapshot, 'stage_description', ''):
                    snapshot.processing_stage = 'extraction'
                    snapshot.stage_description = f'æ•°æ®æå–å®Œæˆ - è¡¨ {table_name}ï¼Œå…± {len(snapshot.rows)} è¡Œæ•°æ®'
                
                self.current_context.io_manager.append_snapshot(snapshot)
                
                self._sync_current_snapshot_to_context()
        
        self.logger.info(f' [æå–å®Œæˆ] å¼€å§‹åŸºç¡€éªŒè¯: {table_name}')
        basic_violations_triggered_fix = await self._auto_basic_verification(table_name, snapshots, context_data)
        self.logger.info(f' [æå–å®Œæˆ] åŸºç¡€éªŒè¯è¿”å›: triggered_fix={basic_violations_triggered_fix}')
        
        if not basic_violations_triggered_fix:
            self.logger.info(f' [æå–å®Œæˆ] æœªè§¦å‘ä¿®å¤ï¼Œå¼€å§‹å®Œæ•´éªŒè¯: {table_name}')
            await self._signal_verify_data(table_name, snapshots, context_data)
        else:
            self.logger.info(f'â³ [æå–å®Œæˆ] å·²è§¦å‘ä¿®å¤ï¼Œç­‰å¾…ä¿®å¤å®Œæˆåè‡ªåŠ¨éªŒè¯: {table_name}')
    
    async def _handle_batch_extraction_complete(self, table_name: str, snapshots: List, 
                                                batch_info: dict, context_data: dict):
        """å¤„ç† batch æå–å®Œæˆ"""
        batch_index = batch_info.get('batch_index')
        total_batches = batch_info.get('total_batches')
        is_warm_start = batch_info.get('is_warm_start', False)
        
        warm_start_label = " (warm start)" if is_warm_start else ""
        self.logger.info(f' è¡¨ {table_name} - Batch {batch_index}/{total_batches}{warm_start_label} æå–å®Œæˆ')
        
        if hasattr(self, 'coordinator'):
            self.coordinator.mark_batch_completed(table_name, batch_index, is_warm_start=is_warm_start)
        
        if not hasattr(self.current_context, 'batch_tracking'):
            self.current_context.batch_tracking = {}
        
        if table_name not in self.current_context.batch_tracking:
            self.logger.warning(f'è¡¨ {table_name} æ²¡æœ‰ batch è¿½è¸ªä¿¡æ¯ï¼Œåˆå§‹åŒ–ä¸­...')
            self.current_context.batch_tracking[table_name] = {
                'total_batches': total_batches,
                'completed_batches': 0,
                'snapshots': []
            }
        
        tracking = self.current_context.batch_tracking[table_name]
        
        if snapshots:
            tracking['snapshots'].extend(snapshots)
            tracking['completed_batches'] += 1
            
            self.logger.debug(
                f'Batchæ”¶é›†è¿›åº¦ï¼š{tracking["completed_batches"]}/{tracking["total_batches"]} '
                f'(å·²æ”¶é›† {len(tracking["snapshots"])} ä¸ª snapshots)'
            )
        
        if snapshots:
            self.logger.info(f'å¼€å§‹å¯¹ Batch {batch_index} çš„ç»“æœè¿›è¡ŒéªŒè¯')
            
            context_data['is_batch_verification'] = True
            context_data['batch_index'] = batch_index
            context_data['batch_total'] = total_batches
            context_data['is_warm_start'] = is_warm_start  #  ä¼ é€’ warm start æ ‡å¿—
            
            basic_violations_triggered_fix = await self._auto_basic_verification(
                table_name, snapshots, context_data
            )
            
            if not basic_violations_triggered_fix:
                await self._signal_verify_data(table_name, snapshots, context_data)
        
        if tracking['completed_batches'] >= tracking['total_batches']:
            self.logger.info(f'ğŸ‰ è¡¨ {table_name} - æ‰€æœ‰ {total_batches} ä¸ª batch å¤„ç†å®Œæˆï¼Œå¼€å§‹åˆå¹¶...')
            
            merged_snapshot = self._merge_batch_snapshots(table_name, tracking['snapshots'])
            
            if merged_snapshot:
                self.current_context.all_snapshots[table_name] = merged_snapshot
                merged_snapshot.processing_stage = 'extraction'
                merged_snapshot.stage_description = (
                    f'åˆ†batchæå–å®Œæˆ - è¡¨ {table_name}ï¼Œ'
                    f'{total_batches} ä¸ªbatchï¼Œå…± {len(merged_snapshot.rows)} è¡Œæ•°æ®'
                )
                
                merged_snapshot.is_batch_merged = True
                merged_snapshot.merged_row_count = len(merged_snapshot.rows)
                
                if not hasattr(self.current_context, 'batch_merged_tables'):
                    self.current_context.batch_merged_tables = {}
                self.current_context.batch_merged_tables[table_name] = len(merged_snapshot.rows)
                self.logger.info(f'ğŸ”’ æ ‡è®°è¡¨ {table_name} ä¸ºbatchåˆå¹¶ç»“æœï¼Œå…± {len(merged_snapshot.rows)} è¡Œï¼Œé˜²æ­¢è¢«è¦†ç›–')
                
                self.current_context.io_manager.append_snapshot(merged_snapshot)
                self._sync_current_snapshot_to_context()
                
                del self.current_context.batch_tracking[table_name]
                
                if hasattr(self, 'coordinator') and self.coordinator:
                    from .signal_coordinator import TableLifecycleState
                    self.coordinator.update_table_state(table_name, TableLifecycleState.EXTRACTED)
                
                self.logger.info(f' è¡¨ {table_name} æ‰€æœ‰ batch å·²åˆå¹¶å®Œæˆ')
                
                if total_batches == 1:
                    self.logger.info(f' è¡¨ {table_name} åªæœ‰1ä¸ªbatchï¼Œå·²åœ¨å•ä¸ªbatchæ—¶éªŒè¯è¿‡ï¼Œè·³è¿‡åˆå¹¶åçš„éªŒè¯')
                else:
                    if 'is_batch_verification' in context_data:
                        del context_data['is_batch_verification']
                    
                    basic_violations_triggered_fix = await self._auto_basic_verification(
                        table_name, [merged_snapshot], context_data
                    )
                    
                    if not basic_violations_triggered_fix:
                        await self._signal_verify_data(table_name, [merged_snapshot], context_data)
            else:
                self.logger.error(f'è¡¨ {table_name} batch åˆå¹¶å¤±è´¥')
    
    def _merge_batch_snapshots(self, table_name: str, snapshots: List) -> Optional[Any]:
        """åˆå¹¶å¤šä¸ª batch çš„ snapshots
        
        ç­–ç•¥ï¼šæ™ºèƒ½å»é‡
        - åªåˆ é™¤å®Œå…¨ç›¸åŒçš„è¡Œ
        - æˆ–åˆ é™¤æ˜¯å…¶ä»–è¡Œå­é›†çš„è¡Œ
        """
        if not snapshots:
            return None
        
        if len(snapshots) == 1:
            return snapshots[0]
        
        try:
            from ...memory import TableSnapshot
            from ...core.io import IOManager
            
            base_snapshot = snapshots[0]
            
            all_rows = []
            for snapshot in snapshots:
                if hasattr(snapshot, 'rows') and snapshot.rows:
                    all_rows.extend(snapshot.rows)
            
            if not all_rows:
                return base_snapshot
            
            unique_rows = []
            removed_count = 0
            
            for i, row1 in enumerate(all_rows):
                should_keep = True
                
                for j, row2 in enumerate(all_rows):
                    if i == j:
                        continue
                    
                    relation = self._compare_rows(row1, row2)
                    
                    if relation == 'identical':
                        if j < i:
                            should_keep = False
                            self.logger.debug(f' å»é‡: è¡Œ {i} ä¸è¡Œ {j} å®Œå…¨ç›¸åŒï¼Œåˆ é™¤è¡Œ {i}')
                            removed_count += 1
                            break
                    elif relation == 'row1_subset':
                        should_keep = False
                        self.logger.debug(f' å»é‡: è¡Œ {i} æ˜¯è¡Œ {j} çš„å­é›†ï¼Œåˆ é™¤è¡Œ {i}')
                        removed_count += 1
                        break
                
                if should_keep:
                    unique_rows.append(row1)
            
            if removed_count > 0:
                self.logger.info(f' Batchåˆå¹¶æ—¶å»é‡äº† {removed_count} è¡Œæ•°æ®ï¼ˆå®Œå…¨ç›¸åŒæˆ–å­é›†å…³ç³»ï¼‰')
            
            merged_snapshot = TableSnapshot(
                run_id=base_snapshot.run_id if hasattr(base_snapshot, 'run_id') else None,
                table=table_name,
                rows=unique_rows,
                created_at=IOManager.get_timestamp(),
                table_id=base_snapshot.table_id if hasattr(base_snapshot, 'table_id') else table_name
            )
            
            self.logger.info(
                f' Batchåˆå¹¶å®Œæˆï¼š{len(snapshots)} ä¸ªbatch â†’ {len(unique_rows)} è¡Œæ•°æ®ï¼ˆå»é‡åï¼‰'
            )
            
            return merged_snapshot
            
        except Exception as e:
            self.logger.error(f'åˆå¹¶ batch snapshots å¤±è´¥: {e}', exc_info=True)
            return None
    
    def _compare_rows(self, row1, row2) -> str:
        """æ¯”è¾ƒä¸¤è¡Œæ•°æ®çš„å…³ç³»
        
        Returns:
            'identical': å®Œå…¨ç›¸åŒ
            'row1_subset': row1æ˜¯row2çš„å­é›†
            'row2_subset': row2æ˜¯row1çš„å­é›†
            'different': ä¸åŒ
        """
        if not hasattr(row1, 'cells') or not hasattr(row2, 'cells'):
            return 'different'
        
        cells1 = row1.cells if isinstance(row1.cells, dict) else {}
        cells2 = row2.cells if isinstance(row2.cells, dict) else {}
        
        values1 = {}
        values2 = {}
        
        for field, cell in cells1.items():
            value = cell.value if hasattr(cell, 'value') else cell
            if value not in [None, '', 'null', 'NULL']:
                values1[field] = str(value).strip()
        
        for field, cell in cells2.items():
            value = cell.value if hasattr(cell, 'value') else cell
            if value not in [None, '', 'null', 'NULL']:
                values2[field] = str(value).strip()
        
        if values1 == values2:
            return 'identical'
        
        if values1 and values2:
            if all(field in values2 and values1[field] == values2[field] for field in values1):
                if len(values1) < len(values2):
                    return 'row1_subset'
                elif len(values1) == len(values2):
                    return 'identical'
            
            if all(field in values1 and values2[field] == values1[field] for field in values2):
                if len(values2) < len(values1):
                    return 'row2_subset'
        
        return 'different'
    
    async def _handle_segment_extraction_complete(self, table_name: str, snapshots: List, 
                                                  segment_info: dict, context_data: dict):
        """å¤„ç†ç‰‡æ®µæå–å®Œæˆï¼ˆæ”¶é›†å¹¶åˆå¹¶ï¼‰"""
        segment_index = segment_info.get('segment_index')
        total_segments = segment_info.get('total_segments')
        
        self.logger.info(f' è¡¨ {table_name} - ç‰‡æ®µ {segment_index}/{total_segments} æå–å®Œæˆ')
        
        if not hasattr(self.current_context, 'segment_tracking'):
            self.current_context.segment_tracking = {}
        
        if table_name not in self.current_context.segment_tracking:
            self.logger.warning(f'è¡¨ {table_name} æ²¡æœ‰ç‰‡æ®µè¿½è¸ªä¿¡æ¯ï¼Œåˆå§‹åŒ–ä¸­...')
            self.current_context.segment_tracking[table_name] = {
                'total_segments': total_segments,
                'completed_segments': 0,
                'snapshots': []
            }
        
        tracking = self.current_context.segment_tracking[table_name]
        
        if snapshots:
            tracking['snapshots'].extend(snapshots)
            tracking['completed_segments'] += 1
            
            self.logger.debug(
                f'ç‰‡æ®µæ”¶é›†è¿›åº¦ï¼š{tracking["completed_segments"]}/{tracking["total_segments"]} '
                f'(å·²æ”¶é›† {len(tracking["snapshots"])} ä¸ª snapshots)'
            )
        
        if tracking['completed_segments'] >= tracking['total_segments']:
            self.logger.info(f'ğŸ‰ è¡¨ {table_name} - æ‰€æœ‰ {total_segments} ä¸ªç‰‡æ®µæå–å®Œæˆï¼Œå¼€å§‹åˆå¹¶...')
            
            merged_snapshot = self._merge_segment_snapshots(table_name, tracking['snapshots'])
            
            if merged_snapshot:
                self.current_context.all_snapshots[table_name] = merged_snapshot
                merged_snapshot.processing_stage = 'extraction'
                merged_snapshot.stage_description = (
                    f'åˆ†ç‰‡æ®µæå–å®Œæˆ - è¡¨ {table_name}ï¼Œ'
                    f'{total_segments} ä¸ªç‰‡æ®µï¼Œå…± {len(merged_snapshot.rows)} è¡Œæ•°æ®'
                )
                self.current_context.io_manager.append_snapshot(merged_snapshot)
                self._sync_current_snapshot_to_context()
                
                del self.current_context.segment_tracking[table_name]
                
                self.logger.info(f'å¼€å§‹å¯¹åˆå¹¶åçš„è¡¨ {table_name} è¿›è¡ŒéªŒè¯')
                
                basic_violations_triggered_fix = await self._auto_basic_verification(
                    table_name, [merged_snapshot], context_data
                )
                
                if not basic_violations_triggered_fix:
                    await self._signal_verify_data(table_name, [merged_snapshot], context_data)
            else:
                self.logger.error(f'è¡¨ {table_name} ç‰‡æ®µåˆå¹¶å¤±è´¥')
    
    def _merge_warm_start_cells(self, existing_snapshot, warm_start_snapshot, table_name: str):
        """
        Warm startçš„cellçº§åˆ«åˆå¹¶ï¼šç”¨warm startä¿®å¤çš„cellæ›´æ–°batchåˆå¹¶ç»“æœ
        
        Args:
            existing_snapshot: batchåˆå¹¶çš„å®Œæ•´å¿«ç…§
            warm_start_snapshot: warm starté‡æ–°æå–çš„éƒ¨åˆ†å¿«ç…§
            table_name: è¡¨å
            
        Returns:
            åˆå¹¶åçš„snapshot
        """
        try:
            warm_start_rows = {row.tuple_id: row for row in warm_start_snapshot.rows}
            
            updated_cells_count = 0
            
            for existing_row in existing_snapshot.rows:
                if existing_row.tuple_id in warm_start_rows:
                    warm_row = warm_start_rows[existing_row.tuple_id]
                    
                    for attr_name, warm_cell in warm_row.cells.items():
                        if attr_name in existing_row.cells:
                            existing_cell = existing_row.cells[attr_name]
                            warm_value = warm_cell.value if hasattr(warm_cell, 'value') else None
                            existing_value = existing_cell.value if hasattr(existing_cell, 'value') else None
                            
                            if warm_value != existing_value:
                                if hasattr(existing_cell, 'value'):
                                    existing_cell.value = warm_value
                                
                                if hasattr(warm_cell, 'evidences') and hasattr(existing_cell, 'evidences'):
                                    warm_evidences = warm_cell.evidences if isinstance(warm_cell.evidences, list) else [warm_cell.evidences]
                                    existing_evidences = existing_cell.evidences if isinstance(existing_cell.evidences, list) else [existing_cell.evidences]
                                    
                                    warm_evidences_marked = [f"[Warm Start Fix] {e}" if not e.startswith("[Warm Start Fix]") else e for e in warm_evidences]
                                    
                                    all_evidences = existing_evidences + warm_evidences_marked
                                    existing_cell.evidences = list(set(all_evidences))
                                
                                updated_cells_count += 1
            
            self.logger.info(f' Warm start cellçº§åˆ«åˆå¹¶å®Œæˆ: {table_name}, æ›´æ–°äº† {updated_cells_count} ä¸ªcell')
            return existing_snapshot
            
        except Exception as e:
            self.logger.error(f'Warm start cellåˆå¹¶å¤±è´¥: {e}', exc_info=True)
            return existing_snapshot
    
    def _merge_segment_snapshots(self, table_name: str, snapshots: List) -> Optional[Any]:
        """åˆå¹¶å¤šä¸ªç‰‡æ®µçš„ snapshots
        
        ç­–ç•¥ï¼šç®€å•æ‹¼æ¥æ‰€æœ‰ snapshot çš„ rowsï¼Œverifier ä¼šè´Ÿè´£å»é‡å’Œä¸€è‡´æ€§æ£€æŸ¥
        
        Returns:
            åˆå¹¶åçš„ TableSnapshotï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        if not snapshots:
            return None
        
        if len(snapshots) == 1:
            return snapshots[0]
        
        try:
            from ...memory import TableSnapshot
            from ...core.io import IOManager
            
            base_snapshot = snapshots[0]
            
            all_rows = []
            for snapshot in snapshots:
                if hasattr(snapshot, 'rows') and snapshot.rows:
                    all_rows.extend(snapshot.rows)
            
            merged_snapshot = TableSnapshot(
                run_id=base_snapshot.run_id if hasattr(base_snapshot, 'run_id') else None,
                table=table_name,
                rows=all_rows,
                created_at=IOManager.get_timestamp(),
                table_id=base_snapshot.table_id if hasattr(base_snapshot, 'table_id') else table_name
            )
            
            self.logger.info(
                f' åˆå¹¶å®Œæˆï¼š{len(snapshots)} ä¸ªç‰‡æ®µ â†’ {len(all_rows)} è¡Œæ•°æ®'
            )
            
            return merged_snapshot
            
        except Exception as e:
            self.logger.error(f'åˆå¹¶ç‰‡æ®µ snapshots å¤±è´¥: {e}', exc_info=True)
            return None
    
    async def handle_verification_complete(self, signal):
        """å¤„ç†éªŒè¯å®Œæˆä¿¡å·"""
        data = signal.data
        table_name = data.get('table_name')
        violations = data.get('violations', [])
        
        if self.current_context:
            current_state = getattr(self.current_context, 'current_state', None)
            if current_state and str(current_state).endswith('COMPLETED'):
                self.logger.warning(f'â›” ä»»åŠ¡å·²å®Œæˆï¼ˆçŠ¶æ€ï¼š{current_state}ï¼‰ï¼Œå¿½ç•¥æ¥è‡ªè¡¨ {table_name} çš„éªŒè¯å®Œæˆä¿¡å·')
                return
        
        context_data = data.get('context', {})
        batch_index = context_data.get('batch_index')
        if hasattr(self, 'coordinator') and self.coordinator:
            tracker = self.coordinator.get_table_tracker(table_name)
            if tracker:
                current_round = tracker.verification_round
                max_rounds = tracker.max_verification_rounds
                if current_round >= max_rounds:
                    self.logger.warning(
                        f'ğŸ›‘ å¼ºåˆ¶åœæ­¢ï¼šè¡¨ {table_name} å·²å®Œæˆ {current_round} è½®éªŒè¯ï¼Œ'
                        f'è¾¾åˆ°æœ€å¤§è½®æ¬¡ ({max_rounds})'
                    )
                    self._mark_final(table_name, batch_index)
                    return

        
        if hasattr(self, 'coordinator') and self.coordinator:
            from .signal_coordinator import TableLifecycleState
            if self.coordinator.is_table_in_state(table_name, TableLifecycleState.FINAL):
                self.logger.warning(f'â›” è¡¨ {table_name} å·²æ ‡è®°ä¸ºFINALï¼Œå¿½ç•¥éªŒè¯å®Œæˆä¿¡å·')
                
                if hasattr(self.current_context, 'warm_start_attempted') and table_name in self.current_context.warm_start_attempted:
                    if hasattr(self.current_context, 'warm_start_tracking') and table_name in self.current_context.warm_start_tracking:
                        self.current_context.warm_start_tracking.remove(table_name)
                        self.logger.info(f' å·²ä» warm_start_tracking ä¸­ç§»é™¤ FINAL è¡¨: {table_name}')
                
                context_data = data.get('context', {})
                batch_index = context_data.get('batch_index')
                if batch_index is not None:
                    tracker = self.coordinator.get_table_tracker(table_name)
                    if tracker:
                        is_warm_start = context_data.get('is_warm_start_batch', False)
                        batch_key = self.coordinator._get_batch_key(batch_index, is_warm_start)
                        
                        if batch_key in tracker.batch_trackers:
                            batch_tracker = tracker.batch_trackers[batch_key]
                            if batch_tracker.state != 'final':
                                self.logger.info(f' å¼ºåˆ¶æ ‡è®° Batch {batch_index} (warm_start={is_warm_start}) ä¸º finalï¼Œå› ä¸ºè¡¨å·²ç»æ˜¯ FINAL çŠ¶æ€')
                                self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start)
                
                return
        
        snapshot = data.get('snapshot')
        
        is_warm_start_verification = (
            hasattr(self.current_context, 'warm_start_attempted') and 
            table_name in self.current_context.warm_start_attempted
        )
        
        if is_warm_start_verification:
            error_violations = [v for v in violations if getattr(v, 'severity', 'warn') == 'error']
            
            if hasattr(self.current_context, 'warm_start_tracking') and table_name in self.current_context.warm_start_tracking:
                self.current_context.warm_start_tracking.remove(table_name)
                if len(error_violations) == 0:
                    self.logger.info(
                        f' Warm startéªŒè¯å®Œæˆä¸”æ— error: {table_name}, å·²ä»è¿½è¸ªä¸­ç§»é™¤, '
                        f'å‰©ä½™: {list(self.current_context.warm_start_tracking) if self.current_context.warm_start_tracking else []}'
                    )
                else:
                    self.logger.info(
                        f' Warm startéªŒè¯å®Œæˆä½†ä»æœ‰ {len(error_violations)} ä¸ªerror, '
                        f'å·²ä»è¿½è¸ªä¸­ç§»é™¤ï¼ˆé¿å…é˜»å¡ï¼‰, å‰©ä½™: {list(self.current_context.warm_start_tracking) if self.current_context.warm_start_tracking else []}'
                    )
        
        is_batch_verification = context_data.get('is_batch_verification', False)
        
        if hasattr(self.current_context, 'force_format_check_tables') and table_name in self.current_context.force_format_check_tables:
        if hasattr(self.current_context, 'force_format_check_tables') and table_name in self.current_context.force_format_check_tables:
            format_violations = [v for v in violations if v.constraint_type == 'FORMAT']
            if not format_violations:
                self.current_context.force_format_check_tables.remove(table_name)
            else:
                self.current_context.force_format_check_tables.remove(table_name)
        
        if self.current_context:
            if table_name not in self.current_context.all_violations:
                self.current_context.all_violations[table_name] = []
            self.current_context.all_violations[table_name].extend(violations)
            
            if snapshot:
                if hasattr(self.current_context, 'batch_merged_tables') and table_name in self.current_context.batch_merged_tables:
                    merged_row_count = self.current_context.batch_merged_tables[table_name]
                    
                    if len(snapshot.rows) < merged_row_count:
                        self.logger.warning(
                            f'æ£€æµ‹åˆ°éªŒè¯åsnapshot ({len(snapshot.rows)}è¡Œ) å°‘äºbatchåˆå¹¶ç»“æœ ({merged_row_count}è¡Œ)ï¼Œ'
                            f'è¿™å¯èƒ½æ˜¯å•ä¸ªbatchçš„éªŒè¯ï¼Œæ‹’ç»è¦†ç›–åˆå¹¶ç»“æœã€‚'
                        )
                        
                        if table_name in self.current_context.all_snapshots:
                            existing_snapshot = self.current_context.all_snapshots[table_name]
                            snapshot = existing_snapshot
                            self.logger.info(f'å·²ä¿æŒbatchåˆå¹¶ç»“æœçš„å®Œæ•´æ€§ï¼Œç»§ç»­ä½¿ç”¨ {len(snapshot.rows)} è¡Œæ•°æ®')
                
                snapshot.processing_stage = 'verification'
                
                if is_batch_verification:
                    snapshot.stage_description = f'BatchéªŒè¯å®Œæˆ - è¡¨ {table_name}ï¼Œå‘ç° {len(violations)} ä¸ªè´¨é‡é—®é¢˜'
                else:
                    should_update_all_snapshots = True
                    
                    if hasattr(self.current_context, 'batch_merged_tables') and table_name in self.current_context.batch_merged_tables:
                        merged_row_count = self.current_context.batch_merged_tables[table_name]
                        if len(snapshot.rows) < merged_row_count:
                            should_update_all_snapshots = False
                            self.logger.warning(
                                f' è·³è¿‡æ›´æ–°all_snapshotsï¼šéªŒè¯åsnapshot ({len(snapshot.rows)}è¡Œ) < batchåˆå¹¶ç»“æœ ({merged_row_count}è¡Œ)ï¼Œä¿æŠ¤batchåˆå¹¶æ•°æ®'
                            )
                    
                    if should_update_all_snapshots:
                        snapshot.stage_description = f'æ•°æ®éªŒè¯å®Œæˆ - è¡¨ {table_name}ï¼Œå‘ç° {len(violations)} ä¸ªè´¨é‡é—®é¢˜'
                        self.current_context.all_snapshots[table_name] = snapshot
                    
                self.current_context.io_manager.append_snapshot(snapshot)
                
                self._sync_current_snapshot_to_context()
        
        error_violations = [v for v in violations if getattr(v, 'severity', 'warn') == 'error']
        warn_violations = [v for v in violations if getattr(v, 'severity', 'warn') == 'warn']
        
        
        
        if not violations:
            self.logger.info(f' è¡¨ {table_name} éªŒè¯å®Œæˆä¸”æ— è¿è§„ï¼Œæ ‡è®°ä¸ºå®Œæˆ')
            
            if is_batch_verification and batch_index is not None:
                if hasattr(self, 'coordinator'):
                    is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False)
                    self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start=is_warm_start)
                    self.logger.info(f' Batch {batch_index} æ— è¿è§„ï¼Œå·²æ ‡è®°ä¸ºfinal{" (warm start)" if is_warm_start else ""}')
                return  # batchå®Œæˆï¼Œè¿”å›
            
            if hasattr(self, 'coordinator'):
                from .signal_coordinator import TableLifecycleState
                self.coordinator.update_table_state(table_name, TableLifecycleState.FINAL)
            self._set_table_force_completed(table_name)
            return
        
        total_fixes = len(self.current_context.all_fixes.get(table_name, [])) if self.current_context else 0
        error_count = len([v for v in violations if getattr(v, 'severity', 'warn') == 'error'])
        warn_count = len([v for v in violations if getattr(v, 'severity', 'warn') == 'warn'])
        
        if error_count == 0 and warn_count > 0:
            if total_fixes >= 1:  #  å·²ç»ä¿®å¤è¿‡ï¼Œå¯ä»¥å®Œæˆ
                if is_batch_verification and batch_index is not None:
                    if hasattr(self, 'coordinator'):
                        is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False)
                        self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start=is_warm_start)
                        self.logger.info(f' Batch {batch_index} åªæœ‰warningä¸”å·²ä¿®å¤ï¼Œå·²æ ‡è®°ä¸ºfinal{" (warm start)" if is_warm_start else ""}')
                    return  # batchå®Œæˆï¼Œè¿”å›
                self._set_table_force_completed(table_name)
                return
        
        try:
            await asyncio.wait_for(
                self._detect_and_mark_repeated_violations(violations, table_name),
                timeout=10.0  # 10ç§’è¶…æ—¶
            )
        except asyncio.TimeoutError:
            self.logger.warning(f' æ£€æµ‹é‡å¤è¿è§„è¶…æ—¶ï¼ˆè¡¨ {table_name}ï¼‰ï¼Œè·³è¿‡æ­¤æ­¥éª¤')
        except Exception as e:
            self.logger.error(f' æ£€æµ‹é‡å¤è¿è§„å¤±è´¥ï¼ˆè¡¨ {table_name}ï¼‰: {e}')
        
        try:
            filtered_violations = await asyncio.wait_for(
                self._filter_unfixable_violations(violations, table_name),
                timeout=10.0  # 10ç§’è¶…æ—¶
            )
        except asyncio.TimeoutError:
            self.logger.warning(f' è¿‡æ»¤æ— æ³•ä¿®å¤çš„è¿è§„è¶…æ—¶ï¼ˆè¡¨ {table_name}ï¼‰ï¼Œä½¿ç”¨åŸå§‹è¿è§„åˆ—è¡¨')
            filtered_violations = violations
        except Exception as e:
            self.logger.error(f' è¿‡æ»¤æ— æ³•ä¿®å¤çš„è¿è§„å¤±è´¥ï¼ˆè¡¨ {table_name}ï¼‰: {e}')
            filtered_violations = violations
        if len(filtered_violations) < len(violations):
            removed_count = len(violations) - len(filtered_violations)
            self.logger.info(f"è¿‡æ»¤äº† {removed_count} ä¸ªæ— æ³•ä¿®å¤çš„è¿è§„")
        
        violations = filtered_violations
        
        if not violations:
            self.logger.info(f' è¿‡æ»¤åæ— è¿è§„ï¼Œå‡†å¤‡æ ‡è®°å®Œæˆ - è¡¨: {table_name}, is_batch_verification={is_batch_verification}, batch_index={batch_index}')
            if is_batch_verification and batch_index is not None:
                self.logger.info(f'  â†’ è¿™æ˜¯batchéªŒè¯ï¼Œå°†è°ƒç”¨mark_batch_final')
                if hasattr(self, 'coordinator'):
                    is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False)
                    self.logger.info(f'  â†’ å‚æ•°: table_name={table_name}, batch_index={batch_index}, is_warm_start={is_warm_start}')
                    self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start=is_warm_start)
                    self.logger.info(f' Batch {batch_index} è¿‡æ»¤åæ— è¿è§„ï¼Œå·²æ ‡è®°ä¸ºfinal{" (warm start)" if is_warm_start else ""}')
                return  # batchå®Œæˆï¼Œè¿”å›
            else:
                self.logger.info(f'  â†’ è¿™æ˜¯è¡¨çº§éªŒè¯ï¼ˆis_batch_verification={is_batch_verification}, batch_index={batch_index}ï¼‰ï¼Œå°†è°ƒç”¨_set_table_force_completed')
            self._set_table_force_completed(table_name)
            return
        
        if hasattr(self.current_context, 'coordinator'):
            coordinator = self.current_context.coordinator
        else:
            self.logger.warning(' Context æ²¡æœ‰ coordinatorï¼Œæ— æ³•è¿›è¡Œå¾ªç¯æ§åˆ¶æ£€æŸ¥')
            coordinator = None
        
        tool_fixable = self._get_tool_fixable_violations(violations)
        requires_reextraction = self._get_reextraction_violations(violations)
        
        if coordinator:
            tracker = coordinator.get_table_tracker(table_name)
            current_round = tracker.verification_round if tracker else 0
            max_rounds = tracker.max_verification_rounds if tracker else 2
            self.logger.info(
                f' è¡¨ {table_name} éªŒè¯å®Œæˆåˆ†æ: '
                f'éªŒè¯è½®æ¬¡={current_round}/{max_rounds}, '
                f'å·¥å…·ä¿®å¤={len(tool_fixable)}, '
                f'éœ€è¦é‡æå–={len(requires_reextraction)}, '
                f'æ€»è¿è§„={len(violations)}'
            )
        else:
            self.logger.info(
                f'è¿è§„åˆ†ç±»: å·¥å…·ä¿®å¤={len(tool_fixable)}, éœ€è¦é‡æå–={len(requires_reextraction)}'
            )
        
        if tool_fixable:
            check_batch_index = batch_index if is_batch_verification else None
            
            self.logger.info(
                f' å¾ªç¯æ§åˆ¶æ£€æŸ¥: è¡¨={table_name}, '
                f'is_batch_verification={is_batch_verification}, '
                f'batch_index={batch_index}, '
                f'check_batch_index={check_batch_index}'
            )
            
            if coordinator and not coordinator.can_verify_fix_iterate(table_name, check_batch_index):
                self.logger.warning(
                    f' è¾¾åˆ°æœ€å¤§éªŒè¯-ä¿®å¤è½®æ¬¡ï¼Œåœæ­¢ä¿®å¤ - è¡¨ {table_name}, '
                    f'å‰©ä½™ {len(tool_fixable)} ä¸ªå¯ä¿®å¤è¿è§„å°†è¢«å¿½ç•¥'
                )
                
                if requires_reextraction:
                    self.logger.info(
                        f' è™½ç„¶è¾¾åˆ°æœ€å¤§ä¿®å¤è½®æ¬¡ï¼Œä½†ä»æœ‰ {len(requires_reextraction)} ä¸ªéœ€è¦é‡æå–çš„è¿è§„ï¼Œ'
                        f'å°è¯•è§¦å‘ Warm Start'
                    )
                    
                    if coordinator and coordinator.can_warm_start(table_name, check_batch_index):
                        coordinator.increment_warm_start_attempts(table_name, check_batch_index)
                        
                        if is_batch_verification and batch_index is not None:
                            self._mark_final(table_name, batch_index)
                        
                        if is_batch_verification:
                            batch_document_names = context_data.get('batch_document_names', [])
                            text_contents = context_data.get('text_contents', [])
                            batch_total = context_data.get('batch_total', 1)
                            
                            if batch_document_names and text_contents:
                                self.logger.info(
                                    f' Batch {batch_index}/{batch_total} warm start: ä½¿ç”¨å½“å‰batchçš„æ–‡æ¡£ {batch_document_names}'
                                )
                                await self._trigger_batch_warm_start_extraction(
                                    table_name, requires_reextraction, snapshot, 
                                    text_contents, batch_document_names, batch_index, batch_total
                                )
                            else:
                                self.logger.warning(f' Batch {batch_index} ç¼ºå°‘æ–‡æ¡£ä¿¡æ¯ï¼Œè·³è¿‡warm start')
                                self._mark_final(table_name, batch_index)
                        else:
                            await self._trigger_smart_warm_start_extraction(
                                table_name, requires_reextraction, snapshot
                            )
                        return
                    else:
                        self.logger.warning(
                            f' æ— æ³•è§¦å‘ Warm Startï¼ˆå·²è¾¾æœ€å¤§å°è¯•æ¬¡æ•°æˆ–å…¶ä»–é™åˆ¶ï¼‰- è¡¨ {table_name}'
                        )
                
                self._mark_final(table_name, batch_index)
                return
            
            self.logger.info(f' å‡†å¤‡ä¿®å¤ {len(tool_fixable)} ä¸ªè¿è§„...')
            
            if coordinator:
                coordinator.increment_verify_fix_iteration(table_name, check_batch_index)
            
            await self._signal_fix_data(table_name, tool_fixable, snapshot, context_data)
            return  # ä¿®å¤å®Œæˆåä¼šè‡ªåŠ¨è§¦å‘é‡æ–°éªŒè¯
        
        if requires_reextraction:
            self.logger.info(f' æ£€æµ‹åˆ° {len(requires_reextraction)} ä¸ªéœ€è¦é‡æå–çš„è¿è§„')
            
            check_batch_index = batch_index if is_batch_verification else None
            
            if coordinator and not coordinator.can_warm_start(table_name, check_batch_index):
                self.logger.warning(
                    f' è¾¾åˆ°æœ€å¤§ Warm Start å°è¯•æ¬¡æ•°ï¼Œåœæ­¢é‡æå– - è¡¨ {table_name}'
                )
                self._mark_final(table_name, batch_index)
                return
            
            if coordinator:
                coordinator.increment_warm_start_attempts(table_name, check_batch_index)
            
            if is_batch_verification and batch_index is not None:
                self._mark_final(table_name, batch_index)
            
            if is_batch_verification:
                batch_document_names = context_data.get('batch_document_names', [])
                text_contents = context_data.get('text_contents', [])
                batch_total = context_data.get('batch_total', 1)
                
                if batch_document_names and text_contents:
                    self.logger.info(
                        f' Batch {batch_index}/{batch_total} warm start: ä½¿ç”¨å½“å‰batchçš„æ–‡æ¡£ {batch_document_names}'
                    )
                    await self._trigger_batch_warm_start_extraction(
                        table_name, requires_reextraction, snapshot, 
                        text_contents, batch_document_names, batch_index, batch_total
                    )
                else:
                    self.logger.warning(f' Batch {batch_index} ç¼ºå°‘æ–‡æ¡£ä¿¡æ¯ï¼Œè·³è¿‡warm start')
                    self._mark_final(table_name, batch_index)
            else:
                await self._trigger_smart_warm_start_extraction(
                    table_name, requires_reextraction, snapshot
                )
            return
        
        self.logger.info(
            f' æ— å¯å¤„ç†çš„violationsï¼Œæ ‡è®°å®Œæˆ - è¡¨ {table_name}, '
            f'å‰©ä½™ {len(violations)} ä¸ªè¿è§„ï¼ˆæ— æ³•ä¿®å¤æˆ–éœ€è¦äººå·¥å¤„ç†ï¼‰'
        )
        self._mark_final(table_name, batch_index)
    
    async def handle_fixing_complete(self, signal):
        """å¤„ç†ä¿®å¤å®Œæˆä¿¡å·"""
        data = signal.data
        table_name = data.get('table_name')
        fixes = data.get('fixes', [])
        
        if self.current_context:
            current_state = getattr(self.current_context, 'current_state', None)
            if current_state and str(current_state).endswith('COMPLETED'):
                self.logger.warning(f'â›” ä»»åŠ¡å·²å®Œæˆï¼ˆçŠ¶æ€ï¼š{current_state}ï¼‰ï¼Œå¿½ç•¥æ¥è‡ªè¡¨ {table_name} çš„ä¿®å¤å®Œæˆä¿¡å·')
                return
        
        if hasattr(self, 'coordinator') and self.coordinator:
            from .signal_coordinator import TableLifecycleState
            if self.coordinator.is_table_in_state(table_name, TableLifecycleState.FINAL):
                self.logger.warning(f'â›” è¡¨ {table_name} å·²æ ‡è®°ä¸ºFINALï¼Œå¿½ç•¥ä¿®å¤å®Œæˆä¿¡å·')
                return
        
        snapshot = data.get('snapshot')
        
        context_data = data.get('context', {})
        is_batch_verification = context_data.get('is_batch_verification', False)
        batch_index = context_data.get('batch_index')
        
        
        if self.current_context and fixes:
            fix_details = {
                'total_fixes': len(fixes),
                'fixes_by_type': {},
                'fixes_summary': []
            }
            
            for fix in fixes:
                fix_type = getattr(fix, 'fix_type', 'unknown')
                if fix_type not in fix_details['fixes_by_type']:
                    fix_details['fixes_by_type'][fix_type] = 0
                fix_details['fixes_by_type'][fix_type] += 1
                
                fix_details['fixes_summary'].append({
                    'id': getattr(fix, 'id', 'unknown'),
                    'tuple_id': getattr(fix, 'tuple_id', 'unknown'),
                    'attr': getattr(fix, 'attr', 'unknown'),
                    'old': getattr(fix, 'old', 'null'),
                    'new': getattr(fix, 'new', 'null'),
                    'fix_type': fix_type
                })
            
            fix_step = {
                'step': f'fixer_{table_name}_completed',
                'step_name': f'fixer_{table_name}_completed',
                'description': f'è¡¨ {table_name} æ•°æ®ä¿®å¤å®Œæˆï¼Œåº”ç”¨äº† {len(fixes)} ä¸ªä¿®å¤',
                'status': 'completed',
                'timestamp': self.current_context.io_manager.get_timestamp(),
                'details': fix_details
            }
            
            self.current_context.step_outputs.append(fix_step)
        
        if self.current_context:
            if table_name not in self.current_context.all_fixes:
                self.current_context.all_fixes[table_name] = []
            self.current_context.all_fixes[table_name].extend(fixes)
            
            if len(fixes) == 0:
                self.logger.warning(f' ä¿®å¤å¤±è´¥ï¼ˆ0ä¸ªä¿®å¤ï¼‰ï¼Œä¿ç•™åŸå§‹æ•°æ®å¹¶è®¾ç½®è¡¨ {table_name} å¼ºåˆ¶å®Œæˆ')
                if snapshot and table_name in self.current_context.all_snapshots:
                    self.logger.info(f' ä¿ç•™è¡¨ {table_name} çš„åŸå§‹æ•°æ® ({len(self.current_context.all_snapshots[table_name].rows)}è¡Œ)')
                
                if is_batch_verification and batch_index is not None:
                    if hasattr(self, 'coordinator'):
                        is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False)
                        self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start=is_warm_start)
                        self.logger.info(f' Batch {batch_index} ä¿®å¤å¤±è´¥ï¼Œå·²æ ‡è®°ä¸ºfinal{" (warm start)" if is_warm_start else ""}')
                    return  # batchå®Œæˆï¼Œè¿”å›
                    
                self._set_table_force_completed(table_name)
                return
            
            if snapshot:
                should_update_snapshot = True
                
                if hasattr(self.current_context, 'batch_merged_tables') and table_name in self.current_context.batch_merged_tables:
                    merged_row_count = self.current_context.batch_merged_tables[table_name]
                    
                    if len(snapshot.rows) < merged_row_count:
                        self.logger.warning(
                            f' æ£€æµ‹åˆ°ä¿®å¤åsnapshot ({len(snapshot.rows)}è¡Œ) å°‘äºbatchåˆå¹¶ç»“æœ ({merged_row_count}è¡Œ)ï¼'
                            f'å°†åº”ç”¨ä¿®å¤åˆ°å®Œæ•´çš„batchåˆå¹¶æ•°æ®ä¸Šï¼Œè€Œä¸æ˜¯è¦†ç›–ã€‚'
                        )
                        
                        if table_name in self.current_context.all_snapshots:
                            existing_snapshot = self.current_context.all_snapshots[table_name]
                            
                            snapshot_dict = {getattr(row, 'tuple_id', None): row for row in snapshot.rows if hasattr(row, 'tuple_id')}
                            
                            updated_count = 0
                            for i, row in enumerate(existing_snapshot.rows):
                                tuple_id = getattr(row, 'tuple_id', None)
                                if tuple_id and tuple_id in snapshot_dict:
                                    existing_snapshot.rows[i] = snapshot_dict[tuple_id]
                                    updated_count += 1
                                    self.logger.debug(f'  æ›´æ–°row: {tuple_id}')
                            
                            snapshot = existing_snapshot
                            self.logger.info(
                                f' å·²å°† {updated_count} ä¸ªä¿®å¤åº”ç”¨åˆ°batchåˆå¹¶ç»“æœä¸Šï¼Œ'
                                f'ä¿æŒå®Œæ•´æ€§ï¼Œæœ€ç»ˆ {len(snapshot.rows)} è¡Œæ•°æ®'
                            )
                            should_update_snapshot = True  # éœ€è¦æ›´æ–°ï¼Œå› ä¸ºæˆ‘ä»¬ä¿®æ”¹äº†existing_snapshot
                
                if should_update_snapshot:
                    self.current_context.all_snapshots[table_name] = snapshot
                snapshot.processing_stage = 'fixing'
                snapshot.stage_description = f'æ•°æ®ä¿®å¤å®Œæˆ - è¡¨ {table_name}ï¼Œåº”ç”¨äº† {len(fixes)} ä¸ªä¿®å¤'
                self.current_context.io_manager.append_snapshot(snapshot)
                
                self._sync_current_snapshot_to_context()
                
                if self._should_skip_reverification(table_name, len(fixes)):
                    self.logger.info(f' è·³è¿‡é‡æ–°éªŒè¯ï¼Œè¡¨ {table_name} å·²æ ‡è®°å®Œæˆ')
                    
                    if is_batch_verification and batch_index is not None:
                        if hasattr(self, 'coordinator'):
                            is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False)
                            self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start=is_warm_start)
                            self.logger.info(f' Batch {batch_index} ä¿®å¤å®Œæˆä¸”æ— éœ€é‡æ–°éªŒè¯ï¼Œå·²æ ‡è®°ä¸ºfinal{" (warm start)" if is_warm_start else ""}')
                        return  # batchå®Œæˆï¼Œè¿”å›
                    
                    if hasattr(self, 'coordinator') and self.coordinator:
                        from .signal_coordinator import TableLifecycleState
                        self.coordinator.update_table_state(table_name, TableLifecycleState.FINAL)
                    self._set_table_force_completed(table_name)
                else:
                    check_batch_index = batch_index if is_batch_verification else None
                    
                    if hasattr(self, 'coordinator') and self.coordinator:
                        if not self.coordinator.can_verify_fix_iterate(table_name, check_batch_index):
                            self.logger.warning(
                                f' ä¿®å¤å®Œæˆï¼Œä½†å·²è¾¾åˆ°æœ€å¤§éªŒè¯-ä¿®å¤è½®æ¬¡ï¼Œåœæ­¢é‡æ–°éªŒè¯ - è¡¨ {table_name}'
                            )
                            if is_batch_verification and batch_index is not None:
                                is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False)
                                self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start=is_warm_start)
                                self.logger.info(f' Batch {batch_index} å·²è¾¾æœ€å¤§è½®æ¬¡ï¼Œæ ‡è®°ä¸ºfinal{" (warm start)" if is_warm_start else ""}')
                            else:
                                from .signal_coordinator import TableLifecycleState
                                self.coordinator.update_table_state(table_name, TableLifecycleState.FINAL)
                                self._set_table_force_completed(table_name)
                            return  # ä¸å†è§¦å‘é‡æ–°éªŒè¯
                    
                    self.logger.info(f' è§¦å‘é‡æ–°éªŒè¯ï¼Œè¡¨ {table_name}ï¼Œå·²åº”ç”¨ {len(fixes)} ä¸ªä¿®å¤')
                    await self._signal_verify_data(table_name, [snapshot], context_data)
            else:
                self.logger.warning(f' ä¿®å¤å®Œæˆä½†snapshotä¸ºç©ºï¼Œè®¾ç½®è¡¨ {table_name} å¼ºåˆ¶å®Œæˆ')
                
                if is_batch_verification and batch_index is not None:
                    if hasattr(self, 'coordinator'):
                        is_warm_start = context_data.get('warm_start', False) or context_data.get('is_warm_start', False)
                        self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start=is_warm_start)
                        self.logger.info(f' Batch {batch_index} snapshotä¸ºç©ºï¼Œå·²æ ‡è®°ä¸ºfinal{" (warm start)" if is_warm_start else ""}')
                    return  # batchå®Œæˆï¼Œè¿”å›
                    
                self._set_table_force_completed(table_name)
    
    def _sync_current_snapshot_to_context(self):
        """å®æ—¶åŒæ­¥å½“å‰å¿«ç…§æ•°æ®åˆ°context.snapshotsï¼Œç¡®ä¿æœåŠ¡å±‚èƒ½ç«‹å³è¯»å–"""
        if not self.current_context or not self.current_context.all_snapshots:
            return
            
        self.current_context.snapshots = list(self.current_context.all_snapshots.values())
    
    async def handle_component_error(self, signal):
        """å¤„ç†ç»„ä»¶é”™è¯¯ä¿¡å·"""
        data = signal.data
        error = data.get('error')
        table_name = data.get('table_name', 'unknown')
        
        
        if hasattr(self, 'logger'):
            self.logger.error(f"ç»„ä»¶é”™è¯¯ - è¡¨ {table_name}: {error}")
        
    
    async def _trigger_smart_warm_start_extraction(self, table_name: str, violations_requiring_reextraction, snapshot):
        """è§¦å‘ Warm Start é‡æå– - é€šè¿‡ä¿¡å·ç³»ç»Ÿå‘é€è¯·æ±‚ç»™ Orchestrator
        
        æ–°æ¶æ„ï¼šä¸å†ç›´æ¥è°ƒç”¨ orchestrator æ–¹æ³•ï¼Œè€Œæ˜¯å‘é€ WARM_START_REQUEST ä¿¡å·
        """
        self.logger.info(f" å‘é€ Warm Start è¯·æ±‚: {table_name}, {len(violations_requiring_reextraction)} ä¸ªè¿è§„")
        
        if self.current_context and hasattr(self.current_context, 'step_outputs'):
            from ...core.io import IOManager
            warm_start_step = {
                'step': f'warm_start_extraction_{table_name}',
                'step_name': f'warm_start_extraction_{table_name}',
                'status': 'in_progress',
                'description': f'Warm Start: æ£€æµ‹åˆ° {len(violations_requiring_reextraction)} ä¸ªéœ€è¦é‡æ–°æå–çš„è¿è§„',
                'details': {
                    'table_name': table_name,
                    'violations_count': len(violations_requiring_reextraction),
                    'violation_types': list(set([v.constraint_type for v in violations_requiring_reextraction])),
                    'mode': 'warm_start_extraction'
                },
                'timestamp': IOManager.get_timestamp()
            }
            self.current_context.step_outputs.append(warm_start_step)
        
        try:
            from ...signals.core import SignalType
            
            signal_data = {
                'table_name': table_name,
                'violations': violations_requiring_reextraction,
                'snapshot': snapshot,
                'run_id': self.current_context.run_id if self.current_context else None,
                'schema': self.current_context.schema if self.current_context else None
            }
            
            await self.broadcaster.emit_simple_signal(
                SignalType.WARM_START_REQUEST,
                data=signal_data,
                correlation_id=f"{signal_data['run_id']}_{table_name}_warmstart"
            )
            
            self.logger.info(f" Warm Start è¯·æ±‚å·²å‘é€ï¼Œç­‰å¾… Orchestrator å¤„ç†")
            
        except Exception as e:
            self.logger.error(f"å‘é€ Warm Start è¯·æ±‚å¤±è´¥: {e}", exc_info=True)
            
            if self.current_context and hasattr(self.current_context, 'step_outputs'):
                from ...core.io import IOManager
                for step in self.current_context.step_outputs:
                    if step.get('step') == f'warm_start_extraction_{table_name}' and step.get('status') == 'in_progress':
                        step['status'] = 'failed'
                        step['description'] = f'Warm Start è¯·æ±‚å¤±è´¥: {str(e)}'
                        step['details']['error'] = str(e)
                        step['timestamp_completed'] = IOManager.get_timestamp()
                        break
    
    def _should_skip_reverification(self, table_name: str, fixes_applied: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡é‡æ–°éªŒè¯ä»¥é¿å…å¾ªç¯
        
        ç®€åŒ–é€»è¾‘ï¼š
        1. å¦‚æœæ²¡æœ‰åº”ç”¨ä»»ä½•ä¿®å¤ï¼Œè·³è¿‡
        2. å¦‚æœæ˜¯åŸºç¡€ä¿®å¤ï¼Œä¸è·³è¿‡ï¼ˆéœ€è¦å®Œæ•´éªŒè¯ï¼‰
        3. å¦åˆ™ï¼Œæ€»æ˜¯è§¦å‘é‡æ–°éªŒè¯ï¼ˆè®© coordinator æ§åˆ¶å¾ªç¯æ¬¡æ•°ï¼‰
        """
        
        if fixes_applied == 0:
            self.logger.info(f"è·³è¿‡é‡æ–°éªŒè¯ï¼šæœªåº”ç”¨ä»»ä½•ä¿®å¤")
            return True
        
        if hasattr(self.current_context, 'basic_fix_tables') and table_name in self.current_context.basic_fix_tables:
            self.current_context.basic_fix_tables.remove(table_name)
            self.logger.info(f"ä¸è·³è¿‡é‡æ–°éªŒè¯ï¼šåŸºç¡€ä¿®å¤å®Œæˆï¼Œéœ€è¦å®Œæ•´éªŒè¯")
            return False
        
        self.logger.info(f"ä¸è·³è¿‡é‡æ–°éªŒè¯ï¼šå·²åº”ç”¨ {fixes_applied} ä¸ªä¿®å¤")
        return False
    
    async def _detect_and_mark_repeated_violations(self, violations: List, table_name: str):
        """
        æ£€æµ‹å¹¶æ ‡è®°é‡å¤è¿è§„ï¼šå•å…ƒæ ¼å·²ç»è¢«ä¿®å¤è¿‡ä½†violationè¿˜åœ¨ï¼Œè¯´æ˜ä¿®å¤å¤±è´¥
        """
        if not violations:
            return
        
        try:
            if not hasattr(self, 'memory_manager') or not self.memory_manager:
                return
            
            run_id = getattr(self.current_context, 'run_id', None) if hasattr(self, 'current_context') else None
            if not run_id:
                return
            
            total_fixes = len(self.current_context.all_fixes.get(table_name, [])) if self.current_context else 0
            if total_fixes == 0:
                return
            
            repeated_violations = await self.memory_manager.check_repeated_violations(
                violations, run_id, table_name
            )
            
            if repeated_violations:
                pass  # Auto-fixed empty block
                
                for violation in repeated_violations:
                    try:
                        await self.memory_manager.mark_violation_unfixable(violation.id, run_id)
                        self.logger.info(f"æ ‡è®°é‡å¤è¿è§„ä¸ºunfixable: {violation.id}")
                    except Exception as mark_error:
                        self.logger.error(f"æ ‡è®°è¿è§„ {violation.id} ä¸ºunfixableå¤±è´¥: {mark_error}")
                
        except Exception as e:
            self.logger.error(f"æ£€æµ‹é‡å¤è¿è§„å¤±è´¥: {e}")
    
    async def _filter_unfixable_violations(self, violations: List, table_name: str) -> List:
        """è¿‡æ»¤æ‰å·²æ ‡è®°ä¸ºæ— æ³•ä¿®å¤çš„è¿è§„"""
        if not violations:
            return violations
        
        try:
            if not hasattr(self, 'memory_manager') or not self.memory_manager:
                return violations
            
            run_id = getattr(self.current_context, 'run_id', None) if hasattr(self, 'current_context') else None
            if not run_id:
                return violations
            
            unfixable_ids = await self.memory_manager.get_unfixable_violations(run_id)
            
            if unfixable_ids:
                for vid in unfixable_ids:
                    pass  # Auto-fixed empty block
            else:
                pass  # Auto-fixed empty block
            
            filtered = [v for v in violations if v.id not in unfixable_ids]
            
            removed_count = len(violations) - len(filtered)
            if removed_count > 0:
                for v in violations:
                    if v.id in unfixable_ids:
                        pass  # Auto-fixed empty block
            
            return filtered
            
        except Exception as e:
            self.logger.error(f"è¿‡æ»¤unfixable violationså¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return violations
    
    def _extract_relevant_documents(self, violations: List, snapshot) -> List[str]:
        """ä»violationsä¸­æå–ç›¸å…³æ–‡æ¡£ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå¤šç§åŒ¹é…ç­–ç•¥ï¼‰
        
        ç­–ç•¥ä¼˜å…ˆçº§ï¼š
        1. ä¼˜å…ˆä½¿ç”¨ located_segments ç¼“å­˜ä¿¡æ¯ï¼ˆæœ€å‡†ç¡®ï¼‰
        2. ä» evidences ä¸­æå–æ–‡æ¡£åï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
        3. å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆå›é€€åˆ°ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£ï¼‰
        
        Args:
            violations: è¿è§„åˆ—è¡¨
            snapshot: è¡¨å¿«ç…§
            
        Returns:
            ç›¸å…³æ–‡æ¡£åç§°åˆ—è¡¨ï¼ˆå¦‚ ['2024-CIRTRAN CORP-j.txt', ...]ï¼‰
        """
        if not violations or not snapshot or not hasattr(snapshot, 'rows'):
            self.logger.debug('violationsæˆ–snapshotä¸ºç©ºï¼Œè¿”å›ç©ºæ–‡æ¡£åˆ—è¡¨')
            return []
        
        relevant_docs = set()
        violation_tuple_ids = set([v.tuple_id for v in violations])
        
        if hasattr(self, 'current_context') and self.current_context:
            if hasattr(self.current_context, 'located_segments') and self.current_context.located_segments:
                segment_docs = set([
                    seg.source_document for seg in self.current_context.located_segments 
                    if hasattr(seg, 'source_document') and seg.source_document
                ])
                if segment_docs:
                    relevant_docs.update(segment_docs)
                    self.logger.info(f' ç­–ç•¥1ï¼šä»located_segmentsç¼“å­˜ä¸­æå–åˆ° {len(segment_docs)} ä¸ªæ–‡æ¡£ï¼š{segment_docs}')
        
        evidence_based_docs = set()
        for row in snapshot.rows:
            if row.tuple_id in violation_tuple_ids:
                for cell_name, cell_data in row.cells.items():
                    if hasattr(cell_data, 'evidences') and cell_data.evidences:
                        for evidence in cell_data.evidences:
                            import os
                            doc_name = os.path.basename(evidence) if evidence else None
                            
                            if doc_name and ('.' in doc_name or doc_name.endswith('-j.txt')):
                                evidence_based_docs.add(doc_name)
        
        if evidence_based_docs:
            relevant_docs.update(evidence_based_docs)
            self.logger.info(f' ç­–ç•¥2ï¼šä»evidencesä¸­æå–åˆ° {len(evidence_based_docs)} ä¸ªæ–‡æ¡£ï¼š{evidence_based_docs}')
        
        if not relevant_docs and hasattr(self, 'current_context') and self.current_context:
            if hasattr(self.current_context, 'documents') and self.current_context.documents:
                all_evidences = set()
                for row in snapshot.rows:
                    if row.tuple_id in violation_tuple_ids:
                        for cell_name, cell_data in row.cells.items():
                            if hasattr(cell_data, 'evidences') and cell_data.evidences:
                                all_evidences.update(cell_data.evidences)
                
                import os
                actual_docs = [os.path.basename(d) for d in self.current_context.documents]
                for evidence in all_evidences:
                    for actual_doc in actual_docs:
                        if evidence.lower() in actual_doc.lower() or actual_doc.lower() in evidence.lower():
                            relevant_docs.add(actual_doc)
                
                if relevant_docs:
                    self.logger.info(f' ç­–ç•¥3ï¼šæ¨¡ç³ŠåŒ¹é…æå–åˆ° {len(relevant_docs)} ä¸ªæ–‡æ¡£ï¼š{relevant_docs}')
        
        result = list(relevant_docs)
        if not result:
            self.logger.warning(f' æœªèƒ½æå–åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå°†å›é€€åˆ°ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£')
        else:
            self.logger.info(f' æœ€ç»ˆè¯†åˆ«åˆ° {len(result)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š{result}')
        
        return result
    
    def _get_tool_fixable_violations(self, violations: List[Violation]) -> List[Violation]:
        """æå–å¯å·¥å…·ä¿®å¤çš„è¿è§„
        
        å¯å·¥å…·ä¿®å¤çš„è¿è§„ç‰¹å¾ï¼š
        - processing_category åŒ…å« 'tool_fixable' æˆ– 'fixable'
        - æˆ–è€…çº¦æŸç±»å‹åœ¨å¯ä¿®å¤åˆ—è¡¨ä¸­ï¼ˆFORMAT, TYPE, VALUEç­‰ï¼‰
        """
        tool_fixable = []
        for v in violations:
            category = getattr(v, 'processing_category', None)
            if category:
                category_str = category.value if hasattr(category, 'value') else str(category)
                if 'tool_fixable' in category_str.lower() or 'fixable' in category_str.lower():
                    tool_fixable.append(v)
                    continue
            
            constraint_type = getattr(v, 'constraint_type', '').upper()
            if constraint_type in ['FORMAT', 'TYPE', 'VALUE', 'LOGIC', 'AGGREGATION']:
                tool_fixable.append(v)
        
        return tool_fixable
    
    def _get_reextraction_violations(self, violations: List[Violation]) -> List[Violation]:
        """æå–éœ€è¦é‡æå–çš„è¿è§„
        
        éœ€è¦é‡æå–çš„è¿è§„ç‰¹å¾ï¼š
        - processing_category åŒ…å« 'reextraction' æˆ– 'reextract'
        - æˆ–è€…æ˜¯ç©ºå€¼ã€ç¼ºå¤±å€¼ç­‰æ— æ³•å·¥å…·ä¿®å¤çš„é—®é¢˜
        """
        reextraction = []
        for v in violations:
            category = getattr(v, 'processing_category', None)
            if category:
                category_str = category.value if hasattr(category, 'value') else str(category)
                if 'reextract' in category_str.lower():
                    reextraction.append(v)
        
        return reextraction
    
    def _mark_final(self, table_name: str, batch_index: Optional[int] = None):
        """ç»Ÿä¸€çš„æ ‡è®°å®Œæˆæ–¹æ³•
        
        Args:
            table_name: è¡¨å
            batch_index: batchç´¢å¼•ï¼ˆNone è¡¨ç¤ºè¡¨çº§ï¼‰
        """
        if batch_index is not None:
            if hasattr(self, 'coordinator') and self.coordinator:
                is_warm_start = context_data.get('is_warm_start', False) if context_data else False
                self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start)
                self.logger.info(f' Batch {batch_index} æ ‡è®°ä¸º final')
        else:
            self._set_table_force_completed(table_name)
    
    def _set_table_force_completed(self, table_name: str):
        """è®¾ç½®è¡¨å¼ºåˆ¶å®Œæˆæ ‡è®°"""
        if not hasattr(self.current_context, 'force_completed_tables'):
            self.current_context.force_completed_tables = set()
        
        self.current_context.force_completed_tables.add(table_name)
        
        if hasattr(self, 'coordinator') and self.coordinator:
            from .signal_coordinator import TableLifecycleState
            self.coordinator.update_table_state(table_name, TableLifecycleState.FINAL)
            self.logger.info(f' è¡¨ {table_name} æ ‡è®°ä¸ºFINAL')
            
            tracker = self.coordinator.get_table_tracker(table_name)
            if tracker and hasattr(tracker, 'batch_trackers') and tracker.batch_trackers:
                self.logger.info(f'   å¼€å§‹æ ‡è®°è¡¨ {table_name} çš„æ‰€æœ‰batchä¸ºfinalï¼Œæ€»å…± {len(tracker.batch_trackers)} ä¸ªbatch')
                for batch_key, batch_tracker in tracker.batch_trackers.items():
                    if batch_tracker.state != 'final':
                        batch_index = batch_tracker.batch_index
                        is_warm_start = batch_tracker.is_warm_start
                        self.logger.info(f'   æ ‡è®° Batch {batch_index} (warm_start={is_warm_start}, å½“å‰çŠ¶æ€={batch_tracker.state})')
                        self.coordinator.mark_batch_final(table_name, batch_index, is_warm_start)
                        self.logger.info(f'   Batch {batch_index} å·²æ ‡è®°ä¸ºfinal')
                    else:
                        self.logger.info(f'    Batch {batch_tracker.batch_index} å·²ç»æ˜¯finalçŠ¶æ€ï¼Œè·³è¿‡')
                self.logger.info(f'   è¡¨ {table_name} çš„æ‰€æœ‰batchå·²æ ‡è®°ä¸ºfinal')
            else:
                if hasattr(self.current_context, 'batch_tracking') and table_name in self.current_context.batch_tracking:
                    total_batches = self.current_context.batch_tracking[table_name].get('total_batches', 1)
                    self.logger.info(f'   æœªæ‰¾åˆ°batch_trackersï¼Œä½†å‘ç°batch_trackingä¿¡æ¯: total_batches={total_batches}')
                    if total_batches == 1:
                        try:
                            self.coordinator.mark_batch_final(table_name, 0, is_warm_start=False)
                            self.logger.info(f'   å•batchæ¨¡å¼ï¼šBatch 0 æ ‡è®°ä¸ºfinal')
                        except Exception as e:
                            self.logger.warning(f'   æ ‡è®°batch 0ä¸ºfinalå¤±è´¥: {e}')
                    else:
                        self.logger.warning(f'   å¤šbatchæ¨¡å¼ä½†æ²¡æœ‰batch_trackersï¼Œå¯èƒ½å­˜åœ¨çŠ¶æ€ä¸ä¸€è‡´ï¼')
    
    def _is_table_force_completed(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å·²è¢«å¼ºåˆ¶å®Œæˆ"""
        if not hasattr(self.current_context, 'force_completed_tables'):
            return False
        
        is_completed = table_name in self.current_context.force_completed_tables
        if is_completed:
            pass  # Auto-fixed empty block
        return is_completed
    
    async def _auto_basic_verification(self, table_name: str, snapshots: List[TableSnapshot], context_data: dict) -> bool:
        """
        è‡ªåŠ¨åŸºç¡€éªŒè¯ - å°±åƒç¼–è¯‘å™¨çš„è¯­æ³•æ£€æŸ¥
        
        ç”±Orchestratoræ§åˆ¶ï¼Œæ¯æ¬¡æå–åå¼ºåˆ¶æ‰§è¡ŒFormatå’ŒTypeéªŒè¯ï¼Œ
        ç¡®ä¿æ•°æ®è´¨é‡çš„åŸºç¡€ä¿è¯
        
        Returns:
            bool: æ˜¯å¦è§¦å‘äº†ä¿®å¤æµç¨‹
        """
        if not snapshots:
            return False
        
        
        try:
            if not hasattr(self, '_basic_verifiers') or self._basic_verifiers is None:
                from ..mcp import FormatMCP, TypeMCP
                self._basic_verifiers = {
                    'format': FormatMCP(),
                    'type': TypeMCP()
                }
                self.logger.info("åˆå§‹åŒ–åŸºç¡€éªŒè¯å™¨ï¼ˆFormat + Typeï¼‰")
            
            all_violations = []
            
            for snapshot in snapshots:
                format_violations = self._basic_verifiers['format'].verify(
                    snapshot=snapshot,
                    schema=context_data.get('schema'),
                    table_name=table_name
                )
                all_violations.extend(format_violations)
                
                type_violations = self._basic_verifiers['type'].verify(
                    snapshot=snapshot,
                    schema=context_data.get('schema'),
                    table_name=table_name
                )
                all_violations.extend(type_violations)
            
            if self.current_context:
                run_id = context_data.get('run_id')
                
                if all_violations:
                    if table_name not in self.current_context.all_violations:
                        self.current_context.all_violations[table_name] = []
                    self.current_context.all_violations[table_name].extend(all_violations)
                
                if self.memory_manager and run_id and all_violations:
                    try:
                        await self.memory_manager.store_violations(all_violations, run_id, table_name)
                    except Exception as store_error:
                        self.logger.warning(f"å­˜å‚¨è‡ªåŠ¨éªŒè¯ç»“æœåˆ°memoryå¤±è´¥: {store_error}")
            
            error_count = len([v for v in all_violations if getattr(v, 'severity', 'warn') == 'error'])
            warn_count = len([v for v in all_violations if getattr(v, 'severity', 'warn') == 'warn'])
            
            self.logger.info(
                f' [åŸºç¡€éªŒè¯] å®Œæˆ - è¡¨: {table_name}, '
                f'FORMATè¿è§„: {len([v for v in all_violations if v.constraint_type == "FORMAT"])}, '
                f'TYPEè¿è§„: {len([v for v in all_violations if v.constraint_type == "TYPE"])}, '
                f'æ€»è®¡: {len(all_violations)} (é”™è¯¯: {error_count}, è­¦å‘Š: {warn_count})'
            )
            
            if all_violations:
                self.logger.info(f' [åŸºç¡€éªŒè¯] å‘ç° {len(all_violations)} ä¸ªè¿è§„ï¼Œå¼€å§‹åˆ†ç±»...')
                
                violations_requiring_reextraction = []
                violations_for_tool_fixing = []
                
                for violation in all_violations:
                    processing_category = getattr(violation, 'processing_category', None)
                    if not processing_category:
                        from ..verifier.mcp_verifier import ViolationCategory
                        if violation.constraint_type in ['FORMAT', 'TYPE']:
                            processing_category = ViolationCategory.TOOL_FIXABLE
                            violation.processing_category = processing_category
                        else:
                            if violation.severity == 'error':
                                processing_category = ViolationCategory.REQUIRES_REEXTRACTION
                                violation.processing_category = processing_category
                            else:
                                processing_category = ViolationCategory.TOOL_FIXABLE
                                violation.processing_category = processing_category
                    
                    try:
                        from ..verifier.mcp_verifier import ViolationCategory
                        if processing_category == ViolationCategory.TOOL_FIXABLE:
                            violations_for_tool_fixing.append(violation)
                        else:
                            violations_requiring_reextraction.append(violation)
                    except:
                        if getattr(violation, 'severity', 'warn') == 'error':
                            violations_requiring_reextraction.append(violation)
                        else:
                            violations_for_tool_fixing.append(violation)
                
                reextraction_in_tool_fixable = [
                    v for v in violations_for_tool_fixing 
                    if getattr(v, 'processing_category', '') == 'requires_reextraction'
                ]
                
                actual_tool_fixable = [
                    v for v in violations_for_tool_fixing 
                    if v not in reextraction_in_tool_fixable
                ]
                
                if reextraction_in_tool_fixable:
                    violations_requiring_reextraction.extend(reextraction_in_tool_fixable)
                
                self.logger.info(
                    f' [åŸºç¡€éªŒè¯] è¿è§„åˆ†ç±»å®Œæˆ: '
                    f'å¯å·¥å…·ä¿®å¤={len(actual_tool_fixable)}, '
                    f'éœ€è¦é‡æå–={len(violations_requiring_reextraction)}'
                )
                
                if not hasattr(self.current_context, 'basic_fix_attempts'):
                    self.current_context.basic_fix_attempts = {}
                
                if table_name not in self.current_context.basic_fix_attempts:
                    self.current_context.basic_fix_attempts[table_name] = 0
                
                if self.current_context.basic_fix_attempts[table_name] >= 1:
                    self.logger.warning(f' è¡¨ {table_name} åŸºç¡€ä¿®å¤å·²å°è¯• {self.current_context.basic_fix_attempts[table_name]} æ¬¡ï¼Œåœæ­¢é‡è¯•')
                    return False
                
                if actual_tool_fixable:
                    self.logger.info(f' [åŸºç¡€éªŒè¯] å‡†å¤‡è§¦å‘ä¿®å¤æµç¨‹: {len(actual_tool_fixable)} ä¸ªè¿è§„')
                    if table_name in self.current_context.all_snapshots:
                        snapshot = self.current_context.all_snapshots[table_name]
                        
                        if not hasattr(self.current_context, 'basic_fix_tables'):
                            self.current_context.basic_fix_tables = set()
                        self.current_context.basic_fix_tables.add(table_name)
                        
                        self.current_context.basic_fix_attempts[table_name] += 1
                        self.logger.info(f' è§¦å‘åŸºç¡€ä¿®å¤ï¼ˆç¬¬ {self.current_context.basic_fix_attempts[table_name]} æ¬¡ï¼‰: {table_name}ï¼Œ{len(actual_tool_fixable)} ä¸ªè¿è§„')
                        
                        try:
                            await asyncio.wait_for(
                                self._signal_fix_data(table_name, actual_tool_fixable, snapshot, context_data),
                                timeout=60.0  # 60ç§’è¶…æ—¶
                            )
                            return True  # è¡¨ç¤ºè§¦å‘äº†ä¿®å¤
                        except asyncio.TimeoutError:
                            self.logger.error(f' åŸºç¡€ä¿®å¤è¶…æ—¶: {table_name}ï¼Œå°†ç»§ç»­å®Œæ•´éªŒè¯')
                            return False  # è¶…æ—¶ï¼Œè¿”å›Falseè§¦å‘å®Œæ•´éªŒè¯
                        except Exception as fix_error:
                            self.logger.error(f' åŸºç¡€ä¿®å¤å¤±è´¥: {table_name}ï¼Œé”™è¯¯: {fix_error}')
                            import traceback
                            traceback.print_exc()
                            return False  # å¤±è´¥ï¼Œè¿”å›Falseè§¦å‘å®Œæ•´éªŒè¯
                    else:
                        self.logger.warning(f' [åŸºç¡€éªŒè¯] è¡¨ {table_name} ä¸åœ¨ all_snapshots ä¸­ï¼Œæ— æ³•ä¿®å¤')
                        self.logger.info(f' [åŸºç¡€éªŒè¯] å°†è¿›è¡Œå®Œæ•´éªŒè¯ä»¥å»ºç«‹å¿«ç…§')
                        return False  # æ— æ³•ä¿®å¤ï¼Œè¿”å›Falseè§¦å‘å®Œæ•´éªŒè¯
                else:
                    self.logger.info(f'â„¹ [åŸºç¡€éªŒè¯] æ²¡æœ‰å¯å·¥å…·ä¿®å¤çš„è¿è§„')
                    return False  # æ²¡æœ‰å¯ä¿®å¤çš„ï¼Œè¿”å›Falseè§¦å‘å®Œæ•´éªŒè¯
                
            self.logger.info(f' [åŸºç¡€éªŒè¯] æ— è¿è§„ï¼Œè¿”å› False')
            return False
            
        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨åŸºç¡€éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


class TableProcessingWaiter:
    """è¡¨å¤„ç†ç­‰å¾…å™¨ï¼Œè´Ÿè´£ç­‰å¾…å•ä¸ªè¡¨çš„å¤„ç†å®Œæˆ"""
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger('table_processing_waiter')
        self.orchestrator = None  # å°†åœ¨å¤–éƒ¨è®¾ç½®
    
    async def wait_for_table_processing_complete(self, context: Doc2DBContext, table_name: str):
        """ç­‰å¾…å•ä¸ªè¡¨çš„å¤„ç†å®Œæˆï¼ˆæ”¯æŒå¤šè½®éªŒè¯-ä¿®å¤å¾ªç¯ï¼‰"""
        max_wait_time = 100  # æ¢å¤æ›´é•¿çš„ç­‰å¾…æ—¶é—´
        wait_interval = 1.0  # æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        waited_time = 0
        max_fix_attempts = 1  #  æœ€å¤§ä¿®å¤å°è¯•æ¬¡æ•°
        max_iterations = 2  # æœ€å¤§ç­‰å¾…è¿­ä»£æ¬¡æ•°ï¼ˆæ¯”coordinatorçš„é™åˆ¶å¤šä¸€äº›ï¼‰
        iteration_count = 0
        
        
        extraction_done = False
        verification_started = False  # æ˜¯å¦å¼€å§‹éªŒè¯
        last_total_count = -1  # è·Ÿè¸ªæ€»è¿è§„æ•°é‡å˜åŒ–
        fix_attempt_count = 0  # è·Ÿè¸ªä¿®å¤å°è¯•æ¬¡æ•°
        last_violation_snapshot = None  # è®°å½•ä¸Šæ¬¡è¿è§„å¿«ç…§ï¼Œç”¨äºæ£€æµ‹ä¿®å¤æ•ˆæœ
        
        while waited_time < max_wait_time:
            if hasattr(context, 'force_completed_tables') and table_name in context.force_completed_tables:
                return True
            
            if not extraction_done and table_name in context.all_snapshots:
                if hasattr(context, 'warm_start_in_progress') and table_name in context.warm_start_in_progress:
                    pass  # Auto-fixed empty block
                else:
                    extraction_done = True
            
            if extraction_done:
                if table_name in context.all_violations:
                    verification_started = True
                    violations = context.all_violations[table_name]
                    current_total_count = len(violations)
                    error_violations = [v for v in violations if getattr(v, 'severity', 'warn') == 'error']
                    warn_violations = [v for v in violations if getattr(v, 'severity', 'warn') == 'warn']
                    
                    
                    if len(error_violations) > 0:
                        reextraction_violations = [
                            v for v in error_violations 
                            if getattr(v, 'processing_category', '') == 'requires_reextraction'
                        ]
                        
                        for i, v in enumerate(error_violations[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                            pc = getattr(v, 'processing_category', 'None')
                        
                        if reextraction_violations:
                            if not hasattr(context, 'warm_start_attempted'):
                                context.warm_start_attempted = set()
                            
                            if table_name not in context.warm_start_attempted:
                                
                                context.warm_start_attempted.add(table_name)
                                fix_attempt_count += 1  # æ ‡è®°å·²å°è¯•
                                
                                if table_name in context.all_snapshots:
                                    old_snapshot = context.all_snapshots[table_name]
                                    if not hasattr(context, 'warm_start_in_progress'):
                                        context.warm_start_in_progress = set()
                                    context.warm_start_in_progress.add(table_name)
                                
                                extraction_done = False
                                verification_started = False  # é‡ç½®éªŒè¯çŠ¶æ€
                                
                                await self._trigger_warm_start_extraction(context, table_name)
                                waited_time = 0
                                last_total_count = -1  # é‡ç½®è¿è§„è®¡æ•°
                                continue  # é‡æ–°å¼€å§‹ç­‰å¾…å¾ªç¯
                            else:
                                if not extraction_done:
                                    pass
                                else:
                                    return True
                    
                    violations_signature = f"{len(error_violations)}-{len(warn_violations)}"
                    progress_made = (last_total_count != current_total_count) or (last_total_count == -1)
                    
                    if progress_made:
                        last_total_count = current_total_count
                        iteration_count += 1
                        
                        if current_total_count == 0:
                            return True
                        
                        if len(error_violations) > 0:
                            if fix_attempt_count < max_fix_attempts:
                                fix_attempt_count += 1
                                waited_time = 0  # é‡ç½®ç­‰å¾…æ—¶é—´
                            else:
                                if not hasattr(context, 'warm_start_attempted'):
                                    context.warm_start_attempted = set()
                                if table_name not in context.warm_start_attempted:
                                    context.warm_start_attempted.add(table_name)
                                    fix_attempt_count = 0
                                    await self._trigger_warm_start_extraction(context, table_name)
                                    waited_time = 0
                                else:
                                    return True
                        elif len(warn_violations) > 0:
                            total_fixes = len(context.all_fixes.get(table_name, []))
                            
                            min_wait_for_mcp = 3.0  # è‡³å°‘ç­‰å¾…3ç§’ç»™MCPéªŒè¯æœºä¼š
                            
                            should_complete = (
                                (total_fixes >= 1 and waited_time >= min_wait_for_mcp) or  # æœ‰ä¿®å¤ä¸”ç­‰å¾…äº†è¶³å¤Ÿæ—¶é—´
                                waited_time >= 15 or  # ç­‰å¾…æ—¶é—´è¶…è¿‡15ç§’
                                fix_attempt_count >= 2  # å·²ç»å°è¯•ä¿®å¤è¿‡2æ¬¡
                            )
                            
                            if should_complete:
                                return True
                            elif fix_attempt_count < max_fix_attempts:
                                fix_attempt_count += 1
                                waited_time = 0
                            else:
                                return True
                    else:
                        if waited_time > 30:
                            if len(error_violations) == 0:
                                return True
                            else:
                                return True
                        
                elif extraction_done and waited_time > 10:
                    if not verification_started and waited_time > 20:
                        return True
            elif extraction_done and waited_time > 30:
                return True
                    
            await asyncio.sleep(wait_interval)
            waited_time += wait_interval
        
        final_total_violations = 0
        final_error_count = 0
        final_warn_count = 0
        if table_name in context.all_violations:
            violations = context.all_violations[table_name]
            error_violations = [v for v in violations if getattr(v, 'severity', 'warn') == 'error']
            warn_violations = [v for v in violations if getattr(v, 'severity', 'warn') == 'warn']
            final_total_violations = len(violations)
            final_error_count = len(error_violations)
            final_warn_count = len(warn_violations)
        
        if iteration_count >= max_iterations:
            self.logger.warning(
                f' è¡¨ {table_name} å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œ'
                f'æœ€ç»ˆçŠ¶æ€: {final_total_violations} ä¸ªè¿è§„ '
                f'(Error: {final_error_count}, Warning: {final_warn_count})ï¼Œ'
                f'å¼ºåˆ¶ç»ˆæ­¢å¤„ç†'
            )
            if hasattr(context, 'force_completed_tables'):
                context.force_completed_tables.add(table_name)
            else:
                context.force_completed_tables = {table_name}
        else:
            self.logger.info(
                f' è¡¨ {table_name} å¤„ç†å®Œæˆæˆ–è¶…æ—¶ï¼Œ'
                f'è¿­ä»£æ¬¡æ•°: {iteration_count}/{max_iterations}ï¼Œ'
                f'æœ€ç»ˆè¿è§„: {final_total_violations} ä¸ª'
            )
        
        
        return True
    
    async def _trigger_warm_start_extraction(self, context: Doc2DBContext, table_name: str):
        """è§¦å‘ warm start é‡æ–°æå–ï¼ˆå¸¦æœ‰violationsä¿¡æ¯ï¼‰
        
        æ³¨æ„ï¼šè°ƒç”¨æ–¹åº”è¯¥å·²ç»æ£€æŸ¥è¿‡ warm_start_attempted æ ‡è®°ï¼Œ
        è¿™é‡Œä¸å†é‡å¤æ£€æŸ¥å’Œæ ‡è®°ï¼Œé¿å…é€»è¾‘æ··ä¹±ã€‚
        """
        
        
        violations = context.all_violations.get(table_name, [])
        snapshot = context.all_snapshots.get(table_name)
        
        if snapshot:
            pass  # Auto-fixed empty block
        
        if not violations:
            return
        
        violations_requiring_reextraction = [
            v for v in violations 
            if getattr(v, 'processing_category', '') == 'requires_reextraction'
        ]
        
        relevant_documents = self._extract_relevant_documents_in_waiter(
            violations_requiring_reextraction, snapshot, context
        )
        
        if hasattr(self, 'orchestrator') and self.orchestrator:
            pass  # Auto-fixed empty block
            
            if hasattr(self.orchestrator, '_trigger_smart_warm_start_extraction'):
                await self.orchestrator._trigger_smart_warm_start_extraction(
                    table_name, violations_requiring_reextraction, snapshot
                )
            else:
                await self._trigger_filtered_extraction(context, table_name, relevant_documents)
        else:
            pass  # Auto-fixed empty block
        
    
    def _extract_relevant_documents_in_waiter(self, violations: List, snapshot, context) -> List[str]:
        """åœ¨waiterä¸­æå–ç›¸å…³æ–‡æ¡£ï¼ˆå¤ç”¨SignalHandlerMixinçš„é€»è¾‘ï¼‰"""
        if not violations or not snapshot or not hasattr(snapshot, 'rows'):
            return []
        
        relevant_docs = set()
        violation_tuple_ids = set([v.tuple_id for v in violations])
        
        
        for row in snapshot.rows:
            if row.tuple_id in violation_tuple_ids:
                for cell_name, cell_data in row.cells.items():
                    if hasattr(cell_data, 'evidences') and cell_data.evidences:
                        for evidence in cell_data.evidences:
                            import os
                            doc_name = os.path.basename(evidence)
                            if doc_name:
                                relevant_docs.add(doc_name)
        
        result = list(relevant_docs)
        return result
    
    async def _trigger_filtered_extraction(self, context, table_name: str, relevant_documents: List[str]):
        """ä½¿ç”¨è¿‡æ»¤åçš„æ–‡æ¡£è§¦å‘æå–"""
        if not relevant_documents:
            if hasattr(self.orchestrator, '_signal_extract_data'):
                await self.orchestrator._signal_extract_data(context, table_name)
            return
        
        
        all_documents = context.documents
        relevant_text_contents = []
        
        from .utils import DocumentUtils
        document_utils = DocumentUtils()
        
        for doc_path in all_documents:
            doc_basename = os.path.basename(doc_path)
            if doc_basename in relevant_documents:
                text_content = document_utils.convert_documents_to_text([doc_path])
                if text_content:
                    relevant_text_contents.extend(text_content)
        
        if relevant_text_contents:
            pass  # æš‚æœªå®ç°è‡ªå®šä¹‰text_contentsåŠŸèƒ½
        else:
            if hasattr(self.orchestrator, '_signal_extract_data'):
                await self.orchestrator._signal_extract_data(context, table_name)
