"""åè°ƒå™¨å·¥å…·æ¨¡å—"""
import os
import json
import logging
from typing import List, Dict, Any, Optional

from .context import Doc2DBContext


class SchemaUtils:
    """Schemaå¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def extract_table_names_from_schema(schema: Dict[str, Any]) -> List[str]:
        """ä»Schemaä¸­æå–æ‰€æœ‰è¡¨åï¼Œå¹¶è‡ªåŠ¨å¤„ç†åŒåè¡¨ï¼ˆæ·»åŠ ç¼–å·åç¼€ï¼‰"""
        table_names = []
        
        if 'tables' in schema and isinstance(schema['tables'], list):
            for table in schema['tables']:
                if isinstance(table, dict) and 'name' in table:
                    table_names.append(table['name'])
        
        elif 'table_name' in schema and 'columns' in schema:
            table_names.append(schema['table_name'])
        
        if table_names:
            name_counts = {}
            unique_names = []
            
            for name in table_names:
                if name not in name_counts:
                    name_counts[name] = 0
                name_counts[name] += 1
            
            name_indices = {}
            for name in table_names:
                if name_counts[name] > 1:
                    if name not in name_indices:
                        name_indices[name] = 1
                    unique_name = f"{name}_{name_indices[name]}"
                    name_indices[name] += 1
                    unique_names.append(unique_name)
                else:
                    unique_names.append(name)
            
            return unique_names
        
        return table_names
    
    @staticmethod
    def classify_tables_by_type(schema: Dict[str, Any]) -> Dict[str, List[str]]:
        """å°†è¡¨åˆ†ç±»ä¸ºå®ä½“è¡¨å’Œå…³ç³»è¡¨
        
        æ–°é€»è¾‘ï¼šç›´æ¥è¯»å– schema ä¸­è¡¨çš„ "type" å­—æ®µæ¥åˆ†ç±»
        - type: "entity" â†’ å®ä½“è¡¨
        - type: "relationship" â†’ å…³ç³»è¡¨
        - å¦‚æœæ²¡æœ‰ type å­—æ®µï¼Œé»˜è®¤ä¸ºå®ä½“è¡¨
        
        Returns:
            {
                'entity_tables': ['court', 'case', 'person', ...],
                'relation_tables': ['caseparty', ...]
            }
        """
        if not isinstance(schema, dict):
            return {'entity_tables': [], 'relation_tables': []}
        
        entity_tables = []
        relation_tables = []
        
        if 'tables' in schema and isinstance(schema['tables'], list):
            for table in schema['tables']:
                if not isinstance(table, dict) or 'name' not in table:
                    continue
                
                table_name = table['name']
                table_type = table.get('type', 'entity')  # é»˜è®¤ä¸ºå®ä½“è¡¨
                
                if table_type == 'relation':
                    relation_tables.append(table_name)
                else:
                    entity_tables.append(table_name)
        
        elif 'table_name' in schema:
            table_name = schema['table_name']
            table_type = schema.get('type', 'entity')
            
            if table_type == 'relation':
                relation_tables.append(table_name)
            else:
                entity_tables.append(table_name)
        
        return {
            'entity_tables': entity_tables,
            'relation_tables': relation_tables
        }
    
    @staticmethod
    def extract_table_specific_schema(schema: Dict[str, Any], table_name: str) -> Optional[Dict[str, Any]]:
        """ä»å®Œæ•´schemaä¸­æå–ç‰¹å®šè¡¨çš„schemaå®šä¹‰
        
        æ”¯æŒç¼–å·åçš„è¡¨åï¼ˆå¦‚ tax_1, tax_2ï¼‰ï¼Œä¼šè‡ªåŠ¨è§£æå¹¶æ‰¾åˆ°å¯¹åº”çš„åŸå§‹è¡¨å®šä¹‰
        """
        if not isinstance(schema, dict):
            return None
        
        if 'tables' in schema and isinstance(schema['tables'], list):
            for table in schema['tables']:
                if isinstance(table, dict) and table.get('name') == table_name:
                    return {
                        'tables': [table],
                        'table_name': table_name
                    }
            
            if '_' in table_name and table_name.rsplit('_', 1)[-1].isdigit():
                base_name = table_name.rsplit('_', 1)[0]
                suffix_num = int(table_name.rsplit('_', 1)[1])
                
                matching_tables = [
                    table for table in schema['tables']
                    if isinstance(table, dict) and table.get('name') == base_name
                ]
                
                if matching_tables and 1 <= suffix_num <= len(matching_tables):
                    target_table = matching_tables[suffix_num - 1]
                    return {
                        'tables': [target_table],
                        'table_name': table_name  # ä½¿ç”¨ç¼–å·åçš„è¡¨å
                    }
        
        elif schema.get('table_name') == table_name and 'columns' in schema:
            return schema  # å·²ç»æ˜¯å•è¡¨æ ¼å¼
        
        return None


class DocumentUtils:
    """æ–‡æ¡£å¤„ç†å·¥å…·ç±»"""
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger('document_utils')
    
    def convert_documents_to_text(self, documents: List[str]) -> List[str]:
        """å°†æ–‡æ¡£è·¯å¾„è½¬æ¢ä¸ºæ–‡æœ¬å†…å®¹"""
        text_contents = []
        
        for doc_path in documents:
            try:
                file_ext = os.path.splitext(doc_path.lower())[1]
                
                if file_ext in ['.txt', '.md']:
                    try:
                        with open(doc_path, 'r', encoding='utf-8') as f:
                            text_content = f.read()
                    except UnicodeDecodeError:
                        with open(doc_path, 'r', encoding='gbk') as f:
                            text_content = f.read()
                    text_contents.append(text_content)
                    
                elif file_ext in ['.pdf', '.docx', '.doc']:
                    from ...utils.textin_parser import TextinParser
                    parser = TextinParser()
                    
                    parse_result = parser.parse_pdf_with_textin(doc_path, main_content_only=True)
                    markdown_content = parse_result.get('markdown_content', '')
                    
                    if markdown_content:
                        text_contents.append(markdown_content)
                    else:
                        self.logger.warning(f"æœªèƒ½ä»æ–‡æ¡£ {doc_path} ä¸­æå–åˆ°å†…å®¹")
                        text_contents.append("")
                        
                else:
                    self.logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {doc_path}")
                    text_contents.append("")
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡æ¡£ {doc_path} å¤±è´¥: {e}")
                text_contents.append("")
        
        return text_contents


class SignalUtils:
    """ä¿¡å·å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def create_extraction_signal_data(context: Doc2DBContext, table_name: str, table_specific_schema: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæå–ä¿¡å·æ•°æ®"""
        document_utils = DocumentUtils()
        
        return {
            'context': {
                'run_id': context.run_id,
                'table_name': table_name,
                'schema': table_specific_schema,
                'full_schema': context.schema,
                'text_contents': document_utils.convert_documents_to_text(context.documents),
                'nl_prompt': context.nl_prompt or context.user_query,
                'processing_context': context
            }
        }
    
    @staticmethod
    def create_verification_signal_data(context: Doc2DBContext, table_name: str, 
                                      table_specific_schema: Dict[str, Any], snapshots: List) -> Dict[str, Any]:
        """åˆ›å»ºéªŒè¯ä¿¡å·æ•°æ®"""
        return {
            'context': {
                'run_id': context.run_id,
                'table_name': table_name,
                'schema': table_specific_schema,
                'full_schema': context.schema,
                'processing_context': context
            },
            'snapshots': snapshots
        }
    
    @staticmethod
    def create_fixing_signal_data(context: Doc2DBContext, table_name: str, 
                                table_specific_schema: Dict[str, Any], violations: List, snapshot) -> Dict[str, Any]:
        """åˆ›å»ºä¿®å¤ä¿¡å·æ•°æ®"""
        return {
            'context': {
                'run_id': context.run_id,
                'table_name': table_name,
                'schema': table_specific_schema,
                'full_schema': context.schema,
                'processing_context': context
            },
            'violations': violations,
            'snapshot': snapshot
        }
    
    @staticmethod
    def get_correlation_id(run_id: str, table_name: str, operation: str, timestamp: Optional[int] = None) -> str:
        """ç”Ÿæˆå…³è”ID"""
        if timestamp is None:
            import time
            timestamp = int(time.time() * 1000)
        return f"{run_id}_{table_name}_{operation}_{timestamp}"


class StepUtils:
    """æ­¥éª¤å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def create_start_step(context: Doc2DBContext) -> Dict[str, Any]:
        """åˆ›å»ºå¼€å§‹å¤„ç†æ­¥éª¤"""
        return {
            'step': 'orchestrator_start',
            'description': 'Orchestratorå¼€å§‹å¤„ç†æµç¨‹',
            'status': 'completed',
            'timestamp': context.io_manager.get_timestamp(),
            'details': {
                'run_id': context.run_id,
                'target_tables': getattr(context, 'target_tables', []),
                'signal_mode': True
            }
        }
    
    @staticmethod
    def create_completion_step(context: Doc2DBContext) -> Dict[str, Any]:
        """åˆ›å»ºå®Œæˆæ­¥éª¤"""
        return {
            'step': 'orchestrator_completion',
            'step_name': 'orchestrator_completion',
            'description': 'æ•°æ®å¤„ç†æµç¨‹å®Œæˆ - æ‰€æœ‰è¡¨æ ¼å·²å®Œæˆæå–ã€éªŒè¯å’Œä¿®å¤',
            'status': 'completed',
            'timestamp': context.io_manager.get_timestamp(),
            'details': {
                'total_tables_processed': len(context.all_snapshots),
                'total_snapshots': len(context.all_snapshots),
                'total_violations': sum(len(v) for v in context.all_violations.values()),
                'total_fixes': sum(len(f) for f in context.all_fixes.values()),
                'processing_state': context.current_state.value,
                'processing_summary': {
                    'extraction_completed': True,
                    'verification_completed': True,
                    'fixing_completed': True,
                    'quality_issues_resolved': sum(len(f) for f in context.all_fixes.values())
                }
            },
            'final_stats': {
                'tables': list(context.all_snapshots.keys()),
                'success': True,
                'completion_time': context.io_manager.get_timestamp()
            }
        }
    
    @staticmethod
    def create_error_step(context: Doc2DBContext, error: Exception, step_name: str = 'orchestrator_error') -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯æ­¥éª¤"""
        return {
            'step': step_name,
            'description': f'Orchestratorå¤„ç†å¼‚å¸¸: {str(error)}',
            'status': 'failed',
            'timestamp': context.io_manager.get_timestamp(),
            'error': str(error),
            'details': {
                'exception_type': type(error).__name__,
                'processing_state': context.current_state.value
            }
        }


class DataTransferUtils:
    """æ•°æ®è½¬ç§»å·¥å…·ç±»"""
    
    @staticmethod
    def transfer_processing_results(context: Doc2DBContext):
        """å°†å¤šè¡¨å¤„ç†ç»“æœè½¬ç§»åˆ°contextä¸»è¦å­—æ®µä¸­"""
        if context.all_snapshots:
            context.snapshots = list(context.all_snapshots.values())
        
        if context.all_violations:
            context.violations = []
            for table_name, table_violations in context.all_violations.items():
                context.violations.extend(table_violations)
        
        if context.all_fixes:
            context.fixes = []
            for table_name, table_fixes in context.all_fixes.items():
                context.fixes.extend(table_fixes)
    
    @staticmethod
    def save_final_snapshots(context: Doc2DBContext):
        """ä¿å­˜æœ€ç»ˆå®ŒæˆçŠ¶æ€å¿«ç…§"""
        for table_name, snapshot in context.all_snapshots.items():
            if snapshot and hasattr(snapshot, 'rows') and snapshot.rows:
                original_row_count = len(snapshot.rows)
                snapshot.rows = [
                    row for row in snapshot.rows
                    if not any(
                        (hasattr(cell, 'value') and (cell.value == "__DELETED__" or str(cell.value) == "__DELETED__"))
                        for cell in row.cells.values()
                    )
                ]
                deleted_row_count = original_row_count - len(snapshot.rows)
                if deleted_row_count > 0:
                    context.io_manager.logger.info(f'ğŸ§¹ [æœ€ç»ˆæ¸…ç†] è¡¨ {table_name}: æ¸…ç†äº† {deleted_row_count} è¡Œ__DELETED__æ ‡è®°çš„æ•°æ®')
            
            final_snapshot = snapshot
            final_snapshot.processing_stage = 'final'
            final_snapshot.stage_description = f'æœ€ç»ˆå®ŒæˆçŠ¶æ€ - è¡¨ {table_name}'
            context.io_manager.append_snapshot(final_snapshot)
        
        DataTransferUtils.save_result_json(context)
    
    @staticmethod
    def save_result_json(context: Doc2DBContext):
        """ç”Ÿæˆresult.jsonæ–‡ä»¶
        
        ğŸ”§ åªè¾“å‡ºç”¨æˆ·è¯·æ±‚çš„ç›®æ ‡è¡¨ï¼Œä¸åŒ…å«å‚è€ƒè¡¨ï¼ˆç”¨äºå…³ç³»æŠ½å–éªŒè¯çš„è¾…åŠ©è¡¨ï¼‰
        """
        try:
            import json
            from pathlib import Path
            
            best_snapshots = {}
            for table_name in context.target_tables:
                if table_name in context.all_snapshots:
                    snapshot = context.all_snapshots[table_name]
                    
                    if hasattr(snapshot, 'processing_stage') and snapshot.processing_stage == 'reference_data':
                        context.io_manager.logger.debug(f'è·³è¿‡å‚è€ƒè¡¨ {table_name}ï¼ˆä¸åŒ…å«åœ¨result.jsonä¸­ï¼‰')
                        continue
                    
                    best_snapshots[table_name] = snapshot
            
            if not best_snapshots:
                context.io_manager.logger.warning("target_tablesçš„å¿«ç…§ä¸ºç©ºï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–finalå¿«ç…§")
                best_snapshots = DataTransferUtils._find_final_snapshots_from_file(context)
            
            tables_data = {}
            
            schema_field_orders = {}
            if hasattr(context, 'schema') and context.schema:
                schema = context.schema
                if isinstance(schema, str):
                    import json as json_module
                    schema = json_module.loads(schema)
                
                if 'tables' in schema and isinstance(schema['tables'], list):
                    for table in schema['tables']:
                        table_name_key = table.get('name')
                        fields_list = table.get('attributes') or table.get('fields', [])
                        if table_name_key and fields_list:
                            field_order = [f.get('name') or f.get('field_name') for f in fields_list if f.get('name') or f.get('field_name')]
                            schema_field_orders[table_name_key] = field_order
            
            for table_name, snapshot in best_snapshots.items():
                if not snapshot or not hasattr(snapshot, 'rows') or not snapshot.rows:
                    continue
                
                first_row = snapshot.rows[0]
                if not hasattr(first_row, 'cells') or not first_row.cells:
                    continue
                
                if table_name in schema_field_orders:
                    headers = [field for field in schema_field_orders[table_name] if field in first_row.cells]
                    for field in first_row.cells.keys():
                        if field not in headers:
                            headers.append(field)
                else:
                    headers = list(first_row.cells.keys())
                    context.io_manager.logger.warning(f"è¡¨ {table_name} æœªåœ¨ schema ä¸­æ‰¾åˆ°ï¼Œä½¿ç”¨ cells æ¨æ–­çš„é¡ºåº")
                
                table_rows = []
                skipped_deleted_rows = 0  # ç»Ÿè®¡è·³è¿‡çš„åˆ é™¤æ ‡è®°è¡Œ
                
                for row in snapshot.rows:
                    has_deleted_marker = False
                    for header in headers:
                        cell = row.cells.get(header)
                        if cell:
                            cell_value = None
                            if hasattr(cell, 'value'):
                                cell_value = cell.value
                            elif hasattr(cell, 'best') and hasattr(cell.best, 'value'):
                                cell_value = cell.best.value
                            
                            if cell_value == "__DELETED__" or str(cell_value) == "__DELETED__":
                                has_deleted_marker = True
                                break
                    
                    if has_deleted_marker:
                        skipped_deleted_rows += 1
                        context.io_manager.logger.debug(f"è·³è¿‡åˆ é™¤æ ‡è®°çš„è¡Œ: {row.tuple_id}")
                        continue
                    
                    row_data = {}
                    for header in headers:
                        cell = row.cells.get(header)
                        if cell:
                            if hasattr(cell, 'value'):
                                row_data[header] = cell.value
                            elif hasattr(cell, 'best') and hasattr(cell.best, 'value'):
                                row_data[header] = cell.best.value
                            else:
                                row_data[header] = ''
                        else:
                            row_data[header] = ''
                    table_rows.append(row_data)
                
                if skipped_deleted_rows > 0:
                    context.io_manager.logger.info(f"è¡¨ {table_name}: è¿‡æ»¤äº† {skipped_deleted_rows} è¡Œæ ‡è®°ä¸ºåˆ é™¤çš„æ•°æ®")
                
                tables_data[table_name] = table_rows
            
            result_data = {
                'run_id': context.run_id,
                'tables': tables_data
            }
            
            result_file = context.io_manager.output_dir / 'result.json'
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2, sort_keys=False)
            
            import os
            if not (os.path.exists(result_file) and os.path.getsize(result_file) > 0):
                context.io_manager.logger.warning(f"result.json å†™å…¥å¯èƒ½æœªå®Œæˆ: {result_file}")
            
        except Exception as e:
            context.io_manager.logger.error(f"ç”Ÿæˆresult.jsonå¤±è´¥: {e}")
    
    @staticmethod
    def _find_final_snapshots_from_file(context: Doc2DBContext):
        """ä»snapshots.jsonlæ–‡ä»¶ä¸­è¯»å–finalé˜¶æ®µçš„å¿«ç…§
        
        finalé˜¶æ®µçš„å¿«ç…§æ˜¯åœ¨save_final_snapshotsä¸­ä¿å­˜çš„ï¼Œ
        ä»£è¡¨çš„æ˜¯context.all_snapshotsçš„å†…å®¹ï¼ˆå·²ç»åŒ…å«äº†æ‰€æœ‰batchåˆå¹¶åçš„å®Œæ•´æ•°æ®ï¼‰
        """
        try:
            import json
            from ...memory.snapshot import TableSnapshot
            
            snapshots_file = context.io_manager.output_dir / 'snapshots.jsonl'
            if not snapshots_file.exists():
                context.io_manager.logger.warning(f"snapshots.jsonlæ–‡ä»¶ä¸å­˜åœ¨: {snapshots_file}")
                return None
            
            final_snapshots = {}
            with open(snapshots_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        snapshot_dict = json.loads(line)
                        table_name = snapshot_dict.get('table')
                        stage = snapshot_dict.get('processing_stage', '')
                        
                        if stage == 'final' and table_name:
                            try:
                                snapshot_obj = TableSnapshot.from_dict(snapshot_dict)
                                final_snapshots[table_name] = snapshot_obj
                                context.io_manager.logger.info(
                                    f"ä»æ–‡ä»¶è¯»å–è¡¨ {table_name} çš„finalå¿«ç…§: {len(snapshot_obj.rows)}è¡Œ"
                                )
                            except Exception as e:
                                context.io_manager.logger.error(f"è½¬æ¢å¿«ç…§å¤±è´¥: {e}")
                    except Exception as e:
                        continue
            
            return final_snapshots if final_snapshots else None
                
        except Exception as e:
            context.io_manager.logger.error(f"ä»æ–‡ä»¶è¯»å–finalå¿«ç…§å¤±è´¥: {e}")
            return None


class DebugUtils:
    """è°ƒè¯•å·¥å…·ç±»"""
    
    @staticmethod
    def write_debug_log(context: Doc2DBContext, message: str):
        """å†™å…¥è°ƒè¯•æ—¥å¿—"""
        try:
            with open(f"{context.io_manager.base_path}/debug_orchestrator.log", "a", encoding="utf-8") as f:
                f.write(f"[{context.io_manager.get_timestamp()}] {message}\n")
        except:
            pass  # å¿½ç•¥æ—¥å¿—å†™å…¥é”™è¯¯
    
    @staticmethod
    def ensure_completion_step(context: Doc2DBContext) -> bool:
        """ç¡®ä¿æœ‰å®Œæˆæ­¥éª¤è®°å½•ï¼ˆå…œåº•å¤„ç†ï¼‰"""
        has_completion_step = any(
            'Orchestratorå®Œæˆæ‰€æœ‰å¤„ç†æµç¨‹' in step.get('description', '')
            for step in context.step_outputs
        )
        
        if not has_completion_step:
            completion_step = {
                'step': 'orchestrator_completion_fallback',
                'description': 'Orchestratorå®Œæˆæ‰€æœ‰å¤„ç†æµç¨‹',
                'status': 'completed',
                'timestamp': context.io_manager.get_timestamp(),
                'details': {'fallback': True, 'sync_call': True}
            }
            context.step_outputs.append(completion_step)
            return True
        
        return False
