"""æ–‡æ¡£å®šä½å™¨ - ä»é•¿æ–‡æ¡£ä¸­å®šä½ä¸schemaç›¸å…³çš„ç‰‡æ®µ

ğŸ”§ å½“å‰çŠ¶æ€ï¼šå·²åœç”¨
- entity_extractor.py ä¸­ enable_locate é»˜è®¤å€¼å·²æ”¹ä¸º False
- relation_extractor.py ä¸­å·²ç§»é™¤ locator è°ƒç”¨é€»è¾‘
- æ‰€æœ‰æ–‡æ¡£ç°åœ¨ç›´æ¥è¿›è¡ŒæŠ½å–ï¼Œä¸è¿›è¡Œå®šä½å¤„ç†
"""

import os
import json
import logging
import re
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

import llm.main as llm_main


@dataclass
class LocatedSegment:
    """å®šä½åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼ˆç²¾ç®€ç‰ˆï¼‰"""
    content: str           # åŸå§‹æ–‡æœ¬å†…å®¹ï¼ˆä¸€å­—ä¸æ”¹ï¼‰
    start_position: int    # åœ¨åŸæ–‡æ¡£ä¸­çš„èµ·å§‹ä½ç½®
    end_position: int      # åœ¨åŸæ–‡æ¡£ä¸­çš„ç»“æŸä½ç½®
    relevance_score: float # ç›¸å…³æ€§å¾—åˆ†ï¼ˆ0-1ï¼‰
    source_document: str = "" # æ¥æºæ–‡æ¡£åç§°ï¼ˆç”¨äºå¤šæ–‡æ¡£æ¨¡å¼ï¼‰


class DocumentLocator:
    """
    æ–‡æ¡£å®šä½å™¨ - å¤„ç†é•¿æ–‡æ¡£çš„æ™ºèƒ½ç‰‡æ®µå®šä½
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. åˆ¤æ–­æ–‡æ¡£æ˜¯å¦è¿‡é•¿éœ€è¦å®šä½
    2. ä½¿ç”¨LLMæ™ºèƒ½å®šä½ä¸schemaç›¸å…³çš„åŸå§‹ç‰‡æ®µ
    3. ä¿æŒåŸæ–‡ä¸å˜ï¼ˆä¸€å­—ä¸æ”¹ï¼‰
    4. è¿”å›å®šä½åçš„ç‰‡æ®µä¾›åç»­æŠ½å–ä½¿ç”¨
    """
    
    DEFAULT_WORD_THRESHOLD = 30000    # é»˜è®¤è¯æ•°é˜ˆå€¼
    DEFAULT_MAX_SEGMENT_LENGTH = 2000 # å•ä¸ªç‰‡æ®µæœ€å¤§é•¿åº¦
    DEFAULT_CACHE_DIR = ".cache/locator"  # é»˜è®¤ç¼“å­˜ç›®å½•
    
    def __init__(self, 
                 word_threshold: int = DEFAULT_WORD_THRESHOLD,
                 max_segment_length: int = DEFAULT_MAX_SEGMENT_LENGTH,
                 enable_cache: bool = True,
                 cache_dir: str = DEFAULT_CACHE_DIR):
        """
        åˆå§‹åŒ–æ–‡æ¡£å®šä½å™¨
        
        Args:
            word_threshold: è¯æ•°é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼æ‰è¿›è¡Œå®šä½
            max_segment_length: å•ä¸ªç‰‡æ®µæœ€å¤§é•¿åº¦
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜åŠŸèƒ½ï¼ˆé»˜è®¤Trueï¼‰
            cache_dir: ç¼“å­˜æ–‡ä»¶å­˜å‚¨ç›®å½•ï¼ˆé»˜è®¤ .cache/locatorï¼‰
        
        æ³¨æ„ï¼š
        - unified_orchestrator è´Ÿè´£å¤šæ–‡æ¡£çš„åˆ†æ‰¹ç­–ç•¥
        - extractor ä½¿ç”¨ should_locate åˆ¤æ–­æ˜¯å¦éœ€è¦å®šä½
        - æ­¤ç±»æä¾›æ–‡æ¡£å®šä½çš„æ ¸å¿ƒåŠŸèƒ½
        """
        self.word_threshold = word_threshold
        self.max_segment_length = max_segment_length
        self.enable_cache = enable_cache
        self.cache_dir = cache_dir
        self.logger = logging.getLogger('doc2db.locator')
        
        if self.enable_cache:
            Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
    
    def _estimate_word_count(self, text_content: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„è¯æ•°
        
        Args:
            text_content: æ–‡æœ¬å†…å®¹
            
        Returns:
            int: ä¼°ç®—çš„è¯æ•°
        """
        if not text_content:
            return 0
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text_content))
        english_words = len(re.findall(r'[a-zA-Z]+', text_content))
        return chinese_chars + english_words
    
    def should_locate(self, text_content: str) -> bool:
        """
        åˆ¤æ–­æ–‡æ¡£æ˜¯å¦éœ€è¦è¿›è¡Œå®šä½ï¼ˆå³æ˜¯å¦è¿‡é•¿ï¼‰
        
        Args:
            text_content: æ–‡æ¡£å†…å®¹
            
        Returns:
            bool: æ˜¯å¦éœ€è¦å®šä½
        """
        if not text_content or not text_content.strip():
            return False
        
        estimated_word_count = self._estimate_word_count(text_content)
        
        needs_locate = estimated_word_count > self.word_threshold
        
        return needs_locate
    
    def _generate_cache_key(self, 
                           text_content: str, 
                           schema: Dict[str, Any], 
                           table_name: str,
                           nl_prompt: str = "") -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹å’Œschemaçš„å“ˆå¸Œï¼‰
        
        Args:
            text_content: æ–‡æ¡£å†…å®¹
            schema: schemaå®šä¹‰
            table_name: è¡¨å
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            
        Returns:
            str: ç¼“å­˜é”®ï¼ˆMD5å“ˆå¸Œå€¼ï¼‰
        """
        cache_input = json.dumps({
            'text_content_hash': hashlib.md5(text_content.encode('utf-8')).hexdigest(),
            'schema': schema,
            'table_name': table_name,
            'nl_prompt': nl_prompt,
            'max_segment_length': self.max_segment_length
        }, sort_keys=True, ensure_ascii=False)
        
        cache_key = hashlib.md5(cache_input.encode('utf-8')).hexdigest()
        return cache_key
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """
        è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Args:
            cache_key: ç¼“å­˜é”®
            
        Returns:
            Path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        return Path(self.cache_dir) / f"{cache_key}.json"
    
    def _load_from_cache(self, cache_key: str) -> Optional[List[LocatedSegment]]:
        """
        ä»ç¼“å­˜åŠ è½½å®šä½ç»“æœ
        
        Args:
            cache_key: ç¼“å­˜é”®
            
        Returns:
            Optional[List[LocatedSegment]]: ç¼“å­˜çš„ç‰‡æ®µåˆ—è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if not self.enable_cache:
            return None
        
        cache_path = self._get_cache_path(cache_key)
        
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            if not isinstance(cache_data, dict) or 'segments' not in cache_data:
                self.logger.warning(f"ç¼“å­˜æ–‡ä»¶æ ¼å¼æ— æ•ˆ: {cache_path}")
                return None
            
            segments = []
            for seg_dict in cache_data['segments']:
                segments.append(LocatedSegment(
                    content=seg_dict['content'],
                    start_position=seg_dict['start_position'],
                    end_position=seg_dict['end_position'],
                    relevance_score=seg_dict['relevance_score'],
                    source_document=seg_dict.get('source_document', '')
                ))
            
            return segments
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def _save_to_cache(self, cache_key: str, segments: List[LocatedSegment], 
                      metadata: Dict[str, Any] = None):
        """
        ä¿å­˜å®šä½ç»“æœåˆ°ç¼“å­˜
        
        Args:
            cache_key: ç¼“å­˜é”®
            segments: å®šä½åˆ°çš„ç‰‡æ®µåˆ—è¡¨
            metadata: é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        if not self.enable_cache:
            return
        
        cache_path = self._get_cache_path(cache_key)
        
        try:
            segments_data = [
                {
                    'content': seg.content,
                    'start_position': seg.start_position,
                    'end_position': seg.end_position,
                    'relevance_score': seg.relevance_score,
                    'source_document': seg.source_document
                }
                for seg in segments
            ]
            
            cache_data = {
                'version': '1.0',
                'segments': segments_data,
                'metadata': metadata or {},
                'timestamp': self._get_timestamp()
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ç¼“å­˜å·²ä¿å­˜: {cache_path}")
            
        except Exception as e:
            self.logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def locate_relevant_segments(self,
                                 text_content: str,
                                 schema: Dict[str, Any],
                                 table_name: str,
                                 nl_prompt: str = "",
                                 context = None,
                                 source_document: str = "") -> List[LocatedSegment]:
        """
        ä»æ–‡æ¡£ä¸­å®šä½ä¸schemaç›¸å…³çš„ç‰‡æ®µ
        
        æ³¨æ„ï¼š
        - è°ƒç”¨æ­¤æ–¹æ³•å‰åº”å…ˆä½¿ç”¨ should_locate() åˆ¤æ–­æ˜¯å¦éœ€è¦å®šä½
        - æ­¤æ–¹æ³•ç›´æ¥è¿›è¡Œå®šä½å¤„ç†ï¼Œä¸åšæ–‡æœ¬é•¿åº¦åˆ¤æ–­
        - unified_orchestrator è´Ÿè´£å¤šæ–‡æ¡£çš„åˆ†æ‰¹ç­–ç•¥
        
        Args:
            text_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            schema: æ•°æ®åº“schemaå®šä¹‰
            table_name: ç›®æ ‡è¡¨å
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            context: å¤„ç†ä¸Šä¸‹æ–‡
            source_document: æ¥æºæ–‡æ¡£åç§°
            
        Returns:
            List[LocatedSegment]: å®šä½åˆ°çš„ç›¸å…³ç‰‡æ®µåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹å®šä½æ–‡æ¡£ç‰‡æ®µ: {table_name}")
        
        if self.enable_cache:
            cache_key = self._generate_cache_key(text_content, schema, table_name, nl_prompt)
            cached_segments = self._load_from_cache(cache_key)
            
            if cached_segments is not None:
                self.logger.info(f"ä»ç¼“å­˜åŠ è½½å®šä½ç»“æœ: {len(cached_segments)} ä¸ªç‰‡æ®µ")
                if context and hasattr(context, 'step_outputs'):
                    from ...core.io import IOManager
                    context.step_outputs.append({
                        'step': f'locator_document_{table_name}',
                        'status': 'completed',
                        'description': f'æ–‡æ¡£ç‰‡æ®µå®šä½ [è¡¨: {table_name}] (ä»ç¼“å­˜)',
                        'details': {
                            'model': 'cached',
                            'segments_found': len(cached_segments),
                            'segments_info': [
                                {
                                    'preview': s.content[:100] + '...' if len(s.content) > 100 else s.content
                                }
                                for s in cached_segments
                            ]
                        },
                        'timestamp': IOManager.get_timestamp()
                    })
                
                return cached_segments
        
        word_count = self._estimate_word_count(text_content)
        self.logger.info(f"æ–‡æ¡£è¯æ•°: {word_count}")
        
        segments = self._locate_single_document(
            text_content=text_content,
            schema=schema,
            table_name=table_name,
            nl_prompt=nl_prompt,
            context=context,
            source_document=source_document
        )
        
        if self.enable_cache:
            cache_key = self._generate_cache_key(text_content, schema, table_name, nl_prompt)
            metadata = {
                'table_name': table_name,
                'document_length': len(text_content),
                'word_count': word_count,
                'source_document': source_document
            }
            self._save_to_cache(cache_key, segments, metadata)
        
        return segments
    
    def _locate_single_document(self,
                               text_content: str,
                               schema: Dict[str, Any],
                               table_name: str,
                               nl_prompt: str = "",
                               context = None,
                               source_document: str = "") -> List[LocatedSegment]:
        """
        å¯¹å•ä¸ªæ–‡æ¡£è¿›è¡Œå®šä½ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
        
        Args:
            text_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            schema: æ•°æ®åº“schemaå®šä¹‰
            table_name: ç›®æ ‡è¡¨å
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            context: å¤„ç†ä¸Šä¸‹æ–‡
            source_document: æ¥æºæ–‡æ¡£åç§°
            
        Returns:
            List[LocatedSegment]: å®šä½åˆ°çš„ç›¸å…³ç‰‡æ®µåˆ—è¡¨
        """
        
        model = 'gemini-2.5-pro'
        
        table_def = self._get_table_definition(schema, table_name)
        if not table_def:
            self.logger.error(f"æœªæ‰¾åˆ°è¡¨å®šä¹‰: {table_name}")
            return [LocatedSegment(
                content=text_content,
                start_position=0,
                end_position=len(text_content),
                relevance_score=1.0,
                source_document=source_document
            )]
        
        field_descriptions = self._build_field_descriptions(table_def)
        
        try:
            self.logger.info(f'ğŸ” å¼€å§‹å®šä½ç›¸å…³ç‰‡æ®µ - è¡¨: {table_name}, æ–‡æ¡£é•¿åº¦: {len(text_content)}')
            
            prompt = self._build_locate_prompt(
                text_content=text_content,
                table_name=table_name,
                field_descriptions=field_descriptions,
                nl_prompt=nl_prompt,
                max_segment_length=self.max_segment_length
            )
            
            self.logger.info(f'ğŸ” LLMè¾“å…¥ - è¡¨: {table_name}')
            
            response = llm_main.get_answer(
                question=prompt,
                model='gemini-2.5-pro' #qwen-long
            )
            
            self.logger.info(f'ğŸ” LLMè¾“å‡º - è¡¨: {table_name}')
            if response:
                self.logger.debug(f'  å®Œæ•´Response:\n{response[:2000]}...' if len(response) > 2000 else f'  å®Œæ•´Response:\n{response}')
            else:
                self.logger.warning(f'  âš ï¸ LLMè¿”å›ç©ºå“åº”ï¼')
            
            if not response or len(response.strip()) <= 1:
                self.logger.warning(f'  âš ï¸ ç¬¬ä¸€æ¬¡LLMè°ƒç”¨å¤±è´¥ï¼Œå°è¯•GPT-4o fallback')
                response = llm_main.get_answer(
                    question=prompt,
                    model='gpt-4o'
                )
                self.logger.info(f'ğŸ” GPT-4o Fallbackè¾“å‡º')
                if response:
                    self.logger.debug(f'  å®Œæ•´Response:\n{response[:2000]}...' if len(response) > 2000 else f'  å®Œæ•´Response:\n{response}')
            
            segments = self._parse_llm_response(
                response, text_content, source_document
            )
            
            for i, seg in enumerate(segments):
                preview = seg.content[:100].replace('\n', ' ') + '...' if len(seg.content) > 100 else seg.content.replace('\n', ' ')
                if seg.source_document:
                    self.logger.debug(f'  ç‰‡æ®µ {i+1}: é•¿åº¦={len(seg.content)}, ç›¸å…³æ€§={seg.relevance_score:.2f}, æ¥æº={seg.source_document}, é¢„è§ˆ={preview}')
                else:
                    self.logger.debug(f'  ç‰‡æ®µ {i+1}: é•¿åº¦={len(seg.content)}, ç›¸å…³æ€§={seg.relevance_score:.2f}, é¢„è§ˆ={preview}')
            
            if context and hasattr(context, 'step_outputs'):
                from ...core.io import IOManager
                context.step_outputs.append({
                    'step': f'locator_document_{table_name}',
                    'status': 'completed',
                    'description': f'æ–‡æ¡£ç‰‡æ®µå®šä½ [è¡¨: {table_name}]',
                    'details': {
                        'model': model,
                        'original_length': len(text_content),
                        'segments_found': len(segments),
                        'total_segment_length': sum(len(s.content) for s in segments),
                        'compression_ratio': f"{sum(len(s.content) for s in segments) / len(text_content) * 100:.1f}%",
                        'segments_info': [
                            {
                                'length': len(s.content),
                                'relevance': s.relevance_score,
                                'preview': s.content[:100] + '...' if len(s.content) > 100 else s.content
                            }
                            for s in segments
                        ]
                    },
                    'timestamp': IOManager.get_timestamp()
                })
            
            return segments
            
        except Exception as e:
            self.logger.error(f"å®šä½å¤±è´¥: {e}")
            
            return [LocatedSegment(
                content=text_content,
                start_position=0,
                end_position=len(text_content),
                relevance_score=1.0,
                source_document=source_document
            )]
    
    
    def _get_table_definition(self, schema: Dict[str, Any], table_name: str) -> Optional[Dict[str, Any]]:
        """è·å–è¡¨å®šä¹‰ï¼ˆä¸BaseExtractoré€»è¾‘ä¸€è‡´ï¼‰"""
        
        if 'table_name' in schema and 'tables' in schema and len(schema['tables']) == 1:
            result = schema['tables'][0].copy()
            if 'fields' in result and 'attributes' not in result:
                result['attributes'] = result.pop('fields')
            return result
        
        tables = schema.get('tables', [])
        
        for table in tables:
            if table.get('name') == table_name:
                result = table.copy()
                if 'fields' in result and 'attributes' not in result:
                    result['attributes'] = result.pop('fields')
                return result
        
        if '_' in table_name and table_name.rsplit('_', 1)[-1].isdigit():
            base_name = table_name.rsplit('_', 1)[0]
            suffix_num = int(table_name.rsplit('_', 1)[1])
            
            matching_tables = [
                table for table in tables
                if isinstance(table, dict) and table.get('name') == base_name
            ]
            
            if matching_tables and 1 <= suffix_num <= len(matching_tables):
                result = matching_tables[suffix_num - 1].copy()
                if 'fields' in result and 'attributes' not in result:
                    result['attributes'] = result.pop('fields')
                return result
        
        if schema.get('table_name') == table_name and 'columns' in schema:
            columns = schema.get('columns', {})
            if isinstance(columns, dict):
                attributes = [
                    {
                        'name': col_name,
                        'type': col_def.get('type', ''),
                        'description': col_def.get('description', '')
                    }
                    for col_name, col_def in columns.items()
                ]
            else:
                attributes = columns
            
            return {
                'name': table_name,
                'attributes': attributes
            }
        
        return None
    
    def _build_field_descriptions(self, table_def: Dict[str, Any]) -> str:
        """æ„å»ºå­—æ®µæè¿°ä¿¡æ¯"""
        attributes = table_def.get('attributes', [])
        
        field_descriptions = []
        for attr in attributes:
            field_info = f"- {attr['name']} ({attr.get('type', 'TEXT')})"
            if 'description' in attr:
                field_info += f": {attr['description']}"
            field_descriptions.append(field_info)
        
        return '\n'.join(field_descriptions)
    
    def _build_locate_prompt(self,
                            text_content: str,
                            table_name: str,
                            field_descriptions: str,
                            nl_prompt: str,
                            max_segment_length: int) -> str:
        """æ„å»ºæ–‡æ¡£å®šä½çš„ prompt"""
        prompt = f"""æˆ‘æœ‰ä¸€ä¸ªé•¿æ–‡æ¡£ï¼Œéœ€è¦å®šä½ä¸æ•°æ®æŠ½å–schemaç›¸å…³çš„åŸå§‹ç‰‡æ®µã€‚

=== ç›®æ ‡è¡¨æ ¼ä¿¡æ¯ ===
è¡¨å: {table_name}

å­—æ®µå®šä¹‰:
{field_descriptions}

=== ä»»åŠ¡è¦æ±‚ ===
è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œé‡‡ç”¨**å®½æ³›åŒ¹é…ç­–ç•¥**ï¼Œå®šä½æ‰€æœ‰å¯èƒ½ä¸ä¸Šè¿°å­—æ®µç›¸å…³çš„ç‰‡æ®µã€‚

 æ ¸å¿ƒåŸåˆ™ï¼šå®å¯å¤šæå–ï¼Œä¸è¦æ¼æ‰ï¼
1. æå–ç‰‡æ®µæ—¶ä¿ç•™ä¸»è¦å†…å®¹ï¼Œå¯ä»¥é€‚å½“æ¸…ç†å¤šä½™çš„ç©ºæ ¼ã€æ¢è¡Œç­‰æ ¼å¼å™ªéŸ³
2. åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼ˆè‡³å°‘ä¸€ä¸ªå®Œæ•´å¥å­/æ®µè½ï¼‰
3. é‡‡ç”¨**å®½æ³›åŒ¹é…**ï¼šåªè¦å†…å®¹å¯èƒ½ä¸å­—æ®µæœ‰å…³è”ï¼Œå°±åº”è¯¥æå–
4. æä¾›ç›¸å…³æ€§å¾—åˆ†ï¼ˆ0-1ï¼‰ï¼šå³ä½¿ç›¸å…³æ€§è¾ƒä½ï¼ˆå¦‚0.3-0.5ï¼‰ä¹Ÿåº”è¯¥æå–

ğŸ“ ä»€ä¹ˆæ ·çš„å†…å®¹åº”è¯¥è¢«æå–ï¼Ÿï¼ˆå®½æ³›åŒ¹é…åŸåˆ™ï¼‰
âœ… ç›´æ¥åŒ…å«å­—æ®µä¿¡æ¯çš„å†…å®¹ï¼ˆç›¸å…³æ€§ 0.8-1.0ï¼‰
   ä¾‹å¦‚ï¼šåŒ…å«å§“åã€å¹´é¾„ã€å·¥èµ„ç­‰å…·ä½“æ•°æ®çš„æ®µè½
   
âœ… é—´æ¥ç›¸å…³çš„å†…å®¹ï¼Œå¦‚èƒŒæ™¯ä¿¡æ¯ã€è¯´æ˜æ–‡å­—ï¼ˆç›¸å…³æ€§ 0.5-0.7ï¼‰
   ä¾‹å¦‚ï¼šéƒ¨é—¨ä»‹ç»ã€èŒä½è¯´æ˜ã€è–ªèµ„ä½“ç³»è¯´æ˜ç­‰
   
âœ… å¯èƒ½åŒ…å«éšå«ä¿¡æ¯çš„å†…å®¹ï¼ˆç›¸å…³æ€§ 0.3-0.5ï¼‰
   ä¾‹å¦‚ï¼šå…¬å¸ç»„ç»‡æ¶æ„ã€äººå‘˜ç»Ÿè®¡ã€ç›¸å…³æ”¿ç­–æ–‡ä»¶ç­‰
   
âœ… æè¿°æ€§å†…å®¹ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç›¸å…³ç« èŠ‚æ ‡é¢˜ï¼ˆç›¸å…³æ€§ 0.2-0.4ï¼‰
   ä¾‹å¦‚ï¼šç« èŠ‚æ ‡é¢˜ã€ç›®å½•ã€è¡¨æ ¼æ ‡é¢˜ã€æ³¨é‡Šè¯´æ˜ç­‰
   
âœ… ä»»ä½•è¡¨æ ¼ã€åˆ—è¡¨ã€ç»“æ„åŒ–æ•°æ®éƒ½åº”è¯¥å®Œæ•´æå–
   å³ä½¿ä¸ç¡®å®šæ˜¯å¦ç›¸å…³ï¼Œä¹Ÿåº”è¯¥æå–ï¼ˆç›¸å…³æ€§å¯ä»¥æ ‡ä½ä¸€äº›ï¼‰

âŒ åªæœ‰å®Œå…¨æ— å…³çš„å†…å®¹æ‰ä¸æå–
   ä¾‹å¦‚ï¼šå®Œå…¨ä¸æ¶‰åŠç›®æ ‡å­—æ®µçš„å…¶ä»–è¯é¢˜ç« èŠ‚

ğŸ“ ç‰‡æ®µé€‰æ‹©ç­–ç•¥ï¼š
- ä¼˜å…ˆé€‰æ‹©å®Œæ•´æ®µè½/ç« èŠ‚è€Œéå¥å­ç‰‡æ®µ
- å•ä¸ªç‰‡æ®µæœ€å¤§é•¿åº¦: {max_segment_length} å­—ç¬¦
- å¦‚æœç›¸é‚»ç‰‡æ®µéƒ½å¯èƒ½ç›¸å…³ï¼Œåˆå¹¶å®ƒä»¬æˆä¸ºä¸€ä¸ªå¤§ç‰‡æ®µ
- å¦‚æœæ•´ä¸ªæ–‡æ¡£éƒ½ç›¸å…³æˆ–å¯èƒ½ç›¸å…³ï¼Œå¯ä»¥è¿”å›å®Œæ•´æ–‡æ¡£ä½œä¸ºå•ä¸ªç‰‡æ®µ
- **é‡è¦**ï¼šå¯¹äºè¡¨æ ¼ã€åˆ—è¡¨ã€ç»“æ„åŒ–æ•°æ®ï¼Œä¸€å®šè¦å®Œæ•´æå–æ•´ä¸ªè¡¨æ ¼/åˆ—è¡¨
- å¯ä»¥é€‚å½“æ¸…ç†å†…å®¹ä¸­çš„å¤šä½™ç©ºæ ¼ã€è¿ç»­æ¢è¡Œç­‰æ ¼å¼å™ªéŸ³ï¼Œä¿æŒå†…å®¹å¯è¯»å³å¯

ğŸ“ é¢å¤–æç¤º: {nl_prompt if nl_prompt else 'æ— '}

âš ï¸ è®°ä½ï¼šæˆ‘ä»¬çš„ç›®æ ‡æ˜¯**ä¸è¦æ¼æ‰ä»»ä½•å¯èƒ½æœ‰ç”¨çš„ä¿¡æ¯**ï¼Œæå–å®½æ³›ä¸€äº›æ€»æ¯”é—æ¼å…³é”®ä¿¡æ¯è¦å¥½ï¼

=== æ–‡æ¡£å†…å®¹å¼€å§‹ ===
ï¼ˆæ³¨æ„ï¼šä¸‹é¢çš„åˆ†éš”çº¿"=== æ–‡æ¡£å†…å®¹å¼€å§‹ ==="ä»…ç”¨äºæ ‡è®°ä½ç½®ï¼Œä¸æ˜¯æ–‡æ¡£çš„ä¸€éƒ¨åˆ†ï¼
ä½ è¿”å›çš„contentå­—æ®µå¿…é¡»åªåŒ…å«æ–‡æ¡£å†…çš„åŸå§‹æ–‡æœ¬ï¼Œä¸è¦åŒ…å«è¿™äº›åˆ†éš”æ ‡è®°ï¼ï¼‰

{text_content}

=== æ–‡æ¡£å†…å®¹ç»“æŸ ===

=== è¾“å‡ºè¦æ±‚ ===
âš ï¸ é‡è¦æé†’ï¼š
1. contentå­—æ®µå¿…é¡»æ˜¯ä»ä¸Šé¢"æ–‡æ¡£å†…å®¹å¼€å§‹"å’Œ"æ–‡æ¡£å†…å®¹ç»“æŸ"ä¹‹é—´æå–çš„æ–‡æœ¬
2. ä¸è¦åŒ…å«ä»»ä½•åˆ†éš”æ ‡è®°ï¼ˆå¦‚"===ç›®æ ‡è¡¨æ ¼ä¿¡æ¯===" "===æ–‡æ¡£å†…å®¹å¼€å§‹===" "===æ–‡æ¡£å†…å®¹ç»“æŸ==="ç­‰ï¼‰
3. å¯ä»¥é€‚å½“æ¸…ç†å¤šä½™çš„ç©ºæ ¼ï¼ˆå¤šä¸ªè¿ç»­ç©ºæ ¼å¯ä»¥åˆå¹¶ä¸ºä¸€ä¸ªï¼‰ã€å¤šä½™çš„æ¢è¡Œç­‰æ ¼å¼é—®é¢˜
4. ä¿ç•™æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹å’Œç»“æ„ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´

è¿”å›æ‰€æœ‰å®šä½åˆ°çš„ç›¸å…³æˆ–å¯èƒ½ç›¸å…³çš„ç‰‡æ®µã€‚å®å¯å¤šè¿”å›ä¹Ÿä¸è¦é—æ¼ã€‚
å¦‚æœæ•´ä¸ªæ–‡æ¡£éƒ½ç›¸å…³æˆ–å¯èƒ½ç›¸å…³ï¼Œå¯ä»¥è¿”å›ä¸€ä¸ªåŒ…å«å®Œæ•´æ–‡æ¡£çš„ç‰‡æ®µã€‚
åªæœ‰åœ¨æ–‡æ¡£å†…å®¹å®Œå…¨æ— å…³æ—¶æ‰è¿”å›ç©ºæ•°ç»„ã€‚

è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ç»“æœï¼š
[
  {{
    "content": "æ–‡æ¡£ä¸­æå–çš„æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‚å½“æ¸…ç†æ ¼å¼ï¼‰",
    "relevance_score": 0.85
  }},
  {{
    "content": "å¦ä¸€æ®µç›¸å…³å†…å®¹ï¼ˆå¯é€‚å½“æ¸…ç†æ ¼å¼ï¼‰",
    "relevance_score": 0.45
  }},
  ...
]

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
"""
        return prompt
    
    def _clean_prompt_markers(self, content: str) -> str:
        """
        æ¸…ç†LLMè¿”å›å†…å®¹ä¸­å¯èƒ½æ··å…¥çš„promptæ ‡è®°
        
        Args:
            content: LLMè¿”å›çš„å†…å®¹
            
        Returns:
            æ¸…ç†åçš„å†…å®¹
        """
        markers_to_remove = [
            '=== ç›®æ ‡è¡¨æ ¼ä¿¡æ¯ ===',
            '=== ä»»åŠ¡è¦æ±‚ ===',
            '=== æ–‡æ¡£å†…å®¹ ===',
            '=== æ–‡æ¡£å†…å®¹å¼€å§‹ ===',
            '=== æ–‡æ¡£å†…å®¹ç»“æŸ ===',
            '=== è¾“å‡ºè¦æ±‚ ===',
        ]
        
        cleaned = content
        for marker in markers_to_remove:
            if marker in cleaned:
                self.logger.warning(f'âš ï¸ æ£€æµ‹åˆ°LLMè¿”å›å†…å®¹ä¸­åŒ…å«promptæ ‡è®°: "{marker}"ï¼Œæ­£åœ¨æ¸…ç†...')
                parts = cleaned.split(marker)
                if len(parts) > 1:
                    cleaned = parts[1].strip()
        
        return cleaned
    
    def _validate_content_in_original(self, content: str, original_text: str) -> bool:
        """
        éªŒè¯LLMè¿”å›çš„contentæ˜¯å¦çœŸçš„å­˜åœ¨äºåŸæ–‡ä¸­ï¼ˆå…è®¸ç©ºç™½ç¬¦å·®å¼‚ï¼‰
        
        Args:
            content: LLMè¿”å›çš„ç‰‡æ®µå†…å®¹
            original_text: åŸå§‹æ–‡æ¡£
            
        Returns:
            Trueè¡¨ç¤ºå†…å®¹æœ‰æ•ˆï¼ŒFalseè¡¨ç¤ºå¯èƒ½æ··å…¥äº†promptæ ‡è®°
        """
        import re
        
        prompt_indicators = [
            '=== ç›®æ ‡è¡¨æ ¼ä¿¡æ¯',
            '=== ä»»åŠ¡è¦æ±‚',
            '=== æ–‡æ¡£å†…å®¹',
            '=== è¾“å‡ºè¦æ±‚',
            'å­—æ®µå®šä¹‰:',
            'relevance_score'
        ]
        
        for indicator in prompt_indicators:
            if indicator in content[:200]:  # åªæ£€æŸ¥å‰200ä¸ªå­—ç¬¦
                self.logger.warning(f'âš ï¸ æ£€æµ‹åˆ°contentä¸­åŒ…å«promptæ ‡è®°: "{indicator}"')
                return False
        
        if content in original_text:
            return True
        
        content_stripped = content.strip()
        if content_stripped in original_text:
            return True
        
        content_normalized = re.sub(r'\s+', ' ', content_stripped.lower())
        
        if len(content_normalized) < 30:
            original_normalized = re.sub(r'\s+', ' ', original_text.lower())
            return content_normalized in original_normalized
        
        anchor_length = min(100, len(content_normalized))
        anchor = content_normalized[:anchor_length]
        
        original_lower = original_text.lower()
        window_size = anchor_length + 500  # ç»™äºˆä¸€å®šå®¹é”™ç©ºé—´
        
        for start_idx in range(0, len(original_lower) - anchor_length + 1, 100):
            end_idx = min(start_idx + window_size, len(original_lower))
            window = original_lower[start_idx:end_idx]
            window_normalized = re.sub(r'\s+', ' ', window)
            
            if anchor in window_normalized:
                return True
        
        return False
    
    def _parse_llm_response(self,
                           response: str,
                           original_text: str,
                           source_document: str = "") -> List[LocatedSegment]:
        """
        è§£æ LLM è¿”å›çš„ JSON æ ¼å¼ç»“æœ
        
        Args:
            response: LLM è¿”å›çš„æ–‡æœ¬
            original_text: åŸå§‹æ–‡æ¡£æ–‡æœ¬
            source_document: æ¥æºæ–‡æ¡£åç§°
            
        Returns:
            è½¬æ¢åçš„LocatedSegmentåˆ—è¡¨
        """
        segments = []
        
        if not response or not response.strip():
            self.logger.warning(f'âš ï¸ LLMå“åº”ä¸ºç©ºï¼Œè¿”å›å®Œæ•´æ–‡æ¡£ä½œä¸ºfallback')
            return [LocatedSegment(
                content=original_text,
                start_position=0,
                end_position=len(original_text),
                relevance_score=1.0,
                source_document=source_document
            )]
        
        try:
            response = response.strip()
            
            if response.startswith('```'):
                lines = response.split('\n')
                if len(lines) > 1:
                    response = '\n'.join(lines[1:])
                if response.endswith('```'):
                    response = response[:-3].strip()
            
            parsed_segments = json.loads(response)
            
            if not isinstance(parsed_segments, list):
                raise ValueError("è¿”å›çš„ä¸æ˜¯æ•°ç»„æ ¼å¼")
            
            if not parsed_segments:
                self.logger.warning(f'âš ï¸ LLMè¿”å›ç©ºæ•°ç»„ï¼Œä½¿ç”¨å®Œæ•´æ–‡æ¡£ä½œä¸ºfallback')
                return [LocatedSegment(
                    content=original_text,
                    start_position=0,
                    end_position=len(original_text),
                    relevance_score=1.0,
                    source_document=source_document
                )]
            
            
            if len(parsed_segments) == 0:
                self.logger.warning(f'âš ï¸ æœªèƒ½è§£æå‡ºä»»ä½•ç‰‡æ®µï¼Œå°†ä½¿ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹')
            
            for i, seg_dict in enumerate(parsed_segments):
                try:
                    content = seg_dict.get('content', '')
                    
                    if not content or not content.strip():
                        continue
                    
                    if not self._validate_content_in_original(content, original_text):
                        self.logger.warning(f'âš ï¸ ç‰‡æ®µ {i+1} å†…å®¹æœªåœ¨åŸæ–‡ä¸­æ‰¾åˆ°ï¼Œå¯èƒ½æ··å…¥äº†promptæ ‡è®°ï¼Œå°è¯•æ¸…ç†...')
                        cleaned_content = self._clean_prompt_markers(content)
                        if cleaned_content != content and self._validate_content_in_original(cleaned_content, original_text):
                            self.logger.info(f'âœ… ç‰‡æ®µ {i+1} æ¸…ç†æˆåŠŸï¼Œä½¿ç”¨æ¸…ç†åçš„å†…å®¹')
                            content = cleaned_content
                        else:
                            self.logger.warning(f'âš ï¸ ç‰‡æ®µ {i+1} æ¸…ç†å¤±è´¥ï¼Œè·³è¿‡æ­¤ç‰‡æ®µ')
                            continue
                    
                    import re
                    content_stripped = content.strip()
                    
                    start_pos = original_text.find(content_stripped)
                    if start_pos >= 0:
                        end_pos = start_pos + len(content_stripped)
                    else:
                        anchor_length = min(100, len(content_stripped))
                        if anchor_length > 20:  # ç¡®ä¿é”šç‚¹è¶³å¤Ÿé•¿
                            anchor = content_stripped[:anchor_length]
                            anchor_normalized = re.sub(r'\s+', ' ', anchor.lower().strip())
                            
                            original_lower = original_text.lower()
                            window_size = len(anchor) + 300  # ç»™äºˆä¸€å®šå®¹é”™ç©ºé—´
                            best_match_pos = -1
                            
                            for start_idx in range(0, len(original_text) - len(anchor_normalized) + 1, 100):
                                end_idx = min(start_idx + window_size, len(original_text))
                                window = original_lower[start_idx:end_idx]
                                window_normalized = re.sub(r'\s+', ' ', window.strip())
                                
                                if anchor_normalized in window_normalized:
                                    best_match_pos = start_idx
                                    break
                            
                            if best_match_pos >= 0:
                                start_pos = best_match_pos
                                estimated_end = best_match_pos + int(len(content_stripped) * 1.2)  # å…è®¸20%çš„é•¿åº¦å·®å¼‚
                                end_pos = min(estimated_end, len(original_text))
                            else:
                                simple_anchor = anchor[:50] if len(anchor) >= 50 else anchor
                                anchor_pos = original_lower.find(simple_anchor.lower())
                                if anchor_pos >= 0:
                                    start_pos = anchor_pos
                                    end_pos = anchor_pos + len(content_stripped)
                                else:
                                    self.logger.warning(f'âš ï¸ ç‰‡æ®µ {i+1} æ— æ³•å®šä½ï¼ˆé”šç‚¹: "{anchor[:30]}..."ï¼‰ï¼Œfallbackåˆ°å®Œæ•´æ–‡æ¡£')
                                    start_pos = 0
                                    end_pos = len(original_text)
                        else:
                            self.logger.warning(f'âš ï¸ ç‰‡æ®µ {i+1} å†…å®¹å¤ªçŸ­({len(content_stripped)}å­—ç¬¦)ï¼Œfallbackåˆ°å®Œæ•´æ–‡æ¡£')
                            start_pos = 0
                            end_pos = len(original_text)
                    
                    relevance_score = float(seg_dict.get('relevance_score', 0.8))
                    
                    segment = LocatedSegment(
                        content=content,
                        start_position=start_pos,
                        end_position=end_pos,
                        relevance_score=relevance_score,
                        source_document=source_document
                    )
                    
                    segments.append(segment)
                    preview = content[:80].replace('\n', ' ') + '...' if len(content) > 80 else content.replace('\n', ' ')
                    
                except Exception as e:
                    self.logger.warning(f"è§£æç‰‡æ®µ {i+1} å¤±è´¥: {e}")
                    continue
            
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            self.logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´æ–‡æ¡£ä½œä¸ºfallback")
            segments = [LocatedSegment(
                content=original_text,
                start_position=0,
                end_position=len(original_text),
                relevance_score=1.0,
                source_document=source_document
            )]
        except Exception as e:
            self.logger.error(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            segments = [LocatedSegment(
                content=original_text,
                start_position=0,
                end_position=len(original_text),
                relevance_score=1.0,
                source_document=source_document
            )]
        
        if not segments:
            segments = [LocatedSegment(
                content=original_text,
                start_position=0,
                end_position=len(original_text),
                relevance_score=1.0,
                source_document=source_document
            )]
        
        self._print_location_stats(segments, original_text)
        
        return segments
    
    def _print_location_stats(self, segments: List[LocatedSegment], original_text: str):
        """æ‰“å°å®šä½ç»Ÿè®¡ä¿¡æ¯"""
        if not segments:
            return
        
        total_located_chars = sum(len(seg.content) for seg in segments)
        coverage_ratio = total_located_chars / len(original_text) if len(original_text) > 0 else 0
        avg_relevance = sum(seg.relevance_score for seg in segments) / len(segments)
        
        
        high_relevance = sum(1 for seg in segments if seg.relevance_score >= 0.7)
        medium_relevance = sum(1 for seg in segments if 0.4 <= seg.relevance_score < 0.7)
        low_relevance = sum(1 for seg in segments if seg.relevance_score < 0.4)
        
    
    def merge_segments(self, segments: List[LocatedSegment]) -> str:
        """
        åˆå¹¶å¤šä¸ªç‰‡æ®µä¸ºå•ä¸€æ–‡æœ¬ï¼ˆç”¨äºåç»­æå–ï¼‰
        
        Args:
            segments: å®šä½åˆ°çš„ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            str: åˆå¹¶åçš„æ–‡æœ¬å†…å®¹ï¼ˆåŒ…å«æ¥æºæ–‡æ¡£æ ‡æ³¨å’Œç›¸å…³æ€§ä¿¡æ¯ï¼‰
        """
        if not segments:
            return ""
        
        if len(segments) == 1:
            seg = segments[0]
            header = self._build_segment_header(seg, 1, 1)
            return f"{header}\n\n{seg.content}"
        
        sorted_segments = sorted(
            segments,
            key=lambda s: (-s.relevance_score, s.start_position if s.start_position >= 0 else float('inf'))
        )
        
        merged_parts = []
        for i, seg in enumerate(sorted_segments, 1):
            header = self._build_segment_header(seg, i, len(sorted_segments))
            merged_parts.append(f"{header}\n\n{seg.content}")
        
        separator = "\n\n" + "="*80 + "\n\n"
        merged = separator.join(merged_parts)
        
        return merged
    
    def _build_segment_header(self, segment: LocatedSegment, index: int, total: int) -> str:
        """
        æ„å»ºç‰‡æ®µå¤´éƒ¨æ ‡æ³¨ä¿¡æ¯
        
        Args:
            segment: ç‰‡æ®µå¯¹è±¡
            index: ç‰‡æ®µç¼–å·
            total: æ€»ç‰‡æ®µæ•°
            
        Returns:
            str: æ ¼å¼åŒ–çš„å¤´éƒ¨æ ‡æ³¨
        """
        header_lines = [
            "=" * 80,
            f"ã€ç‰‡æ®µ {index}/{total}ã€‘",
        ]
        
        if segment.source_document:
            header_lines.append(f"æ¥æºæ–‡æ¡£: {segment.source_document}")
        
        header_lines.append(f"ç›¸å…³æ€§å¾—åˆ†: {segment.relevance_score:.2f}")
        header_lines.append("=" * 20)
        
        return "\n".join(header_lines)
    
    
    def concatenate_documents(self, 
                             text_contents: List[str], 
                             document_names: List[str] = None) -> Tuple[str, List[Dict[str, Any]]]:
        """
        å°†å¤šä¸ªæ–‡æ¡£æ‹¼æ¥æˆä¸€ä¸ªå¤§æ–‡æ¡£ï¼Œå¹¶æ ‡æ³¨æ¯ä¸ªæ–‡æ¡£çš„è¾¹ç•Œ
        
        Args:
            text_contents: æ–‡æ¡£å†…å®¹åˆ—è¡¨
            document_names: æ–‡æ¡£åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ doc_1, doc_2...ï¼‰
            
        Returns:
            Tuple[str, List[Dict]]: 
                - æ‹¼æ¥åçš„å®Œæ•´æ–‡æ¡£
                - æ–‡æ¡£è¾¹ç•Œä¿¡æ¯åˆ—è¡¨ [{'name': 'doc1', 'start': 0, 'end': 100}, ...]
        """
        if not text_contents:
            return "", []
        
        if document_names is None or len(document_names) != len(text_contents):
            document_names = [f"doc_{i+1}" for i in range(len(text_contents))]
        
        
        concatenated_parts = []
        document_boundaries = []
        current_position = 0
        
        for i, (text_content, doc_name) in enumerate(zip(text_contents, document_names)):
            separator = f"\n{'='*80}\nã€æ–‡æ¡£ {i+1}: {doc_name}ã€‘\n{'='*80}\n\n"
            
            if i > 0:  # ç¬¬ä¸€ä¸ªæ–‡æ¡£å‰ä¸åŠ åˆ†éš”ç¬¦
                concatenated_parts.append(separator)
                current_position += len(separator)
            
            doc_start = current_position
            concatenated_parts.append(text_content)
            current_position += len(text_content)
            doc_end = current_position
            
            document_boundaries.append({
                'name': doc_name,
                'start': doc_start,
                'end': doc_end,
                'length': len(text_content)
            })
            
        
        concatenated_text = "".join(concatenated_parts)
        
        
        return concatenated_text, document_boundaries
    
    def locate_from_multi_documents(self,
                                    text_contents: List[str],
                                    document_names: List[str],
                                    schema: Dict[str, Any],
                                    table_name: str,
                                    nl_prompt: str = "",
                                    context = None) -> List[LocatedSegment]:
        """
        ä»å¤šä¸ªæ–‡æ¡£ä¸­å®šä½ç›¸å…³ç‰‡æ®µï¼ˆç”¨äºentity_extractorçš„batchæ¨¡å¼ï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•å¤„ç†å·²ç»åˆ†å¥½çš„batchï¼Œä¸å†è¿›è¡Œé¢å¤–çš„åˆ†æ‰¹ã€‚
        unified_orchestrator å·²ç»è´Ÿè´£äº†æ–‡æ¡£çš„åˆ†æ‰¹ç­–ç•¥ã€‚
        
        Args:
            text_contents: å¤šä¸ªæ–‡æ¡£å†…å®¹åˆ—è¡¨ï¼ˆå·²ç»æ˜¯ä¸€ä¸ªbatchï¼‰
            document_names: æ–‡æ¡£åç§°åˆ—è¡¨
            schema: æ•°æ®åº“schemaå®šä¹‰
            table_name: ç›®æ ‡è¡¨å
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            context: å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            List[LocatedSegment]: å®šä½åˆ°çš„ç›¸å…³ç‰‡æ®µåˆ—è¡¨ï¼ˆå¸¦æ¥æºæ ‡æ³¨ï¼‰
        """
        if document_names is None or len(document_names) != len(text_contents):
            document_names = [f"doc_{i+1}" for i in range(len(text_contents))]
        
        self.logger.info(f'å¤šæ–‡æ¡£å®šä½: {len(text_contents)} ä¸ªæ–‡æ¡£')
        
        if self.enable_cache:
            combined_text = '\n\n'.join(text_contents)
            cache_key = self._generate_cache_key(combined_text, schema, table_name, nl_prompt)
            cached_segments = self._load_from_cache(cache_key)
            
            if cached_segments is not None:
                self.logger.info(f"ä»ç¼“å­˜åŠ è½½å¤šæ–‡æ¡£å®šä½ç»“æœ: {len(cached_segments)} ä¸ªç‰‡æ®µ")
                return cached_segments
        
        concatenated_text, document_boundaries = self.concatenate_documents(
            text_contents, document_names
        )
        
        if not self.should_locate(concatenated_text):
            self.logger.info('æ–‡æ¡£é•¿åº¦æœªè¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›å®Œæ•´æ–‡æ¡£')
            segments = []
            for text_content, doc_name in zip(text_contents, document_names):
                segments.append(LocatedSegment(
                    content=text_content,
                    start_position=0,
                    end_position=len(text_content),
                    relevance_score=1.0,
                    source_document=doc_name
                ))
            return segments
        
        self.logger.info(f'æ–‡æ¡£éœ€è¦å®šä½ï¼Œæ€»é•¿åº¦: {len(concatenated_text)} å­—ç¬¦')
        
        source_doc_name = document_names[0] if len(document_names) == 1 else "concatenated"
        
        located_segments = self.locate_relevant_segments(
            text_content=concatenated_text,
            schema=schema,
            table_name=table_name,
            nl_prompt=nl_prompt,
            context=context,
            source_document=source_doc_name
        )
        
        total_segments = len(located_segments)
        fallback_segments = 0
        concat_length = len(concatenated_text)
        
        for segment in located_segments:
            if (segment.start_position == 0 and 
                segment.end_position >= concat_length * 0.95):  # å…è®¸5%çš„è¯¯å·®
                fallback_segments += 1
        
        fallback_ratio = fallback_segments / total_segments if total_segments > 0 else 0
        has_unreliable_positions = fallback_ratio > 0.5
        
        if has_unreliable_positions:
            self.logger.warning(
                f'âš ï¸ æ£€æµ‹åˆ°locateæ•ˆæœä¸ä½³ï¼š{fallback_segments}/{total_segments} '
                f'ä¸ªç‰‡æ®µä¸ºå®Œæ•´æ–‡æ¡£ ({fallback_ratio*100:.1f}%)ï¼Œæ”¾å¼ƒlocate'
            )
        
        if has_unreliable_positions:
            segments_with_source = []
            for doc_idx, (text_content, doc_name) in enumerate(zip(text_contents, document_names)):
                segments_with_source.append(LocatedSegment(
                    content=text_content,
                    start_position=0,
                    end_position=len(text_content),
                    relevance_score=1.0,  # å®Œæ•´æ–‡æ¡£ï¼Œç›¸å…³æ€§è®¾ä¸º1.0
                    source_document=doc_name
                ))
                self.logger.info(f'  è¿”å›å®Œæ•´æ–‡æ¡£ {doc_idx+1}: {doc_name} ({len(text_content)} å­—ç¬¦)')
        else:
            self.logger.info(
                f'âœ… LocateæˆåŠŸï¼š{total_segments - fallback_segments}/{total_segments} '
                f'ä¸ªç‰‡æ®µæˆåŠŸå®šä½ ({(1-fallback_ratio)*100:.1f}%)ï¼Œä½¿ç”¨å®šä½ç»“æœ'
            )
            segments_with_source = []
            for seg_idx, segment in enumerate(located_segments):
                segment_start = segment.start_position
                source_doc = "unknown"
                
                for boundary in document_boundaries:
                    if boundary['start'] <= segment_start < boundary['end']:
                        source_doc = boundary['name']
                        break
                
                new_segment = LocatedSegment(
                    content=segment.content,
                    start_position=segment.start_position,
                    end_position=segment.end_position,
                    relevance_score=segment.relevance_score,
                    source_document=source_doc
                )
                segments_with_source.append(new_segment)
        
        if self.enable_cache:
            combined_text = '\n\n'.join(text_contents)
            cache_key = self._generate_cache_key(combined_text, schema, table_name, nl_prompt)
            metadata = {
                'table_name': table_name,
                'document_count': len(text_contents),
                'document_names': document_names
            }
            self._save_to_cache(cache_key, segments_with_source, metadata)
        
        self.logger.info(f'å¤šæ–‡æ¡£å®šä½å®Œæˆ: {len(segments_with_source)} ä¸ªç‰‡æ®µ')
        return segments_with_source
    
    
    def get_segments_summary(self, segments: List[LocatedSegment]) -> Dict[str, Any]:
        """
        è·å–ç‰‡æ®µæ‘˜è¦ä¿¡æ¯
        
        Args:
            segments: ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            Dict: æ‘˜è¦ä¿¡æ¯
        """
        if not segments:
            return {
                'total_segments': 0,
                'total_length': 0,
                'average_relevance': 0.0
            }
        
        return {
            'total_segments': len(segments),
            'total_length': sum(len(seg.content) for seg in segments),
            'average_relevance': sum(seg.relevance_score for seg in segments) / len(segments),
            'segments_info': [
                {
                    'length': len(seg.content),
                    'relevance': seg.relevance_score
                }
                for seg in segments
            ]
        }
    
    def clear_cache(self, older_than_days: int = None) -> int:
        """
        æ¸…ç†ç¼“å­˜æ–‡ä»¶
        
        Args:
            older_than_days: åªæ¸…ç†æŒ‡å®šå¤©æ•°ä¹‹å‰çš„ç¼“å­˜ï¼ˆNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰ç¼“å­˜ï¼‰
            
        Returns:
            int: æ¸…ç†çš„æ–‡ä»¶æ•°é‡
        """
        if not self.enable_cache:
            return 0
        
        cache_path = Path(self.cache_dir)
        if not cache_path.exists():
            return 0
        
        cache_files = list(cache_path.glob("*.json"))
        
        if not cache_files:
            return 0
        
        deleted_count = 0
        
        if older_than_days is None:
            for cache_file in cache_files:
                try:
                    cache_file.unlink()
                    deleted_count += 1
                except Exception as e:
                    self.logger.warning(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, {e}")
            
        else:
            import time
            cutoff_time = time.time() - (older_than_days * 24 * 60 * 60)
            
            for cache_file in cache_files:
                try:
                    if cache_file.stat().st_mtime < cutoff_time:
                        cache_file.unlink()
                        deleted_count += 1
                except Exception as e:
                    self.logger.warning(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, {e}")
            
        
        return deleted_count
    
    def clear_multi_document_cache(self) -> int:
        """
        æ¸…ç†å¤šæ–‡æ¡£æ¨¡å¼çš„ç¼“å­˜æ–‡ä»¶ï¼ˆä¿ç•™å•æ–‡æ¡£æ¨¡å¼çš„ç¼“å­˜ï¼‰
        
        é€šè¿‡æ£€æŸ¥ç¼“å­˜æ–‡ä»¶çš„metadataå­—æ®µæ¥åŒºåˆ†ï¼š
        - å¤šæ–‡æ¡£æ¨¡å¼: metadataä¸­åŒ…å« 'document_names' æˆ– 'document_count' > 1
        - å•æ–‡æ¡£æ¨¡å¼: metadataä¸­åªæœ‰ 'source_document'
        
        Returns:
            int: æ¸…ç†çš„æ–‡ä»¶æ•°é‡
        """
        if not self.enable_cache:
            return 0
        
        cache_path = Path(self.cache_dir)
        if not cache_path.exists():
            return 0
        
        cache_files = list(cache_path.glob("*.json"))
        
        if not cache_files:
            return 0
        
        
        deleted_count = 0
        single_doc_count = 0
        error_count = 0
        
        for cache_file in cache_files:
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                metadata = cache_data.get('metadata', {})
                
                is_multi_doc = (
                    'document_names' in metadata or 
                    metadata.get('document_count', 0) > 1
                )
                
                if is_multi_doc:
                    cache_file.unlink()
                    deleted_count += 1
                    doc_names = metadata.get('document_names', [])
                    doc_count = metadata.get('document_count', len(doc_names))
                else:
                    single_doc_count += 1
                    source_doc = metadata.get('source_document', 'unknown')
                    
            except Exception as e:
                error_count += 1
                self.logger.warning(f"å¤„ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, {e}")
        
        if error_count > 0:
            self.logger.warning(f"âš ï¸ æ¸…ç†ç¼“å­˜æ—¶é‡åˆ° {error_count} ä¸ªé”™è¯¯")
        
        return deleted_count
    
    def get_cache_info(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ä¿¡æ¯
        
        Returns:
            Dict: ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.enable_cache:
            return {
                'enabled': False,
                'cache_dir': self.cache_dir,
                'total_files': 0,
                'total_size': 0
            }
        
        cache_path = Path(self.cache_dir)
        if not cache_path.exists():
            return {
                'enabled': True,
                'cache_dir': self.cache_dir,
                'total_files': 0,
                'total_size': 0
            }
        
        cache_files = list(cache_path.glob("*.json"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        return {
            'enabled': True,
            'cache_dir': str(cache_path.absolute()),
            'total_files': len(cache_files),
            'total_size': total_size,
            'total_size_mb': round(total_size / (1024 * 1024), 2)
        }


def example_locate_segments():
    """æ–‡æ¡£å®šä½å™¨ä½¿ç”¨ç¤ºä¾‹
    
    æ¼”ç¤ºï¼š
    1. ä½¿ç”¨ should_locate åˆ¤æ–­æ˜¯å¦éœ€è¦å®šä½
    2. å¦‚æœéœ€è¦ï¼Œè°ƒç”¨ locate_relevant_segments
    3. å¦‚æœä¸éœ€è¦ï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡æ¡£
    """
    
    locator = DocumentLocator(
        word_threshold=500  # æ¼”ç¤ºç”¨çš„å°é˜ˆå€¼
    )
    
    long_document = """
å…¬å¸2023å¹´åº¦å‘˜å·¥ä¿¡æ¯æ±‡æ€»æŠ¥å‘Š

ä¸€ã€ç ”å‘éƒ¨é—¨
1. å¼ ä¸‰ï¼Œ28å²ï¼Œé«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæœˆè–ª15000å…ƒï¼Œ2020å¹´3æœˆ1æ—¥å…¥èŒã€‚
   æ“…é•¿Pythonå’ŒJavaå¼€å‘ï¼Œå‚ä¸è¿‡å¤šä¸ªæ ¸å¿ƒé¡¹ç›®ã€‚
   
2. æå››ï¼Œ32å²ï¼ŒæŠ€æœ¯æ€»ç›‘ï¼Œæœˆè–ª25000å…ƒï¼Œ2018å¹´8æœˆ15æ—¥å…¥èŒã€‚
   è´Ÿè´£æŠ€æœ¯å›¢é˜Ÿç®¡ç†ï¼Œæœ‰10å¹´ä»¥ä¸Šå¼€å‘ç»éªŒã€‚

äºŒã€äº§å“éƒ¨é—¨  
1. ç‹äº”ï¼Œ29å²ï¼Œäº§å“ç»ç†ï¼Œæœˆè–ª18000å…ƒï¼Œ2021å¹´6æœˆ1æ—¥å…¥èŒã€‚
   è´Ÿè´£äº§å“è§„åˆ’å’Œéœ€æ±‚åˆ†æã€‚
   
2. èµµå…­ï¼Œ35å²ï¼Œäº§å“æ€»ç›‘ï¼Œæœˆè–ª28000å…ƒï¼Œ2017å¹´4æœˆ20æ—¥å…¥èŒã€‚
   è´Ÿè´£äº§å“æˆ˜ç•¥è§„åˆ’ã€‚

ä¸‰ã€å¸‚åœºéƒ¨é—¨
1. å­™ä¸ƒï¼Œ26å²ï¼Œå¸‚åœºä¸“å‘˜ï¼Œæœˆè–ª12000å…ƒï¼Œ2022å¹´9æœˆ1æ—¥å…¥èŒã€‚
   è´Ÿè´£å¸‚åœºæ¨å¹¿æ´»åŠ¨ã€‚
   
å››ã€å…¶ä»–ä¿¡æ¯
å…¬å¸æ€»éƒ¨ä½äºåŒ—äº¬ï¼Œæˆç«‹äº2015å¹´ï¼Œç›®å‰æœ‰å‘˜å·¥200ä½™äºº...
    """ * 50  # é‡å¤50æ¬¡æ¨¡æ‹Ÿé•¿æ–‡æ¡£
    
    schema = {
        'tables': [{
            'name': 'employees',
            'attributes': [
                {'name': 'name', 'type': 'TEXT', 'description': 'å‘˜å·¥å§“å'},
                {'name': 'age', 'type': 'INTEGER', 'description': 'å¹´é¾„'},
                {'name': 'position', 'type': 'TEXT', 'description': 'èŒä½'},
                {'name': 'salary', 'type': 'DECIMAL', 'description': 'æœˆè–ª'},
                {'name': 'hire_date', 'type': 'DATE', 'description': 'å…¥èŒæ—¥æœŸ'}
            ]
        }]
    }
    
    if locator.should_locate(long_document):
        print(f"æ–‡æ¡£é•¿åº¦: {len(long_document)} å­—ç¬¦ï¼Œéœ€è¦å®šä½")
        
        segments = locator.locate_relevant_segments(
            text_content=long_document,
            schema=schema,
            table_name='employees',
            nl_prompt="æå–æ‰€æœ‰å‘˜å·¥çš„åŸºæœ¬ä¿¡æ¯"
        )
        
        summary = locator.get_segments_summary(segments)
        print(f"å®šä½åˆ° {summary['total_segments']} ä¸ªç‰‡æ®µ")
        
        merged_text = locator.merge_segments(segments)
        
        return segments, merged_text
    else:
        print(f"æ–‡æ¡£é•¿åº¦: {len(long_document)} å­—ç¬¦ï¼Œä¸éœ€è¦å®šä½ï¼Œç›´æ¥ä½¿ç”¨")
        return [], long_document


if __name__ == "__main__":
    example_locate_segments()

