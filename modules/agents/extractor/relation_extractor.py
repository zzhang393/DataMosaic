"""
å…³ç³»æå–å™¨æ¨¡å—
è´Ÿè´£ä»æ–‡æ¡£ä¸­æå–å…³ç³»è¡¨æ•°æ®ï¼ˆCold Startå’ŒWarm Startï¼‰
"""
import os
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

from ...memory import TableRow, CellData
from ...core.ids import IdGenerator

from llm.main import get_answer

try:
    from pydantic import BaseModel, Field
    from langchain_core.output_parsers import PydanticOutputParser
    from langchain_openai import ChatOpenAI
    from langchain.output_parsers import OutputFixingParser
    STRUCTURED_OUTPUT_AVAILABLE = True
except ImportError:
    STRUCTURED_OUTPUT_AVAILABLE = False


class RelationExtractor:
    """å…³ç³»æå–å™¨ - å¤„ç†å…³ç³»è¡¨çš„æ•°æ®æå–"""
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger('doc2db.extractor.relation')
        self.entity_mention_log_file = None
        self._mention_handler = None
    
    def _setup_entity_mention_logging(self, output_dir: str):
        """è®¾ç½®å®ä½“æåŠæ—¥å¿—è®°å½•åˆ°æ–‡ä»¶"""
        if not output_dir or self._mention_handler:
            return  # å·²ç»è®¾ç½®è¿‡æˆ–æ²¡æœ‰output_dir
        
        from pathlib import Path
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        self.entity_mention_log_file = output_path / 'entity_mentions.log'
        
        self._mention_handler = logging.FileHandler(self.entity_mention_log_file, mode='a', encoding='utf-8')
        self._mention_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        self._mention_handler.setFormatter(formatter)
        
        if not any(isinstance(h, logging.FileHandler) and 
                  hasattr(h, 'baseFilename') and 
                  'entity_mentions.log' in h.baseFilename 
                  for h in self.logger.handlers):
            self.logger.addHandler(self._mention_handler)
            self.logger.setLevel(logging.DEBUG)
            self.logger.info(f"ğŸ“ å®ä½“æåŠæ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {self.entity_mention_log_file}")
    
    def extract_cold_start(self, text_content: str, table_def: Dict[str, Any],
                          nl_prompt: str, doc_source: str, context=None,
                          doc_index: int = 0, full_schema: Dict[str, Any] = None,
                          get_model_api_config_func=None,
                          create_list_model_func=None,
                          build_schema_context_func=None,
                          parse_structured_response_func=None,
                          record_extraction_step_func=None,
                          fallback_func=None,
                          extraction_strategy: str = 'entity_anchored',
                          use_batch_processing: bool = True,
                          batch_size: int = 20) -> List[TableRow]:
        """
        å…³ç³»è¡¨çš„Cold Startæå– - æ”¯æŒå¤šç§æå–ç­–ç•¥
        
        Args:
            extraction_strategy: æå–ç­–ç•¥é€‰æ‹©
                - "entity_anchored": Entity-anchored blocked extraction
                - "global": ä¼ ç»Ÿå…¨å±€æå–
                ä¼˜å…ˆçº§ï¼šæ–¹æ³•å‚æ•° > table_def['relation_extraction']['strategy'] > é»˜è®¤å€¼'global'
            use_batch_processing: æ˜¯å¦ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼ˆä»…å¯¹entity_anchoredç­–ç•¥æœ‰æ•ˆï¼‰
                - True: æ‰¹é‡å¤„ç†å¤šä¸ªå®ä½“ï¼Œå‡å°‘LLMè°ƒç”¨æ¬¡æ•°ï¼ˆæ¨èï¼‰
                - False: é€ä¸ªå¤„ç†å®ä½“ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
            batch_size: æ‰¹å¤„ç†æ—¶æ¯æ‰¹çš„å®ä½“æ•°é‡ï¼ˆé»˜è®¤20ï¼‰
        
        Entity-anchoredç­–ç•¥ï¼š
        1. ä»¥ç¬¬ä¸€ä¸ªå‚è€ƒè¡¨ä¸ºanchorï¼ˆå¦‚Studentè¡¨ï¼‰
        2. å¯¹æ¯ä¸ªanchorå®ä½“ï¼Œè°ƒç”¨LLMæ‰¾åˆ°æ–‡æ¡£ä¸­æåŠå®ƒçš„å¥å­
        3. åœ¨è¿™äº›å¥å­ä¸­æå–ä¸è¯¥å®ä½“ç›¸å…³çš„å…³ç³»
        4. å¤æ‚åº¦ä»O(mÃ—n)é™ä½åˆ°O(m)æ¬¡LLMè°ƒç”¨
        5. æ‰¹å¤„ç†æ¨¡å¼è¿›ä¸€æ­¥é™ä½åˆ°O(m/batch_size)æ¬¡LLMè°ƒç”¨
        
        Globalç­–ç•¥ï¼š
        1. ä¸€æ¬¡æ€§ä»æ•´ä¸ªæ–‡æ¡£æå–æ‰€æœ‰å…³ç³»
        2. é€‚åˆæ–‡æ¡£è¾ƒçŸ­æˆ–å…³ç³»è¾ƒå°‘çš„åœºæ™¯
        """
        if not STRUCTURED_OUTPUT_AVAILABLE:
            return fallback_func(text_content, table_def, nl_prompt, doc_source, context, doc_index)
        
        try:
            table_name = table_def.get('name', 'data_table')
            
            relation_extraction_config = table_def.get('relation_extraction', {})
            
            self.logger.info(f"ğŸ” [DEBUG] å¼€å§‹å¤„ç†å…³ç³»è¡¨ {table_name}")
            self.logger.info(f"   åŸå§‹text_contenté•¿åº¦: {len(text_content)} å­—ç¬¦")
            self.logger.info(f"   relation_extractioné…ç½®: {relation_extraction_config.keys() if relation_extraction_config else 'None'}")
            
            text_content = self._load_document_sources(relation_extraction_config, text_content, context)
            
            self.logger.info(f"   åŠ è½½åtext_contenté•¿åº¦: {len(text_content)} å­—ç¬¦")
            if 'å­¦ç”Ÿå…¥å­¦ç™»è®°è¡¨' in text_content:
                self.logger.warning(f"   âš ï¸ text_contentä¸­åŒ…å«'å­¦ç”Ÿå…¥å­¦ç™»è®°è¡¨'ï¼Œå¯èƒ½åŠ è½½äº†é”™è¯¯çš„æ–‡æ¡£")
            if 'æ•™åŠ¡ç³»ç»Ÿè¿è¡Œæ—¥å¿—' in text_content or 'é€‰è¯¾' in text_content:
                self.logger.info(f"   âœ… text_contentä¸­åŒ…å«'æ•™åŠ¡ç³»ç»Ÿè¿è¡Œæ—¥å¿—'æˆ–'é€‰è¯¾'ç›¸å…³å†…å®¹ï¼Œæ–‡æ¡£æ­£ç¡®")
            
            reference_tables_data = None
            
            if relation_extraction_config.get('enabled', False):
                self.logger.info(f"ğŸ”— [Entity-Anchored] åŠ è½½å‚è€ƒè¡¨æ•°æ®...")
                reference_tables_data = self._load_reference_tables(relation_extraction_config, context)
                if reference_tables_data:
                    self.logger.info(f"âœ… [Entity-Anchored] æˆåŠŸåŠ è½½ {len(reference_tables_data)} ä¸ªå‚è€ƒè¡¨")
            
            if not reference_tables_data and context:
                reference_tables_data = self._get_entity_snapshots_as_reference(context, table_def)
                if reference_tables_data:
                    self.logger.info(f"âœ… [Entity-Anchored] ä»contextè·å– {len(reference_tables_data)} ä¸ªå®ä½“è¡¨æ•°æ®")
            
            strategy = extraction_strategy

            
            if strategy == 'entity_anchored':
                if not reference_tables_data:
                    self.logger.warning(f"âš ï¸ é…ç½®è¦æ±‚ä½¿ç”¨entity_anchoredç­–ç•¥ï¼Œä½†æ²¡æœ‰å‚è€ƒè¡¨æ•°æ®ï¼Œé™çº§ä¸ºå…¨å±€æå–")
                    strategy = 'global'
                else:
                    self.logger.info(f"ğŸ“Œ [ç­–ç•¥é€‰æ‹©] ä½¿ç”¨Entity-Anchoredç­–ç•¥ï¼ˆç­–ç•¥é…ç½®: {strategy}ï¼‰")
            
            if strategy == 'global':
                self.logger.info(f"ğŸ“Œ [ç­–ç•¥é€‰æ‹©] ä½¿ç”¨å…¨å±€æå–ç­–ç•¥ï¼ˆç­–ç•¥é…ç½®: {strategy}ï¼‰")
                return self._extract_global(
                    text_content, table_def, nl_prompt, doc_source, context, doc_index,
                    full_schema, reference_tables_data, get_model_api_config_func, 
                    create_list_model_func, build_schema_context_func, 
                    parse_structured_response_func, record_extraction_step_func, fallback_func
                )
            
            anchor_table_name = list(reference_tables_data.keys())[0]
            anchor_entities = reference_tables_data[anchor_table_name]
            
            self.logger.info(
                f"ğŸ¯ [Entity-Anchored] ä½¿ç”¨é”šç‚¹è¡¨: {anchor_table_name}, "
                f"å…± {len(anchor_entities)} ä¸ªé”šç‚¹å®ä½“"
            )
            
            model = context.model if context and hasattr(context, 'model') else "gpt-4o"
            api_config = get_model_api_config_func(model)
            
            all_relation_rows = []
            
            if use_batch_processing:

                anchor_entities_with_mentions = self._find_entity_mentions_via_llm_batch(
                    text_content, anchor_entities, anchor_table_name, 
                    model, api_config, context, batch_size=batch_size
                )
                
                all_relation_rows = self._extract_relations_for_anchors_batch(
                    anchor_entities_with_mentions, table_def,
                    reference_tables_data, relation_extraction_config,
                    model, api_config, doc_source,
                    create_list_model_func, build_schema_context_func,
                    parse_structured_response_func, full_schema,
                    batch_size=batch_size
                )
                
            else:
                self.logger.info(f"ğŸŒ [é€ä¸ªå¤„ç†æ¨¡å¼] ä½¿ç”¨ä¼ ç»Ÿä¸²è¡Œå¤„ç†")
                
                for i, anchor_entity in enumerate(anchor_entities):
                    self.logger.info(
                        f"  [{i+1}/{len(anchor_entities)}] å¤„ç†é”šç‚¹: {anchor_entity}"
                    )
                    
                    relevant_sentences = self._find_entity_mentions_via_llm(
                        text_content, anchor_entity, anchor_table_name, model, api_config, context
                    )
                    
                    if not relevant_sentences:
                        self.logger.debug(f"    æœªæ‰¾åˆ°æåŠè¯¥å®ä½“çš„å¥å­ï¼Œè·³è¿‡")
                        continue
                    
                    self.logger.info(f"    æ‰¾åˆ° {len(relevant_sentences)} ä¸ªç›¸å…³å¥å­")
                    
                    rows = self._extract_relations_for_anchor(
                        anchor_entity, relevant_sentences, table_def, 
                        reference_tables_data, relation_extraction_config,
                        model, api_config, doc_source,
                        create_list_model_func, build_schema_context_func,
                        parse_structured_response_func, full_schema
                    )
                    
                    all_relation_rows.extend(rows)
                    self.logger.info(f"    æå–åˆ° {len(rows)} æ¡å…³ç³»è®°å½•")
            
            unique_rows = self._deduplicate_relation_rows(all_relation_rows)
            
            self.logger.info(
                f"âœ… [Entity-Anchored] å®Œæˆï¼š{len(all_relation_rows)} æ¡åŸå§‹è®°å½• "
                f"-> {len(unique_rows)} æ¡å»é‡åè®°å½•"
            )
            
            record_extraction_step_func(
                context, table_name, doc_index, doc_source, 
                'entity_anchored_relation_extraction', model, 
                f"Entity-Anchored extraction with {len(anchor_entities)} anchors",
                f"Extracted {len(unique_rows)} unique relations", 
                len(unique_rows)
            )
            
            return unique_rows
            
        except Exception as e:
            self.logger.error(f"âŒ [Entity-Anchored] å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return fallback_func(text_content, table_def, nl_prompt, doc_source, context, doc_index)
    
    def extract_warm_start(self, text_content: str, table_def: Dict[str, Any],
                          nl_prompt: str, doc_source: str, context=None,
                          doc_index: int = 0, previous_snapshot=None,
                          violations: List = None,
                          check_warm_start_limit_func=None,
                          get_model_api_config_func=None,
                          create_list_model_func=None,
                          build_schema_context_func=None,
                          parse_structured_response_func=None,
                          record_extraction_step_func=None,
                          full_schema: Dict[str, Any] = None) -> List[TableRow]:
        """
        å…³ç³»è¡¨çš„Warm Startæå– - Entity-Anchoredå¢é‡é‡æå–
        
        è®ºæ–‡é€»è¾‘ï¼š
        1. ä»violationè¯†åˆ«æ¶‰åŠçš„anchor entities
        2. åªé’ˆå¯¹è¿™äº›entitiesé‡æ–°è¿è¡Œentity-anchoredæå–
        3. ç§»é™¤å—å½±å“çš„æ—§è®°å½•ï¼Œæ·»åŠ æ–°è®°å½•ï¼Œä¿ç•™æ— å…³è®°å½•
        
        Args:
            text_content: æ–‡æ¡£å†…å®¹
            table_def: è¡¨å®šä¹‰
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            doc_source: æ–‡æ¡£æ¥æº
            context: å¤„ç†ä¸Šä¸‹æ–‡
            doc_index: æ–‡æ¡£ç´¢å¼•
            previous_snapshot: ä¸Šä¸€æ¬¡å¿«ç…§
            violations: è¿è§„åˆ—è¡¨
            check_warm_start_limit_func: æ£€æŸ¥warm starté™åˆ¶çš„å‡½æ•°
            get_model_api_config_func: è·å–æ¨¡å‹APIé…ç½®å‡½æ•°
            create_list_model_func: åˆ›å»ºåˆ—è¡¨æ¨¡å‹å‡½æ•°
            build_schema_context_func: æ„å»ºschemaä¸Šä¸‹æ–‡å‡½æ•°
            parse_structured_response_func: è§£æç»“æ„åŒ–å“åº”å‡½æ•°
            record_extraction_step_func: è®°å½•æå–æ­¥éª¤å‡½æ•°
            full_schema: å®Œæ•´schemaå®šä¹‰
            
        Returns:
            æ›´æ–°åçš„TableRowåˆ—è¡¨
        """
        table_name = table_def.get('name') or table_def.get('table_name', 'unknown')
        
        if not check_warm_start_limit_func(context, table_name):
            return previous_snapshot.rows if previous_snapshot else []
        
        self.logger.info(f"ğŸ”„ [å…³ç³»Warm Start] å¼€å§‹Entity-Anchoredå¢é‡é‡æå– - è¡¨: {table_name}")
        
        affected_anchor_entities = self._identify_affected_anchors(
            violations, previous_snapshot, table_def, context
        )
        
        if not affected_anchor_entities:
            self.logger.info(f"âš ï¸ [å…³ç³»Warm Start] æœªè¯†åˆ«åˆ°å—å½±å“çš„anchor entitiesï¼Œä¿æŒåŸå¿«ç…§")
            return previous_snapshot.rows if previous_snapshot else []
        
        self.logger.info(
            f"ğŸ¯ [å…³ç³»Warm Start] è¯†åˆ«åˆ° {len(affected_anchor_entities)} ä¸ªå—å½±å“çš„anchor entities"
        )
        
        relation_extraction_config = table_def.get('relation_extraction', {})
        text_content = self._load_document_sources(relation_extraction_config, text_content, context)
        
        reference_tables_data = None
        if relation_extraction_config.get('enabled', False):
            reference_tables_data = self._load_reference_tables(relation_extraction_config, context)
        
        if not reference_tables_data and context:
            reference_tables_data = self._get_entity_snapshots_as_reference(context, table_def)
        
        if not reference_tables_data:
            self.logger.warning(f"âš ï¸ [å…³ç³»Warm Start] æ— å‚è€ƒè¡¨æ•°æ®ï¼Œæ— æ³•æ‰§è¡Œentity-anchoredé‡æå–ï¼Œä¿æŒåŸå¿«ç…§")
            return previous_snapshot.rows if previous_snapshot else []
        
        model = context.model if context and hasattr(context, 'model') else "gpt-4o"
        api_config = get_model_api_config_func(model)
        
        re_extracted_rows = []
        anchor_table_name = list(reference_tables_data.keys())[0]
        
        for i, anchor_entity in enumerate(affected_anchor_entities):
            anchor_desc = ", ".join([f"{k}={v}" for k, v in anchor_entity.items()])
            self.logger.info(f"  [{i+1}/{len(affected_anchor_entities)}] ğŸ”„ é‡æ–°æå–anchor: {anchor_desc}")
            
            relevant_sentences = self._find_entity_mentions_via_llm(
                text_content, anchor_entity, anchor_table_name,
                model, api_config, context
            )
            
            if relevant_sentences:
                rows = self._extract_relations_for_anchor(
                    anchor_entity, relevant_sentences, table_def,
                    reference_tables_data, relation_extraction_config,
                    model, api_config, doc_source,
                    create_list_model_func, build_schema_context_func,
                    parse_structured_response_func, full_schema
                )
                re_extracted_rows.extend(rows)
                self.logger.info(f"    æå–åˆ° {len(rows)} æ¡å…³ç³»")
            else:
                self.logger.info(f"    æœªæ‰¾åˆ°ç›¸å…³æ–‡æœ¬ï¼Œè¯¥anchoræ— å…³ç³»è®°å½•")
        
        updated_rows = self._merge_warm_start_results(
            previous_snapshot.rows if previous_snapshot else [],
            re_extracted_rows,
            affected_anchor_entities,
            table_def
        )
        
        self.logger.info(
            f"âœ… [å…³ç³»Warm Start] å®Œæˆï¼šåŸ {len(previous_snapshot.rows if previous_snapshot else [])} æ¡ "
            f"-> é‡æå– {len(re_extracted_rows)} æ¡ -> æœ€ç»ˆ {len(updated_rows)} æ¡è®°å½•"
        )
        
        if record_extraction_step_func:
            record_extraction_step_func(
                context, table_name, doc_index, doc_source,
                'entity_anchored_warm_start', model,
                f"Re-extracted {len(affected_anchor_entities)} affected anchors",
                f"Updated to {len(updated_rows)} rows (re-extracted {len(re_extracted_rows)})",
                len(updated_rows)
            )
        
        return updated_rows
    
    def _get_entity_snapshots_as_reference(self, context, table_def: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """ä»contextä¸­è·å–å·²æŠ½å–çš„å®ä½“è¡¨æ•°æ®ä½œä¸ºå‚è€ƒ
        
        ç”¨äºå…³ç³»æå–æ—¶ï¼Œå°†å·²æŠ½å–çš„å®ä½“è¡¨æ•°æ®è½¬æ¢ä¸ºå‚è€ƒæ ¼å¼
        """
        reference_data = {}
        
        if not hasattr(context, 'all_snapshots') or not context.all_snapshots:
            return reference_data
        
        attributes = table_def.get('attributes', table_def.get('fields', []))
        referenced_tables = set()
        
        for attr in attributes:
            constraints = attr.get('constraints', {})
            if constraints.get('foreign_key'):
                field_name = attr.get('name', '')
                if field_name.endswith('ID') or field_name.endswith('_id'):
                    ref_table = field_name[:-2] if field_name.endswith('ID') else field_name[:-3]
                    if ref_table:
                        referenced_tables.add(ref_table)
        
        for table_name, snapshot in context.all_snapshots.items():
            if any(table_name.lower() == ref.lower() for ref in referenced_tables):
                rows_data = []
                if hasattr(snapshot, 'rows') and snapshot.rows:
                    for row in snapshot.rows:
                        row_dict = {}
                        for field_name, cell_data in row.cells.items():
                            if hasattr(cell_data, 'value'):
                                row_dict[field_name] = cell_data.value
                        if row_dict:
                            rows_data.append(row_dict)
                
                if rows_data:
                    reference_data[table_name] = rows_data
                    self.logger.info(f"ğŸ“‹ ä»contextè·å–å‚è€ƒè¡¨ {table_name}: {len(rows_data)} è¡Œ")
        
        return reference_data
    
    def _find_project_root(self) -> Optional[Path]:
        """æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å«datasetç›®å½•çš„ç›®å½•ï¼‰
        
        ä»å½“å‰ç›®å½•å‘ä¸ŠæŸ¥æ‰¾ï¼Œç›´åˆ°æ‰¾åˆ°åŒ…å«datasetç›®å½•çš„ç›®å½•
        """
        current = Path.cwd()
        max_depth = 5  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚
        
        for _ in range(max_depth):
            if (current / 'dataset').exists():
                return current
            
            parent = current.parent
            if parent == current:  # å·²åˆ°æ ¹ç›®å½•
                break
            current = parent
        
        return None
    
    def _load_document_sources(self, relation_config: Dict[str, Any], 
                               default_text_content: str, context=None) -> str:
        """ä»é…ç½®çš„document_sourcesåŠ è½½æ–‡æ¡£å†…å®¹
        
        å¦‚æœé…ç½®äº†document_sourcesï¼Œç›´æ¥ä»æŒ‡å®šè·¯å¾„è¯»å–æ–‡æ¡£ï¼Œå¿½ç•¥ä¼ å…¥çš„text_contentã€‚
        è¿™æ ·å¯ä»¥ç»•è¿‡orchestratorçš„batchåˆå¹¶é€»è¾‘ï¼Œç²¾ç¡®æ§åˆ¶å…³ç³»è¡¨çš„æ•°æ®æºã€‚
        
        Args:
            relation_config: relation_extractioné…ç½®
            default_text_content: é»˜è®¤çš„æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰é…ç½®document_sourcesåˆ™ä½¿ç”¨ï¼‰
            context: å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            æ–‡æ¡£å†…å®¹ï¼ˆå¯èƒ½æ˜¯ä»document_sourcesåŠ è½½çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯é»˜è®¤çš„ï¼‰
        """
        self.logger.info(f"ğŸ” [_load_document_sources] å¼€å§‹æ£€æŸ¥document_sourcesé…ç½®")
        self.logger.info(f"   relation_configç±»å‹: {type(relation_config)}, å†…å®¹: {relation_config}")
        
        if not relation_config:
            self.logger.info(f"   âš ï¸ relation_configä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤text_content")
            return default_text_content
        
        document_sources = relation_config.get('document_sources', [])
        self.logger.info(f"   æ£€æµ‹åˆ°document_sources: {document_sources}")
        
        if not document_sources:
            self.logger.info(f"   âš ï¸ document_sourcesä¸ºç©ºåˆ—è¡¨ï¼Œä½¿ç”¨é»˜è®¤text_content")
            return default_text_content
        
        self.logger.info(f"ğŸ¯ æ£€æµ‹åˆ°document_sourcesé…ç½®ï¼Œå°†ç›´æ¥è¯»å–æŒ‡å®šæ–‡æ¡£: {document_sources}")
        
        merged_content = []
        
        for doc_source in document_sources:
            try:
                doc_path = Path(doc_source)
                
                if not doc_path.is_absolute():
                    if context and hasattr(context, 'workspace_path'):
                        doc_path = Path(context.workspace_path) / doc_source
                    else:
                        candidate1 = Path.cwd() / doc_source
                        candidate2 = Path.cwd().parent / doc_source if Path.cwd().name == 'backend' else None
                        project_root = self._find_project_root()
                        candidate3 = project_root / doc_source if project_root else None
                        
                        if candidate1.exists():
                            doc_path = candidate1
                        elif candidate2 and candidate2.exists():
                            doc_path = candidate2
                            self.logger.info(f"âœ… ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•è·¯å¾„: {doc_path}")
                        elif candidate3 and candidate3.exists():
                            doc_path = candidate3
                            self.logger.info(f"âœ… æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•: {doc_path}")
                        else:
                            doc_path = candidate1  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå€™é€‰
                
                if not doc_path.exists():
                    self.logger.error(f"âŒ æ–‡æ¡£æºä¸å­˜åœ¨: {doc_path}")
                    continue
                
                with open(doc_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if content:
                    merged_content.append(f"=== Document: {doc_path.name} ===\n")
                    merged_content.append(content)
                    merged_content.append(f"\n=== End of {doc_path.name} ===\n\n")
                    self.logger.info(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {doc_path.name} ({len(content)} å­—ç¬¦)")
                
            except Exception as e:
                self.logger.error(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {doc_source}: {e}")
                continue
        
        if merged_content:
            result = "\n".join(merged_content)
            self.logger.info(f"âœ… ä»document_sourcesåŠ è½½äº† {len(document_sources)} ä¸ªæ–‡æ¡£ï¼Œæ€»è®¡ {len(result)} å­—ç¬¦")
            return result
        else:
            self.logger.warning(f"âš ï¸ document_sourcesé…ç½®çš„æ–‡æ¡£éƒ½åŠ è½½å¤±è´¥ï¼Œé™çº§ä½¿ç”¨ä¼ å…¥çš„text_content")
            return default_text_content
    
    def _load_reference_tables(self, relation_config: Dict[str, Any], 
                              context=None) -> Dict[str, List[Dict[str, Any]]]:
        """åŠ è½½å…³ç³»æŠ½å–æ‰€éœ€çš„å‚è€ƒè¡¨æ•°æ®
        
        Args:
            relation_config: relation_extractioné…ç½®
            context: æå–ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½åŒ…å«workspaceè·¯å¾„ç­‰ä¿¡æ¯ï¼‰
            
        Returns:
            å­—å…¸ï¼Œkeyä¸ºè¡¨åï¼Œvalueä¸ºè¯¥è¡¨çš„æ•°æ®åˆ—è¡¨
        """
        reference_tables = {}
        
        reference_configs = relation_config.get('reference_tables', [])
        if not reference_configs:
            return reference_tables
        
        for ref_config in reference_configs:
            table_name = ref_config.get('table')
            data_source = ref_config.get('data_source')
            key_fields = ref_config.get('key_fields', [])
            
            if not table_name or not data_source:
                self.logger.warning(f"âš ï¸ å‚è€ƒè¡¨é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡: {ref_config}")
                continue
            
            try:
                data_path = Path(data_source)
                if not data_path.is_absolute():
                    if context and hasattr(context, 'workspace_path'):
                        data_path = Path(context.workspace_path) / data_source
                    else:
                        candidate1 = Path.cwd() / data_source
                        candidate2 = Path.cwd().parent / data_source if Path.cwd().name == 'backend' else None
                        candidate3 = self._find_project_root() / data_source if self._find_project_root() else None
                        
                        if candidate1.exists():
                            data_path = candidate1
                        elif candidate2 and candidate2.exists():
                            data_path = candidate2
                            self.logger.info(f"âœ… ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•è·¯å¾„: {data_path}")
                        elif candidate3 and candidate3.exists():
                            data_path = candidate3
                            self.logger.info(f"âœ… æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•: {data_path}")
                        else:
                            data_path = candidate1  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå€™é€‰
                
                if not data_path.exists():
                    self.logger.error(f"âŒ å‚è€ƒè¡¨æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                    if not data_path.is_absolute():
                        self.logger.error(f"   æç¤º: å½“å‰å·¥ä½œç›®å½•ä¸º {Path.cwd()}")
                    continue
                
                with open(data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if table_name in data:
                    table_data = data[table_name]
                    
                    reference_tables[table_name] = table_data
                    
                    fields_info = f" (key_fields: {key_fields})" if key_fields else ""
                    self.logger.info(f"âœ… åŠ è½½å‚è€ƒè¡¨ {table_name}: {len(table_data)} è¡Œ{fields_info}")
                else:
                    self.logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°è¡¨ {table_name}")
                
            except Exception as e:
                self.logger.error(f"âŒ åŠ è½½å‚è€ƒè¡¨ {table_name} å¤±è´¥: {e}")
                continue
        
        return reference_tables
    
    def _build_reference_tables_prompt(self, reference_tables_data: Dict[str, List[Dict[str, Any]]], 
                                       relation_config: Dict[str, Any]) -> str:
        """æ„å»ºåŒ…å«å‚è€ƒè¡¨æ•°æ®çš„promptéƒ¨åˆ†
        
        Args:
            reference_tables_data: å‚è€ƒè¡¨æ•°æ®å­—å…¸
            relation_config: relation_extractioné…ç½®
            
        Returns:
            æ ¼å¼åŒ–çš„promptå­—ç¬¦ä¸²
        """
        if not reference_tables_data:
            return ""
        
        prompt_parts = ["\nã€Reference Tables for Relationship Extractionã€‘"]
        
        extraction_hint = relation_config.get('extraction_hint', '')
        if extraction_hint:
            prompt_parts.append(f"\nTask: {extraction_hint}\n")
        
        for table_name, table_data in reference_tables_data.items():
            if not table_data:
                continue
            
            table_config = None
            for ref_config in relation_config.get('reference_tables', []):
                if ref_config.get('table') == table_name:
                    table_config = ref_config
                    break
            
            prompt_parts.append(f"\n{table_name} Table:")
            
            if table_config and table_config.get('description'):
                prompt_parts.append(f"  Description: {table_config['description']}")
            
            if table_data:
                fields = list(table_data[0].keys())
                
                header = "  | " + " | ".join(fields) + " |"
                separator = "  |" + "|".join(["-" * (len(f) + 2) for f in fields]) + "|"
                prompt_parts.append(header)
                prompt_parts.append(separator)
                
                max_rows = min(50, len(table_data))
                for row in table_data[:max_rows]:
                    values = [str(row.get(f, '')) for f in fields]
                    data_row = "  | " + " | ".join(values) + " |"
                    prompt_parts.append(data_row)
                
                if len(table_data) > max_rows:
                    prompt_parts.append(f"  ... (total {len(table_data)} rows)")
            
            prompt_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        prompt_parts.append("IMPORTANT Instructions for Relationship Extraction:")
        prompt_parts.append("- Use the reference tables above to match entity names/descriptions to their IDs")
        prompt_parts.append("- For foreign key fields, MUST use the exact ID values from the reference tables")
        
        matching_instructions = []
        for table_name, table_data in reference_tables_data.items():
            if table_data:
                sample_row = table_data[0]
                id_field = None
                name_field = None
                
                for field in sample_row.keys():
                    if 'id' in field.lower():
                        id_field = field
                    elif 'name' in field.lower():
                        name_field = field
                
                if id_field and name_field:
                    matching_instructions.append(
                        f"- Match {table_name} entities by their {name_field} to {table_name}.{id_field}"
                    )
        
        if matching_instructions:
            prompt_parts.extend(matching_instructions)
        
        prompt_parts.append("- Extract ONLY the relationships (ID pairs) that are explicitly mentioned in the document")
        prompt_parts.append("")
        
        return "\n".join(prompt_parts)
    
    def _extract_global(self, text_content: str, table_def: Dict[str, Any],
                       nl_prompt: str, doc_source: str, context=None,
                       doc_index: int = 0, full_schema: Dict[str, Any] = None,
                       reference_tables_data: Dict[str, List[Dict]] = None,
                       get_model_api_config_func=None,
                       create_list_model_func=None,
                       build_schema_context_func=None,
                       parse_structured_response_func=None,
                       record_extraction_step_func=None,
                       fallback_func=None) -> List[TableRow]:
        """
        ä¼ ç»Ÿçš„å…¨å±€å…³ç³»æå–
        """
        relation_extraction_config = table_def.get('relation_extraction', {})
        
        self.logger.info(f"ğŸ” [_extract_global] å¼€å§‹å…¨å±€å…³ç³»æå–")
        self.logger.info(f"   åŸå§‹text_contenté•¿åº¦: {len(text_content)} å­—ç¬¦")
        
        text_content = self._load_document_sources(relation_extraction_config, text_content, context)
        
        self.logger.info(f"   åŠ è½½åtext_contenté•¿åº¦: {len(text_content)} å­—ç¬¦")
        
        temp_schema = {'tables': [table_def]}
        table_name = table_def.get('name', 'data_table')
        
        list_model = create_list_model_func(temp_schema, table_name)
        if not list_model:
            return fallback_func(text_content, table_def, nl_prompt, doc_source, context, doc_index)
        
        model = context.model if context and hasattr(context, 'model') else "gpt-4o"
        api_config = get_model_api_config_func(model)
        api_base = api_config['api_base']
        api_key = api_config['api_key']
        
        llm_kwargs = {"model": model, "temperature": 0}
        if api_base:
            llm_kwargs["base_url"] = api_base
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        parser = PydanticOutputParser(pydantic_object=list_model)
        fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)
        
        schema_context = build_schema_context_func(table_def, full_schema=full_schema)
        
        reference_tables_prompt = self._build_reference_tables_prompt(
            reference_tables_data or {}, relation_extraction_config
        )
        
        prompt = f"""Please extract structured data from the following text according to the schema for table "{table_name}".

{schema_context}

{reference_tables_prompt}

Extract ALL relevant records for the "{table_name}" table from the document. Each record should be a separate item in the list.

Text content:
{text_content}

{parser.get_format_instructions()}

IMPORTANT: 
- Return a JSON object with a "rows" field containing a list of all extracted records for "{table_name}".
- Extract EVERY occurrence of relevant data for this table, even if mentioned multiple times.
- For foreign key fields, use exact values from the reference tables provided above.
"""

        if nl_prompt and nl_prompt.strip():
            prompt += f"\nAdditional Instructions: {nl_prompt}\n"
        
        response = llm.invoke(prompt)
        llm_response = response.content
        
        rows = parse_structured_response_func(
            llm_response, fixing_parser, table_name, doc_source
        )
        
        record_extraction_step_func(
            context, table_name, doc_index, doc_source, 
            'global_relation_extraction', model, prompt, llm_response, len(rows)
        )
        
        return rows
    
    def _find_entity_mentions_via_llm_batch(self, text_content: str, 
                                           anchor_entities: List[Dict[str, Any]],
                                           anchor_table_name: str, model: str,
                                           api_config: Dict[str, str], 
                                           context=None,
                                           batch_size: int = 20) -> Dict[str, List[str]]:
        """
        æ‰¹é‡ä½¿ç”¨LLMæ‰¾åˆ°æ–‡æ¡£ä¸­æåŠå¤šä¸ªé”šç‚¹å®ä½“çš„æ–‡æœ¬å—(chunk)
        
        ç›¸æ¯”å•ä¸ªå¤„ç†ï¼Œæ‰¹å¤„ç†çš„ä¼˜åŠ¿ï¼š
        1. å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°ï¼ˆNæ¬¡ -> N/batch_sizeæ¬¡ï¼‰
        2. æ›´é«˜ååé‡
        3. é™ä½æ€»å»¶è¿Ÿ
        
        Args:
            text_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            anchor_entities: é”šç‚¹å®ä½“åˆ—è¡¨
            anchor_table_name: é”šç‚¹è¡¨å
            model: ä½¿ç”¨çš„æ¨¡å‹
            api_config: APIé…ç½®
            context: å¤„ç†ä¸Šä¸‹æ–‡
            batch_size: æ¯æ‰¹å¤„ç†çš„å®ä½“æ•°é‡
            
        Returns:
            å­—å…¸ï¼Œkeyä¸ºå®ä½“æè¿°ï¼Œvalueä¸ºåŒ…å«è¯¥å®ä½“çš„æ–‡æœ¬å—åˆ—è¡¨
            ä¾‹å¦‚: {"StudentID=s1, StudentName=Alice": ["chunk1", "chunk2"], ...}
        """
        if context:
            output_dir = None
            if hasattr(context, 'io_manager') and hasattr(context.io_manager, 'output_dir'):
                output_dir = context.io_manager.output_dir
            elif hasattr(context, 'output_dir'):
                output_dir = context.output_dir
            if output_dir:
                self._setup_entity_mention_logging(output_dir)
        
        self.logger.info(f"ğŸš€ [æ‰¹å¤„ç†] å¼€å§‹æ‰¹é‡æŸ¥æ‰¾ {len(anchor_entities)} ä¸ªå®ä½“çš„æåŠ")
        
        all_results = {}
        
        for batch_idx in range(0, len(anchor_entities), batch_size):
            batch = anchor_entities[batch_idx:batch_idx + batch_size]
            self.logger.info(f"  æ‰¹æ¬¡ {batch_idx//batch_size + 1}: å¤„ç† {len(batch)} ä¸ªå®ä½“")
            
            entities_list = []
            entity_keys = []  # ç”¨äºæ˜ å°„è¿”å›ç»“æœ
            for entity in batch:
                entity_desc = ", ".join([f"{k}={v}" for k, v in entity.items()])
                entities_list.append(entity_desc)
                entity_keys.append(entity_desc)
            
            system_prompt = """You are a document analysis expert. Your task is to find all relevant text chunks 
that mention specific entities and provide sufficient context for understanding relationships."""
            
            prompt = f"""Find all text chunks in the document that mention ANY of the following entities from the {anchor_table_name} table.

TARGET ENTITIES:
{chr(10).join([f"{i+1}. {desc}" for i, desc in enumerate(entities_list)])}

DOCUMENT TO SEARCH:
{text_content}

TASK:
- For EACH entity (by its ID or name), find ALL text passages that mention it
- Extract complete sentences with context (2-5 sentences per chunk)
- Include surrounding text to help understand relationships

OUTPUT FORMAT (STRICTLY FOLLOW):
<entity id="1">
<chunk>Text passage mentioning entity 1...</chunk>
<chunk>Another passage mentioning entity 1...</chunk>
</entity>
<entity id="2">
<chunk>Text passage mentioning entity 2...</chunk>
</entity>

IMPORTANT:
- You MUST process ALL {len(entities_list)} entities listed above
- If an entity is not mentioned in the document, write: <entity id="X">NO_MENTION</entity>
- Do NOT skip any entity
- Use the exact entity ID numbers (1, 2, 3, etc.)
"""
            
            try:
                llm_response = get_answer(prompt, system_prompt=system_prompt, model=model)
                
                self.logger.debug(f"  [DEBUG] LLMåŸå§‹å“åº”é•¿åº¦: {len(llm_response)} å­—ç¬¦")
                self.logger.debug(f"  [DEBUG] LLMå“åº”å‰500å­—ç¬¦:\n{llm_response[:500]}")
                
                self.logger.info(f"ğŸ“‹ [æ‰¹æ¬¡ {batch_idx//batch_size + 1}] å¤„ç† {len(batch)} ä¸ªå®ä½“çš„æåŠæŸ¥æ‰¾")
                for idx, entity_desc in enumerate(entities_list, 1):
                    self.logger.debug(f"   å®ä½“ {idx}: {entity_desc}")
                
                import re
                
                for i, entity_key in enumerate(entity_keys):
                    entity_num = i + 1
                    
                    entity_pattern = rf'<entity\s+id\s*=\s*["\']?{entity_num}["\']?\s*>(.*?)</entity>'
                    entity_match = re.search(entity_pattern, llm_response, re.DOTALL | re.IGNORECASE)
                    
                    chunks = []
                    if entity_match:
                        entity_content = entity_match.group(1)
                        self.logger.debug(f"  [DEBUG] å®ä½“ {entity_num} åŒ¹é…æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(entity_content)}")
                        
                        if "NO_MENTION" not in entity_content:
                            chunk_matches = re.findall(r'<chunk>(.*?)</chunk>', entity_content, re.DOTALL)
                            
                            for chunk in chunk_matches:
                                chunk = chunk.strip()
                                if chunk and len(chunk) > 20:
                                    if len(chunk) > 800:
                                        chunk = chunk[:800] + "..."
                                    chunks.append(chunk)
                            
                            if not chunks:
                                self.logger.debug(f"  [DEBUG] å®ä½“ {entity_num} æ²¡æœ‰<chunk>æ ‡ç­¾ï¼Œå°è¯•é™çº§è§£æ")
                                potential_chunks = entity_content.strip().split('\n\n')
                                for chunk in potential_chunks:
                                    chunk = chunk.strip()
                                    if chunk and len(chunk) > 30:
                                        if len(chunk) > 800:
                                            chunk = chunk[:800] + "..."
                                        chunks.append(chunk)
                    else:
                        self.logger.debug(f"  [DEBUG] å®ä½“ {entity_num} æœªåŒ¹é…åˆ°<entity>æ ‡ç­¾")
                    
                    chunks = chunks[:10]
                    all_results[entity_key] = chunks
                    
                    if chunks:
                        self.logger.info(f"    âœ… å®ä½“ {entity_num} ({entity_key[:50]}...): {len(chunks)} ä¸ªchunks")
                        for i, chunk in enumerate(chunks, 1):
                            chunk_preview = chunk[:200] + "..." if len(chunk) > 200 else chunk
                            self.logger.debug(f"      Chunk {i}: {chunk_preview}")
                    else:
                        self.logger.info(f"    âš ï¸ å®ä½“ {entity_num} ({entity_key[:50]}...): æœªæ‰¾åˆ°")
                
            except Exception as e:
                self.logger.error(f"âŒ æ‰¹é‡æŸ¥æ‰¾å®ä½“æåŠå¤±è´¥ï¼ˆæ‰¹æ¬¡ {batch_idx//batch_size + 1}ï¼‰: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
                for entity_key in entity_keys:
                    all_results[entity_key] = []
        
        successful_count = sum(1 for chunks in all_results.values() if chunks)
        self.logger.info(f"âœ… [æ‰¹å¤„ç†] å®Œæˆï¼š{len(anchor_entities)} ä¸ªå®ä½“ä¸­æœ‰ {successful_count} ä¸ªæ‰¾åˆ°æåŠ")
        
        return all_results
    
    def _find_entity_mentions_via_llm(self, text_content: str, anchor_entity: Dict[str, Any],
                                     anchor_table_name: str, model: str,
                                     api_config: Dict[str, str], context=None) -> List[str]:
        """
        ä½¿ç”¨LLMæ‰¾åˆ°æ–‡æ¡£ä¸­æåŠè¯¥é”šç‚¹å®ä½“çš„æ–‡æœ¬å—(chunk)
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç”¨äºå•ä¸ªå®ä½“å¤„ç†ã€‚å¯¹äºæ‰¹é‡å¤„ç†ï¼Œè¯·ä½¿ç”¨ _find_entity_mentions_via_llm_batch
        
        Args:
            text_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            anchor_entity: é”šç‚¹å®ä½“ï¼Œä¾‹å¦‚ {'StudentID': 's1', 'StudentName': 'Alice'}
            anchor_table_name: é”šç‚¹è¡¨å
            model: ä½¿ç”¨çš„æ¨¡å‹
            api_config: APIé…ç½®
            context: å¤„ç†ä¸Šä¸‹æ–‡ï¼Œç”¨äºè·å–output_dir
            
        Returns:
            åŒ…å«è¯¥å®ä½“çš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if context:
            output_dir = None
            if hasattr(context, 'io_manager') and hasattr(context.io_manager, 'output_dir'):
                output_dir = context.io_manager.output_dir
            elif hasattr(context, 'output_dir'):
                output_dir = context.output_dir
            
            if output_dir:
                self._setup_entity_mention_logging(output_dir)
        
        entity_desc = ", ".join([f"{k}={v}" for k, v in anchor_entity.items()])
        
        self.logger.info(f"ğŸ” å¼€å§‹æŸ¥æ‰¾å®ä½“æåŠ - è¡¨: {anchor_table_name}, å®ä½“: {entity_desc}")
        
        system_prompt = """You are a document analysis expert. Your task is to find all relevant text chunks 
that mention a specific entity and provide sufficient context for understanding relationships."""
        
        prompt = f"""Find all text chunks in the document that mention the following entity from the {anchor_table_name} table:

Entity: {entity_desc}

Document:
{text_content}

Instructions:
1. Identify the entity by any of its attributes (ID, name, etc.)
2. Extract complete text chunks (not just single sentences) that mention this entity
3. A chunk should be 2-5 sentences long to provide sufficient context
4. Include surrounding context that helps understand relationships and attributes
5. If a mention is very brief, include the surrounding sentences for context
6. Return ONLY the text chunks, no explanations
7. If no mention found, return "NO_MENTION"

Output format:
<chunk>First text chunk mentioning the entity with context.</chunk>
<chunk>Second text chunk mentioning the entity with context.</chunk>
"""
        
        try:
            llm_response = get_answer(prompt, system_prompt=system_prompt, model=model)
            
            chunks = []
            if "NO_MENTION" in llm_response:
                return chunks
            
            import re
            chunk_matches = re.findall(r'<chunk>(.*?)</chunk>', llm_response, re.DOTALL)
            
            for chunk in chunk_matches:
                chunk = chunk.strip()
                if chunk and len(chunk) > 20:  # ç¡®ä¿chunkæœ‰å®è´¨å†…å®¹
                    if len(chunk) > 800:
                        chunk = chunk[:800] + "..."
                    chunks.append(chunk)
            
            if not chunks:
                potential_chunks = llm_response.strip().split('\n\n')
                for chunk in potential_chunks:
                    chunk = chunk.strip()
                    if chunk and not chunk.startswith('#') and len(chunk) > 30:
                        if len(chunk) > 800:
                            chunk = chunk[:800] + "..."
                        chunks.append(chunk)
            
            final_chunks = chunks[:15]  # é™åˆ¶æœ€å¤š15ä¸ªchunks
            if final_chunks:
                self.logger.info(f"âœ… æ‰¾åˆ° {len(final_chunks)} ä¸ªæ–‡æœ¬å—æåŠå®ä½“ {entity_desc}")
                for i, chunk in enumerate(final_chunks, 1):
                    chunk_preview = chunk[:200] + "..." if len(chunk) > 200 else chunk
                    self.logger.debug(f"   Chunk {i}: {chunk_preview}")
            else:
                self.logger.info(f"âš ï¸ æœªæ‰¾åˆ°æåŠå®ä½“ {entity_desc} çš„æ–‡æœ¬å—")
            
            return final_chunks
            
        except Exception as e:
            self.logger.error(f"âŒ LLMæŸ¥æ‰¾å®ä½“æåŠå¤±è´¥ - è¡¨: {anchor_table_name}, å®ä½“: {entity_desc}, é”™è¯¯: {e}")
            return []
    
    def _extract_relations_for_anchors_batch(self, 
                                            anchor_entities_with_context: Dict[str, List[str]],
                                            table_def: Dict[str, Any],
                                            reference_tables_data: Dict[str, List[Dict]],
                                            relation_config: Dict[str, Any],
                                            model: str,
                                            api_config: Dict[str, str],
                                            doc_source: str,
                                            create_list_model_func=None,
                                            build_schema_context_func=None,
                                            parse_structured_response_func=None,
                                            full_schema: Dict[str, Any] = None,
                                            batch_size: int = 10) -> List[TableRow]:
        """
        æ‰¹é‡ä¸ºå¤šä¸ªé”šç‚¹å®ä½“æå–å…³ç³»
        
        ç›¸æ¯”å•ä¸ªå¤„ç†çš„ä¼˜åŠ¿ï¼š
        1. å‡å°‘LLMè°ƒç”¨æ¬¡æ•°ï¼ˆNæ¬¡ -> N/batch_sizeæ¬¡ï¼‰
        2. å…±äº«schemaå’Œreference tablesçš„ä¸Šä¸‹æ–‡
        3. æ˜¾è‘—é™ä½æ€»å»¶è¿Ÿ
        
        Args:
            anchor_entities_with_context: å­—å…¸ï¼Œkeyä¸ºå®ä½“æè¿°ï¼Œvalueä¸ºç›¸å…³æ–‡æœ¬å—åˆ—è¡¨
                ä¾‹å¦‚: {"StudentID=s1, StudentName=Alice": ["chunk1", "chunk2"], ...}
            table_def: å…³ç³»è¡¨å®šä¹‰
            reference_tables_data: æ‰€æœ‰å‚è€ƒè¡¨æ•°æ®
            relation_config: å…³ç³»æå–é…ç½®
            model: æ¨¡å‹åç§°
            api_config: APIé…ç½®
            doc_source: æ–‡æ¡£æ¥æº
            batch_size: æ¯æ‰¹å¤„ç†çš„å®ä½“æ•°é‡
            
        Returns:
            æ‰€æœ‰å®ä½“çš„å…³ç³»è¡Œåˆ—è¡¨ï¼ˆå·²åˆå¹¶ï¼‰
        """
        self.logger.info(f"ğŸš€ [æ‰¹å¤„ç†] å¼€å§‹æ‰¹é‡æå– {len(anchor_entities_with_context)} ä¸ªå®ä½“çš„å…³ç³»")
        
        temp_schema = {'tables': [table_def]}
        table_name = table_def.get('name', 'data_table')
        
        list_model = create_list_model_func(temp_schema, table_name)
        if not list_model:
            return []
        
        api_base = api_config['api_base']
        api_key = api_config['api_key']
        
        llm_kwargs = {"model": model, "temperature": 0}
        if api_base:
            llm_kwargs["base_url"] = api_base
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        from langchain_openai import ChatOpenAI
        from langchain_core.output_parsers import PydanticOutputParser
        from langchain.output_parsers import OutputFixingParser
        
        llm = ChatOpenAI(**llm_kwargs)
        parser = PydanticOutputParser(pydantic_object=list_model)
        fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)
        
        schema_context = build_schema_context_func(table_def, full_schema=full_schema)
        reference_tables_prompt = self._build_reference_tables_prompt(
            reference_tables_data, relation_config
        )
        
        all_rows = []
        
        entities_with_text = {k: v for k, v in anchor_entities_with_context.items() if v}
        
        entity_items = list(entities_with_text.items())
        for batch_idx in range(0, len(entity_items), batch_size):
            batch = entity_items[batch_idx:batch_idx + batch_size]
            self.logger.info(f"  æ‰¹æ¬¡ {batch_idx//batch_size + 1}: æå– {len(batch)} ä¸ªå®ä½“çš„å…³ç³»")
            
            entities_context_parts = []
            for i, (entity_desc, relevant_texts) in enumerate(batch, 1):
                context_text = "\n".join(relevant_texts)
                entities_context_parts.append(
                    f"=== Entity {i}: {entity_desc} ===\n{context_text}\n"
                )
            
            combined_context = "\n".join(entities_context_parts)
            
            prompt = f"""Extract relationship records for ALL the following anchor entities:

{schema_context}

{reference_tables_prompt}

Relevant Text for Multiple Entities:
{combined_context}

{parser.get_format_instructions()}

IMPORTANT:
- Extract relationships for ALL {len(batch)} entities listed above
- For each entity, extract ONLY relationships involving that specific entity
- For foreign key fields, use exact values from the reference tables
- Return a JSON object with a "rows" field containing ALL extracted records
- Each record should be properly linked to its anchor entity

TIPS: Process each entity section separately and combine all results.
"""
            
            try:
                response = llm.invoke(prompt)
                llm_response = response.content
                
                self.logger.debug(f"  [DEBUG] å…³ç³»æå–LLMå“åº”é•¿åº¦: {len(llm_response)} å­—ç¬¦")
                self.logger.debug(f"  [DEBUG] å…³ç³»æå–LLMå“åº”å‰300å­—ç¬¦:\n{llm_response[:300]}")
                
                batch_rows = parse_structured_response_func(
                    llm_response, fixing_parser, table_name, doc_source
                )
                
                all_rows.extend(batch_rows)
                self.logger.info(f"    æå–åˆ° {len(batch_rows)} æ¡å…³ç³»è®°å½•")
                
                if batch_rows:
                    self.logger.debug(f"    [æ‰¹æ¬¡ {batch_idx//batch_size + 1}] æå–çš„å…³ç³»è¯¦æƒ…:")
                    for idx, row in enumerate(batch_rows[:5], 1):  # é™åˆ¶æ˜¾ç¤ºå‰5æ¡
                        row_desc = ", ".join([f"{k}={v.value if hasattr(v, 'value') else v}" 
                                             for k, v in (row.cells.items() if hasattr(row, 'cells') else [])])
                        self.logger.debug(f"      å…³ç³» {idx}: {row_desc}")
                    if len(batch_rows) > 5:
                        self.logger.debug(f"      ... (è¿˜æœ‰ {len(batch_rows)-5} æ¡å…³ç³»)")
                
            except Exception as e:
                self.logger.error(f"âŒ æ‰¹é‡æå–å…³ç³»å¤±è´¥ï¼ˆæ‰¹æ¬¡ {batch_idx//batch_size + 1}ï¼‰: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
        
        self.logger.info(f"âœ… [æ‰¹å¤„ç†] å®Œæˆï¼šæ€»è®¡æå– {len(all_rows)} æ¡å…³ç³»è®°å½•")
        
        return all_rows
    
    def _extract_relations_for_anchor(self, anchor_entity: Dict[str, Any],
                                     relevant_sentences: List[str],
                                     table_def: Dict[str, Any],
                                     reference_tables_data: Dict[str, List[Dict]],
                                     relation_config: Dict[str, Any],
                                     model: str,
                                     api_config: Dict[str, str],
                                     doc_source: str,
                                     create_list_model_func=None,
                                     build_schema_context_func=None,
                                     parse_structured_response_func=None,
                                     full_schema: Dict[str, Any] = None) -> List[TableRow]:
        """
        ä¸ºå•ä¸ªé”šç‚¹å®ä½“åœ¨ç›¸å…³å¥å­ä¸­æå–å…³ç³»
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç”¨äºå•ä¸ªå®ä½“å¤„ç†ã€‚å¯¹äºæ‰¹é‡å¤„ç†ï¼Œè¯·ä½¿ç”¨ _extract_relations_for_anchors_batch
        
        Args:
            anchor_entity: é”šç‚¹å®ä½“
            relevant_sentences: åŒ…å«è¯¥å®ä½“çš„å¥å­åˆ—è¡¨
            table_def: å…³ç³»è¡¨å®šä¹‰
            reference_tables_data: æ‰€æœ‰å‚è€ƒè¡¨æ•°æ®
            relation_config: å…³ç³»æå–é…ç½®
            model: æ¨¡å‹åç§°
            api_config: APIé…ç½®
            doc_source: æ–‡æ¡£æ¥æº
            
        Returns:
            æå–çš„å…³ç³»è¡Œåˆ—è¡¨
        """
        temp_schema = {'tables': [table_def]}
        table_name = table_def.get('name', 'data_table')
        
        list_model = create_list_model_func(temp_schema, table_name)
        if not list_model:
            return []
        
        api_base = api_config['api_base']
        api_key = api_config['api_key']
        
        llm_kwargs = {"model": model, "temperature": 0}
        if api_base:
            llm_kwargs["base_url"] = api_base
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        parser = PydanticOutputParser(pydantic_object=list_model)
        fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)
        
        schema_context = build_schema_context_func(table_def, full_schema=full_schema)
        
        reference_tables_prompt = self._build_reference_tables_prompt(
            reference_tables_data, relation_config
        )
        
        anchor_desc = ", ".join([f"{k}={v}" for k, v in anchor_entity.items()])
        
        context_text = "\n".join(relevant_sentences)
        
        prompt = f"""Extract relationship records involving the following anchor entity:

Anchor Entity: {anchor_desc}

{schema_context}

{reference_tables_prompt}

Relevant Text (sentences mentioning this entity):
{context_text}

{parser.get_format_instructions()}

IMPORTANT:
- Extract ONLY relationships involving the anchor entity specified above
- For foreign key fields, use exact values from the reference tables
- Return a JSON object with a "rows" field
- If no relationships found, return {{"rows": []}}

TIPS: Focus on relationships explicitly stated in the relevant text.
"""
        
        try:
            response = llm.invoke(prompt)
            llm_response = response.content
            
            rows = parse_structured_response_func(
                llm_response, fixing_parser, table_name, doc_source
            )
            
            return rows
            
        except Exception as e:
            self.logger.error(f"âŒ ä¸ºé”šç‚¹æå–å…³ç³»å¤±è´¥: {e}")
            return []
    
    def _deduplicate_relation_rows(self, rows: List[TableRow]) -> List[TableRow]:
        """
        å»é‡å…³ç³»è¡Œ
        
        åŸºäºtuple_idæˆ–cellå†…å®¹è¿›è¡Œå»é‡
        """
        if not rows:
            return []
        
        seen_tuples = set()
        unique_rows = []
        
        for row in rows:
            cell_values = tuple(sorted([
                (field_name, str(cell_data.value)) 
                for field_name, cell_data in row.cells.items()
            ]))
            
            if cell_values not in seen_tuples:
                seen_tuples.add(cell_values)
                unique_rows.append(row)
        
        return unique_rows
    
    def _identify_affected_anchors(self, violations: List, previous_snapshot,
                                   table_def: Dict[str, Any], context) -> List[Dict[str, Any]]:
        """
        ä»violationsä¸­è¯†åˆ«å—å½±å“çš„anchor entities
        
        è®ºæ–‡é€»è¾‘ï¼š"identifies the involved entities"
        
        Args:
            violations: è¿è§„åˆ—è¡¨
            previous_snapshot: ä¸Šä¸€æ¬¡å¿«ç…§
            table_def: è¡¨å®šä¹‰
            context: å¤„ç†ä¸Šä¸‹æ–‡
            
        Returns:
            å—å½±å“çš„anchor entityåˆ—è¡¨ï¼Œæ ¼å¼å¦‚ [{'StudentID': 's1', 'StudentName': 'Alice'}, ...]
        """
        if not violations or not previous_snapshot:
            return []
        
        attributes = table_def.get('attributes', table_def.get('fields', []))
        anchor_fk_field = None
        for attr in attributes:
            constraints = attr.get('constraints', {})
            if constraints.get('foreign_key'):
                anchor_fk_field = attr.get('name')
                break  # å–ç¬¬ä¸€ä¸ªå¤–é”®ä½œä¸ºanchor
        
        if not anchor_fk_field:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°anchorå¤–é”®å­—æ®µ")
            return []
        
        self.logger.info(f"ğŸ“Œ ä½¿ç”¨anchorå­—æ®µ: {anchor_fk_field}")
        
        affected_anchor_ids = set()
        for violation in violations:
            
            tuple_ids = []
            
            if hasattr(violation, 'affected_tuple_ids') and violation.affected_tuple_ids:
                tuple_ids = violation.affected_tuple_ids
                self.logger.debug(f"  æ£€æµ‹åˆ°èšåˆç±»å‹è¿è§„ï¼Œå—å½±å“çš„tupleæ•°: {len(tuple_ids)}")
            elif hasattr(violation, 'tuple_id') and violation.tuple_id:
                tuple_ids = [violation.tuple_id]
            elif isinstance(violation, dict):
                tuple_ids = violation.get('tuple_ids', [])
            
            for tuple_id in tuple_ids:
                for row in previous_snapshot.rows:
                    if row.tuple_id == tuple_id:
                        if anchor_fk_field in row.cells:
                            anchor_value = row.cells[anchor_fk_field].value
                            affected_anchor_ids.add(anchor_value)
                            self.logger.debug(f"  ä»violationè¯†åˆ«anchor ID: {anchor_value}")
        
        self.logger.info(f"ğŸ“‹ è¯†åˆ«åˆ° {len(affected_anchor_ids)} ä¸ªå—å½±å“çš„anchor IDs: {affected_anchor_ids}")
        
        relation_config = table_def.get('relation_extraction', {})
        reference_tables_data = self._load_reference_tables(relation_config, context)
        
        if not reference_tables_data:
            reference_tables_data = self._get_entity_snapshots_as_reference(context, table_def)
        
        if not reference_tables_data:
            self.logger.warning("âš ï¸ æ— æ³•è·å–å‚è€ƒè¡¨æ•°æ®")
            return []
        
        anchor_table_name = list(reference_tables_data.keys())[0]
        anchor_entities = reference_tables_data[anchor_table_name]
        
        self.logger.info(f"ğŸ“‹ ä»å‚è€ƒè¡¨ {anchor_table_name} è·å– {len(anchor_entities)} ä¸ªentities")
        
        affected_entities = []
        for entity in anchor_entities:
            entity_id = (entity.get(anchor_fk_field) or 
                        entity.get('ID') or 
                        entity.get('id') or
                        entity.get(f'{anchor_table_name}ID'))
            
            if entity_id in affected_anchor_ids:
                affected_entities.append(entity)
                self.logger.debug(f"  åŒ¹é…åˆ°å—å½±å“entity: {entity}")
        
        self.logger.info(f"âœ… æœ€ç»ˆè¯†åˆ« {len(affected_entities)} ä¸ªå—å½±å“çš„anchor entities")
        
        return affected_entities
    
    def _merge_warm_start_results(self, old_rows: List[TableRow], 
                                   new_rows: List[TableRow],
                                   affected_anchors: List[Dict[str, Any]],
                                   table_def: Dict[str, Any]) -> List[TableRow]:
        """
        åˆå¹¶warm startç»“æœï¼šç§»é™¤æ¶‰åŠaffected_anchorsçš„æ—§è®°å½•ï¼Œæ·»åŠ æ–°è®°å½•ï¼Œä¿ç•™æ— å…³è®°å½•
        
        è®ºæ–‡é€»è¾‘ï¼š"Relationship tuples unrelated to the violation are left unchanged"
        
        Args:
            old_rows: æ—§çš„å…³ç³»è¡Œ
            new_rows: æ–°æå–çš„å…³ç³»è¡Œ
            affected_anchors: å—å½±å“çš„anchor entities
            table_def: è¡¨å®šä¹‰
            
        Returns:
            åˆå¹¶åçš„è¡Œåˆ—è¡¨
        """
        attributes = table_def.get('attributes', table_def.get('fields', []))
        anchor_fk_field = None
        for attr in attributes:
            constraints = attr.get('constraints', {})
            if constraints.get('foreign_key'):
                anchor_fk_field = attr.get('name')
                break
        
        if not anchor_fk_field:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°anchorå­—æ®µï¼Œæ— æ³•ç²¾ç¡®åˆå¹¶ï¼Œè¿”å›å»é‡åçš„æ–°è®°å½•")
            return self._deduplicate_relation_rows(new_rows)
        
        affected_ids = set()
        for anchor in affected_anchors:
            anchor_id = (anchor.get(anchor_fk_field) or 
                        anchor.get('ID') or 
                        anchor.get('id'))
            if anchor_id:
                affected_ids.add(anchor_id)
        
        self.logger.info(f"ğŸ”„ å¼€å§‹åˆå¹¶ç»“æœ - å—å½±å“çš„anchor IDs: {affected_ids}")
        
        unaffected_rows = []
        removed_count = 0
        for row in old_rows:
            if anchor_fk_field in row.cells:
                row_anchor_id = row.cells[anchor_fk_field].value
                if row_anchor_id not in affected_ids:
                    unaffected_rows.append(row)
                else:
                    removed_count += 1
        
        self.logger.info(f"  ä¿ç•™ {len(unaffected_rows)} æ¡æ— å…³è®°å½•ï¼Œç§»é™¤ {removed_count} æ¡å—å½±å“è®°å½•")
        
        merged_rows = unaffected_rows + new_rows
        
        self.logger.info(f"  æ·»åŠ  {len(new_rows)} æ¡æ–°æå–è®°å½•")
        
        unique_rows = self._deduplicate_relation_rows(merged_rows)
        
        self.logger.info(f"âœ… åˆå¹¶å®Œæˆ: {len(old_rows)} æ¡åŸå§‹ -> {len(unique_rows)} æ¡æœ€ç»ˆï¼ˆå»é‡åï¼‰")
        
        return unique_rows

