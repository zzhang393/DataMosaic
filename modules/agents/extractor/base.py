import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Type
from pathlib import Path
from abc import ABC, abstractmethod
from datetime import datetime

from ...memory import TableRow, CellData, Fix
from ...core.ids import IdGenerator

from llm.main import get_answer

from .document_locator import DocumentLocator

from .entity_extractor import EntityExtractor
from .relation_extractor import RelationExtractor

from .utils import ExtractorUtils

try:
    from pydantic import BaseModel, Field, create_model
    from langchain_core.output_parsers import PydanticOutputParser
    from langchain_openai import ChatOpenAI
    from langchain.output_parsers import OutputFixingParser
    STRUCTURED_OUTPUT_AVAILABLE = True
except ImportError:
    STRUCTURED_OUTPUT_AVAILABLE = False
    BaseModel = None
    Field = None
    create_model = None


class BaseExtractor:
    
    def __init__(self, extractor_id: str = "BaseExtractor.v1",
                 enable_locate: bool = False,
                 locate_threshold: int = 50000):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            extractor_id: æå–å™¨æ ‡è¯†ç¬¦
            enable_locate: æ˜¯å¦å¯ç”¨æ–‡æ¡£å®šä½åŠŸèƒ½ï¼ˆé»˜è®¤å…³é—­ï¼‰
            locate_threshold: æ–‡æ¡£å®šä½çš„è¯æ•°é˜ˆå€¼ï¼ˆé»˜è®¤50000è¯ï¼‰
        """
        self.extractor_id = extractor_id
        self.logger = logging.getLogger('doc2db.extractor')
        self.enable_locate = enable_locate
        self.locator = DocumentLocator(
            word_threshold=locate_threshold
        ) if enable_locate else None
        
        self.entity_extractor = EntityExtractor(logger=self.logger)
        self.relation_extractor = RelationExtractor(logger=self.logger)
        
        self.utils = ExtractorUtils(logger=self.logger)
    
    def _get_model_api_config(self, model: str) -> Dict[str, Optional[str]]:
        """æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„APIé…ç½®
        
        Args:
            model: æ¨¡å‹åç§°
            
        Returns:
            åŒ…å« api_base å’Œ api_key çš„å­—å…¸
        """
        model_lower = model.lower()
        
        if 'deepseek' in model_lower:
            return {
                'api_base': os.getenv("DEEPSEEK_URL"),
                'api_key': os.getenv("DEEPSEEK_KEY")
            }
        
        elif 'qwen' in model_lower:
            return {
                'api_base': os.getenv("QWEN_URL"),
                'api_key': os.getenv("QWEN_KEY")
            }
        
        else:
            return {
                'api_base': os.getenv("API_URL") or os.getenv("OPENAI_API_BASE"),
                'api_key': os.getenv("API_KEY") or os.getenv("OPENAI_API_KEY")
            }
    
    def extract(self, text_contents: List[str], schema: Dict[str, Any], 
                table_name: str, nl_prompt: str = "", context=None, 
                warm_start: bool = False, previous_snapshot=None, 
                violations: List = None, full_schema: Dict[str, Any] = None) -> List[TableRow]:
        """ä»æ–‡æœ¬å†…å®¹ä¸­æå–è¡¨æ ¼æ•°æ®
        
        Args:
            text_contents: æ–‡æœ¬å†…å®¹åˆ—è¡¨ï¼ˆå·²ç”±Orchestratoråˆ†å—å¤„ç†ï¼‰
            schema: æ•°æ®åº“æ¶æ„å®šä¹‰ï¼ˆé€šå¸¸æ˜¯è¡¨ç‰¹å®šçš„schemaï¼‰
            table_name: ç›®æ ‡è¡¨å
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            context: å¤„ç†ä¸Šä¸‹æ–‡
            warm_start: æ˜¯å¦ä¸ºæš–å¯åŠ¨æ¨¡å¼
            previous_snapshot: ä¹‹å‰çš„æ•°æ®å¿«ç…§
            violations: éœ€è¦é‡æ–°æå–çš„è¿è§„åˆ—è¡¨
            full_schema: å®Œæ•´çš„æ•°æ®åº“æ¶æ„å®šä¹‰ï¼ˆåŒ…å«æ‰€æœ‰è¡¨å’Œå…³ç³»å®šä¹‰ï¼‰
            
        Returns:
            æå–çš„è¡¨æ ¼è¡Œåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹æå–è¡¨æ ¼ {table_name} çš„æ•°æ®")
        
        if isinstance(schema, str):
            if not schema.strip():
                actual_schema = self.utils.generate_default_schema_from_prompt(table_name, nl_prompt)
            else:
                try:
                    actual_schema = json.loads(schema)
                except Exception as e:
                    raise ValueError(f"Schemaæ ¼å¼é”™è¯¯: {e}")
        else:
            actual_schema = schema
        
        table_def = self._get_table_definition(actual_schema, table_name)
        if not table_def:
            raise ValueError(f"æœªæ‰¾åˆ°è¡¨æ ¼å®šä¹‰: {table_name}")
        
        if self.enable_locate and self.locator and not warm_start:
            self.logger.info(f"å¯ç”¨æ–‡æ¡£å®šä½ï¼Œå¤„ç† {len(text_contents)} ä¸ªæ–‡æ¡£")
            
            document_names = []
            if context and hasattr(context, 'batch_document_names') and context.batch_document_names:
                document_names = context.batch_document_names
            elif context and hasattr(context, 'documents') and context.documents:
                document_names = [os.path.basename(doc_path) for doc_path in context.documents]
            else:
                document_names = [f"doc_{i+1}" for i in range(len(text_contents))]
            
            located_segments = self.locator.locate_from_multi_documents(
                text_contents=text_contents,
                document_names=document_names,
                schema=actual_schema,
                table_name=table_name,
                nl_prompt=nl_prompt,
                context=context
            )
            
            located_text = self.locator.merge_segments(located_segments)
            
            if context:
                context.located_segments = located_segments
                self.logger.info(f'å·²ä¿å­˜ {len(located_segments)} ä¸ªsegmentsåˆ°context')
            
            text_contents = [located_text]
            
            summary = self.locator.get_segments_summary(located_segments)
            self.logger.info(
                f'Locateå®Œæˆï¼š{len(located_segments)} ä¸ªç‰‡æ®µï¼Œ'
                f'æ€»é•¿åº¦ {len(located_text)}'
            )
        
        document_names = []
        if context and hasattr(context, 'documents'):
            document_names = [os.path.basename(doc_path) for doc_path in context.documents]
        
        if False and not warm_start:  # æš‚æ—¶ç¦ç”¨document_filteræœºåˆ¶
            text_contents, document_names = self._apply_document_filter(
                text_contents, document_names, table_def
            )
            
            if not text_contents:
                self.logger.warning(f"âš ï¸ æ–‡æ¡£è¿‡æ»¤åæ²¡æœ‰åŒ¹é…çš„æ–‡æ¡£ï¼Œè¿”å›ç©ºç»“æœ")
                return []
        
        all_rows = []
        
        if warm_start and violations and previous_snapshot and len(text_contents) > 1:
            self.logger.info(f"ğŸ”„ [Warm Startæ‰¹é‡] ä¸€æ¬¡æ€§å¤„ç† {len(text_contents)} ä¸ªæ–‡æ¡£çš„ä¿®å¤")
            
            try:
                all_rows = self._batch_warm_start_fix(
                    text_contents, table_def, nl_prompt, document_names,
                    context, previous_snapshot, violations
                )
                self.logger.info(f"âœ… [Warm Startæ‰¹é‡] å®Œæˆï¼Œå…±ä¿®å¤ {len(all_rows)} è¡Œæ•°æ®")
                return all_rows
            except Exception as e:
                self.logger.error(f"âŒ [Warm Startæ‰¹é‡] å¤±è´¥: {e}ï¼Œé™çº§åˆ°é€ä¸ªå¤„ç†")
        
        current_snapshot = previous_snapshot
        
        for i, text_content in enumerate(text_contents):
            doc_name = document_names[i] if i < len(document_names) else f"æ–‡æ¡£{i+1}"
            
            if not text_content or not text_content.strip():
                self.logger.warning(f"æ–‡æœ¬å†…å®¹ {i} ({doc_name}) ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            try:
                if warm_start and violations and current_snapshot:
                    doc_rows = self._extract_with_warm_start(
                        text_content, table_def, nl_prompt, doc_name, context, i,
                        current_snapshot, violations
                    )
                else:
                    doc_rows = self._extract_with_cold_start(
                        text_content, table_def, nl_prompt, doc_name, context, i, full_schema
                    )
                
                all_rows.extend(doc_rows)
                
                if warm_start and doc_rows:
                    from ...memory.snapshot import TableSnapshot
                    from ...core.io import IOManager
                    current_snapshot = TableSnapshot(
                        run_id=f"temp_fix_{i}",
                        table=table_name,
                        rows=all_rows,
                        created_at=IOManager.get_timestamp(),
                        table_id=previous_snapshot.table_id if hasattr(previous_snapshot, 'table_id') else table_name
                    )
                
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†æ–‡æœ¬å†…å®¹ {i} ({doc_name}) å¤±è´¥: {e}")
                continue
        
        all_rows = self.utils.deduplicate_rows(all_rows)
        
        self.logger.info(f"âœ… å…±æå– {len(all_rows)} è¡Œæ•°æ®")
        return all_rows
    
    def _extract_with_cold_start(self, text_content: str, table_def: Dict[str, Any],
                               nl_prompt: str, doc_source: str, context=None, 
                               doc_index: int = 0, full_schema: Dict[str, Any] = None) -> List[TableRow]:
        """
        Cold Startæå–å…¥å£ - æ ¹æ®è¡¨ç±»å‹åˆ†æ´¾åˆ°å®ä½“æå–æˆ–å…³ç³»æå–
        
        Args:
            text_content: æ–‡æœ¬å†…å®¹
            table_def: è¡¨å®šä¹‰
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            doc_source: æ–‡æ¡£æ¥æº
            context: å¤„ç†ä¸Šä¸‹æ–‡
            doc_index: æ–‡æ¡£ç´¢å¼•
            full_schema: å®Œæ•´schema
            
        Returns:
            æå–çš„è¡¨æ ¼è¡Œåˆ—è¡¨
        """
        table_type = table_def.get('type', 'entity').lower()
        table_name = table_def.get('name', 'data_table')
        
        relation_extraction_config = table_def.get('relation_extraction', {})
        is_relation_table = (table_type in ['relation', 'relationship'] or 
                            relation_extraction_config.get('enabled', False))
        
        if is_relation_table:
            self.logger.info(f"ğŸ”— [Cold Start] å…³ç³»è¡¨æå–: {table_name}")
            return self.relation_extractor.extract_cold_start(
                text_content, table_def, nl_prompt, doc_source, 
                context, doc_index, full_schema,
                get_model_api_config_func=self._get_model_api_config,
                create_list_model_func=self._create_list_model_from_schema,
                build_schema_context_func=self._build_schema_context,
                parse_structured_response_func=self._parse_structured_response,
                record_extraction_step_func=self._record_extraction_step,
                fallback_func=self._extract_with_fallback
            )
        else:
            self.logger.info(f"ğŸ“¦ [Cold Start] å®ä½“è¡¨æå–: {table_name}")
            return self.entity_extractor.extract_cold_start(
                text_content, table_def, nl_prompt, doc_source, 
                context, doc_index, full_schema,
                get_model_api_config_func=self._get_model_api_config,
                create_list_model_func=self._create_list_model_from_schema,
                build_schema_context_func=self._build_schema_context,
                parse_structured_response_func=self._parse_structured_response,
                record_extraction_step_func=self._record_extraction_step,
                fallback_func=self._extract_with_fallback
            )
    
    def _extract_with_warm_start(self, text_content: str, table_def: Dict[str, Any],
                                nl_prompt: str, doc_source: str, context=None, 
                                doc_index: int = 0, previous_snapshot=None, 
                                violations: List = None) -> List[TableRow]:
        """
        Warm Startæå–å…¥å£ - æ ¹æ®è¡¨ç±»å‹åˆ†æ´¾åˆ°å®ä½“æˆ–å…³ç³»çš„warm startæå–
        
        Args:
            text_content: æ–‡æœ¬å†…å®¹
            table_def: è¡¨å®šä¹‰
            nl_prompt: è‡ªç„¶è¯­è¨€æç¤º
            doc_source: æ–‡æ¡£æ¥æº
            context: å¤„ç†ä¸Šä¸‹æ–‡
            doc_index: æ–‡æ¡£ç´¢å¼•
            previous_snapshot: ä¹‹å‰çš„æ•°æ®å¿«ç…§
            violations: éœ€è¦ä¿®å¤çš„è¿è§„åˆ—è¡¨
            
        Returns:
            ä¿®å¤åçš„è¡¨æ ¼è¡Œåˆ—è¡¨
        """
        table_type = table_def.get('type', 'entity').lower()
        table_name = table_def.get('name') or table_def.get('table_name', 'unknown')
        
        relation_extraction_config = table_def.get('relation_extraction', {})
        is_relation_table = (table_type in ['relation', 'relationship'] or 
                            relation_extraction_config.get('enabled', False))
        
        if is_relation_table:
            self.logger.info(f"ğŸ”— [Warm Start] å…³ç³»è¡¨ä¿®å¤: {table_name}")
            return self.relation_extractor.extract_warm_start(
                text_content, table_def, nl_prompt, doc_source, 
                context, doc_index, previous_snapshot, violations,
                check_warm_start_limit_func=self._check_warm_start_limit,
                identify_cells_from_violations_func=self._identify_cells_from_violations,
                build_cell_fix_prompt_func=self._build_cell_fix_prompt,
                parse_cell_fix_json_func=self.utils.parse_cell_fix_json,
                apply_cell_fixes_func=self._apply_cell_fixes
            )
        else:
            self.logger.info(f"ğŸ“¦ [Warm Start] å®ä½“è¡¨ä¿®å¤: {table_name}")
            return self.entity_extractor.extract_warm_start(
                text_content, table_def, nl_prompt, doc_source, 
                context, doc_index, previous_snapshot, violations,
                check_warm_start_limit_func=self._check_warm_start_limit,
                identify_cells_from_violations_func=self._identify_cells_from_violations,
                build_cell_fix_prompt_func=self._build_cell_fix_prompt,
                parse_cell_fix_json_func=self.utils.parse_cell_fix_json,
                apply_cell_fixes_func=self._apply_cell_fixes
            )
    
    def _extract_with_fallback(self, text_content: str, table_def: Dict[str, Any],
                              nl_prompt: str, doc_source: str, context=None,
                              doc_index: int = 0) -> List[TableRow]:
        """é™çº§æå–ç­–ç•¥ - ä½¿ç”¨ç®€å•çš„Markdownè¡¨æ ¼æ ¼å¼"""
        self.logger.info(f"[é™çº§æå–] ä½¿ç”¨Markdownè¡¨æ ¼æ ¼å¼")
        
        rows = []
        attributes = table_def.get('attributes', [])
        
        schema_info = self.utils.build_simple_schema_prompt(table_def)
        
        system_prompt = """You are a professional data extraction expert. 
Your role is to read documents carefully and extract structured data with high accuracy and recall."""
        
        prompt = f"""Extract structured data from the following document according to the schema.

{schema_info}

Return the data in a Markdown table format:

<TABLE BEGIN>
| {attributes[0]['name'] if attributes else 'field1'} | {attributes[1]['name'] if len(attributes) > 1 else 'field2'} | ... |
|---|---|---|
| value1 | value2 | ... |
<TABLE END>

Document Content:
{text_content}

Additional Instructions: {nl_prompt if nl_prompt else 'No special requirements'}
"""
        
        try:
            model = context.model if context and hasattr(context, 'model') else "gpt-4o"
            llm_response = get_answer(prompt, system_prompt=system_prompt, model=model)
            
            extracted_data = self.utils.parse_markdown_table_response(llm_response, table_def, doc_source)
            rows.extend(extracted_data)
            
            self.logger.info(f"âœ… [é™çº§æå–] è·å¾— {len(extracted_data)} è¡Œæ•°æ®")
            
        except Exception as e:
            self.logger.error(f"âŒ [é™çº§æå–] å¤±è´¥: {e}")
        
        return rows
    
    def _parse_structured_response(self, llm_response: str, fixing_parser, 
                                   table_name: str, doc_source: str) -> List[TableRow]:
        """è§£æLLMçš„ç»“æ„åŒ–å“åº”å¹¶è½¬æ¢ä¸ºTableRowæ ¼å¼"""
        try:
            result = fixing_parser.parse(llm_response)
            
            rows_data = []
            if hasattr(result, 'rows'):
                for row in result.rows:
                    if hasattr(row, 'model_dump'):
                        row_dict = row.model_dump()
                    elif hasattr(row, 'dict'):
                        row_dict = row.dict()
                    else:
                        row_dict = dict(row)
                    rows_data.append(row_dict)
            
            self.logger.info(f"âœ… æˆåŠŸè§£æ {len(rows_data)} è¡Œæ•°æ®")
            
            rows = []
            for i, row_dict in enumerate(rows_data):
                tuple_id = IdGenerator.generate_tuple_id(table_name, i, row_dict)
                
                cells = {}
                for field_name, value in row_dict.items():
                    if value is not None:
                        cells[field_name] = CellData(
                            value=value,
                            evidences=[doc_source]
                        )
                
                if cells:
                    table_row = TableRow(tuple_id=tuple_id, cells=cells)
                    rows.append(table_row)
            
            return rows
            
        except Exception as parse_error:
            self.logger.error(f"âŒ è§£æå¤±è´¥: {parse_error}")
            return []
    
    def _record_extraction_step(self, context, table_name: str, doc_index: int, 
                                doc_source: str, extraction_mode: str, model: str,
                                prompt: str, llm_response: str, row_count: int):
        """è®°å½•æå–æ­¥éª¤åˆ°context"""
        if context and hasattr(context, 'step_outputs'):
            from ...core.io import IOManager
            context.step_outputs.append({
                'step': f'extractor_{table_name}_{doc_index}',
                'status': 'completed',
                'description': f"ğŸš€ æ–‡æ¡£ {doc_index+1} ({os.path.basename(doc_source)}) [è¡¨: {table_name}]",
                'details': {
                    'document': os.path.basename(doc_source),
                    'table_name': table_name,
                    'extraction_mode': extraction_mode,
                    'extracted_rows': row_count,
                    'model_used': model,
                    'llm_input': prompt[:1000] + '...' if len(prompt) > 1000 else prompt,
                    'llm_output': llm_response[:1000] + '...' if len(llm_response) > 1000 else llm_response,
                },
                'timestamp': IOManager.get_timestamp()
            })
    
    def _check_warm_start_limit(self, context, table_name: str) -> bool:
        """æ£€æŸ¥warm starté™åˆ¶ï¼Œè¿”å›æ˜¯å¦å¯ä»¥ç»§ç»­"""
        if context and hasattr(context, 'coordinator'):
            coordinator = context.coordinator
            current_batch_index = getattr(context, 'current_batch_index', 1)
            
            if not coordinator.get_table_tracker(table_name):
                coordinator.init_table_tracker(table_name, total_batches=1)
            
            table_tracker = coordinator.get_table_tracker(table_name)
            if current_batch_index not in table_tracker.batch_trackers:
                coordinator.init_batch_tracker(table_name, current_batch_index, 1)
            
            if not coordinator.can_batch_warm_start(table_name, current_batch_index):
                self.logger.warning(f'è¡¨ {table_name} batch {current_batch_index} å·²è¾¾åˆ°æœ€å¤§warm startå°è¯•æ¬¡æ•°')
                return False
            
            coordinator.increment_batch_warm_start_count(table_name, current_batch_index)
        
        return True
    
    def _batch_warm_start_fix(self, text_contents: List[str], table_def: Dict[str, Any],
                             nl_prompt: str, document_names: List[str],
                             context=None, previous_snapshot=None, 
                             violations: List = None) -> List[TableRow]:
        """æ‰¹é‡Warm Startä¿®å¤ - ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ–‡æ¡£"""
        table_name = table_def.get('name') or table_def.get('table_name', 'unknown')
        
        if context and hasattr(context, 'coordinator'):
            coordinator = context.coordinator
            current_batch_index = getattr(context, 'current_batch_index', 1)
            
            if not coordinator.get_table_tracker(table_name):
                coordinator.init_table_tracker(table_name, total_batches=1)
            
            table_tracker = coordinator.get_table_tracker(table_name)
            if current_batch_index not in table_tracker.batch_trackers:
                coordinator.init_batch_tracker(table_name, current_batch_index, 1)
            
            if not coordinator.can_batch_warm_start(table_name, current_batch_index):
                self.logger.warning(f'è¡¨ {table_name} batch {current_batch_index} å·²è¾¾åˆ°æœ€å¤§warm startå°è¯•æ¬¡æ•°')
                return previous_snapshot.rows if previous_snapshot else []
            
            coordinator.increment_batch_warm_start_count(table_name, current_batch_index)
        
        cells_to_fix = self._identify_cells_from_violations(violations, previous_snapshot)
        
        if not cells_to_fix:
            return previous_snapshot.rows if previous_snapshot else []
        
        extraction_prompt = self._build_batch_cell_fix_prompt(
            cells_to_fix, table_def, previous_snapshot, nl_prompt, len(text_contents)
        )
        
        system_prompt = """You are a data extraction expert specializing in fixing data quality issues across multiple documents.
Your task is to analyze ALL documents and extract ONLY the specified field values that need correction.
Output in strict JSON format with the structure: {"fixes": [{"tuple_id": "...", "field": "...", "new_value": "..."}]}
"""
        
        try:
            model = context.model if context and hasattr(context, 'model') else "gpt-4o"
            
            documents_section = "\n\nDocuments Content:\n"
            for i, (text_content, doc_name) in enumerate(zip(text_contents, document_names)):
                if text_content and text_content.strip():
                    documents_section += f"\n=== Document {i+1}: {doc_name} ===\n{text_content}\n"
            
            full_prompt = f"""{extraction_prompt}
{documents_section}

Remember: 
- Analyze ALL {len(text_contents)} documents
- Output ONLY in JSON format with the structure shown above
- Extract accurate values for each field that needs correction
"""
            
            llm_response = get_answer(full_prompt, system_prompt=system_prompt, model=model)
            
            fixes_data = self.utils.parse_cell_fix_json(llm_response)
            
            if not fixes_data:
                return previous_snapshot.rows if previous_snapshot else []
            
            doc_sources = ", ".join(document_names)
            updated_rows, fix_records = self._apply_cell_fixes(
                previous_snapshot.rows if previous_snapshot else [],
                fixes_data,
                f"[Batch: {doc_sources}]",
                table_name
            )
            
            if context and hasattr(context, 'step_outputs'):
                from ...core.io import IOManager
                context.step_outputs.append({
                    'step': f'extractor_batch_warm_start_{table_name}',
                    'status': 'completed',
                    'description': f"ğŸš€ [æ‰¹é‡Warm Start] ä¸€æ¬¡æ€§å¤„ç† {len(text_contents)} ä¸ªæ–‡æ¡£ [è¡¨: {table_name}]",
                    'details': {
                        'documents_count': len(text_contents),
                        'documents': document_names,
                        'table_name': table_name,
                        'mode': 'batch_warm_start',
                        'cells_identified': len(cells_to_fix),
                        'cells_fixed': len(fixes_data),
                        'total_rows': len(updated_rows),
                        'model_used': model,
                    },
                    'timestamp': IOManager.get_timestamp()
                })
            
            return updated_rows
            
        except Exception as e:
            self.logger.error(f"âŒ [æ‰¹é‡Warm Start] å¤±è´¥: {e}")
            raise
    
    
    def _get_table_definition(self, schema: Dict[str, Any], table_name: str) -> Optional[Dict[str, Any]]:
        """è·å–è¡¨æ ¼å®šä¹‰"""
        tables = schema.get('tables', [])
        
        for table in tables:
            if table.get('name') == table_name:
                result = table.copy()
                if 'fields' in result and 'attributes' not in result:
                    result['attributes'] = result.pop('fields')
                return result
        
        if '_' in table_name and table_name.rsplit('_', 1)[-1].isdigit():
            base_name = table_name.rsplit('_', 1)[0]
            suffix_num = int(table_name.rsplit('_', 1)[1])
            
            matching_tables = [t for t in tables if isinstance(t, dict) and t.get('name') == base_name]
            
            if matching_tables and 1 <= suffix_num <= len(matching_tables):
                target_table = matching_tables[suffix_num - 1]
                result = target_table.copy()
                if 'fields' in result and 'attributes' not in result:
                    result['attributes'] = result.pop('fields')
                return result
        
        return None
    
    def _create_pydantic_model_from_schema(self, schema: Dict[str, Any], 
                                          table_name: str) -> Optional[Type]:
        """ä»schemaåŠ¨æ€åˆ›å»ºPydanticæ¨¡å‹"""
        if not STRUCTURED_OUTPUT_AVAILABLE:
            return None
        
        try:
            table_def = self._get_table_definition(schema, table_name)
            if not table_def:
                return None
            
            attributes = table_def.get('attributes', [])
            field_definitions = {}
            
            for attr in attributes:
                field_name = attr['name']
                field_type = attr.get('type', 'VARCHAR')
                field_desc = attr.get('description', '')
                
                if 'VARCHAR' in field_type or 'TEXT' in field_type or 'ENUM' in field_type or 'string' in field_type.lower():
                    python_type = str
                elif 'INT' in field_type or 'integer' in field_type.lower():
                    python_type = int
                elif 'DECIMAL' in field_type or 'FLOAT' in field_type or 'number' in field_type.lower():
                    python_type = float
                elif 'BOOLEAN' in field_type or 'boolean' in field_type.lower():
                    python_type = bool
                elif 'DATE' in field_type or 'date' in field_type.lower():
                    python_type = str
                else:
                    python_type = str
                
                if not attr.get('required', False):
                    python_type = Optional[python_type]
                
                field_definitions[field_name] = (python_type, Field(description=field_desc, default=None))
            
            model_class = create_model(table_name, **field_definitions)
            return model_class
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºPydanticæ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _create_list_model_from_schema(self, schema: Dict[str, Any], 
                                      table_name: str) -> Optional[Type]:
        """åˆ›å»ºåŒ…å«å¤šè¡Œæ•°æ®çš„åˆ—è¡¨æ¨¡å‹"""
        if not STRUCTURED_OUTPUT_AVAILABLE:
            return None
        
        try:
            row_model = self._create_pydantic_model_from_schema(schema, table_name)
            if not row_model:
                return None
            
            list_model = create_model(
                f"{table_name}List",
                rows=(List[row_model], Field(description=f"List of {table_name} records"))
            )
            
            return list_model
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºåˆ—è¡¨æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _build_schema_context(self, table_def: Dict[str, Any], nl_prompt: str = "", full_schema: Dict[str, Any] = None) -> str:
        """æ„å»ºschemaä¸Šä¸‹æ–‡"""
        table_name = table_def.get('name', 'data_table')
        attributes = table_def.get('attributes', [])
        
        context_parts = []
        
        context_parts.append(f"Target Table: {table_name}")
        
        table_type = table_def.get('type', '')
        if table_type:
            context_parts.append(f"Type: {table_type}")
        
        table_desc = table_def.get('description', '')
        if table_desc:
            context_parts.append(f"Description: {table_desc}")
        
        if attributes:
            context_parts.append(f"Fields ({len(attributes)}):")
            for attr in attributes:
                field_name = attr.get('name', 'unknown')
                field_type = attr.get('type', 'VARCHAR')
                field_desc = attr.get('description', '')
                constraints = attr.get('constraints', {})
                
                constraint_info = []
                
                if constraints.get('primary_key'):
                    constraint_info.append('PK')
                if constraints.get('foreign_key'):
                    constraint_info.append('FK')
                if constraints.get('unique'):
                    constraint_info.append('UNIQUE')
                if not constraints.get('nullable', True):
                    constraint_info.append('NOT NULL')
                
                if attr.get('required', False) and 'NOT NULL' not in constraint_info:
                    constraint_info.append('REQUIRED')
                if attr.get('unique', False) and 'UNIQUE' not in constraint_info:
                    constraint_info.append('UNIQUE')
                
                if 'domain' in attr:
                    domain_values = attr['domain']
                    if isinstance(domain_values, list):
                        constraint_info.append(f"domain={domain_values}")
                    else:
                        constraint_info.append(f"domain={domain_values}")
                
                if 'format' in attr:
                    constraint_info.append(f"format='{attr['format']}'")
                
                if 'min' in attr:
                    constraint_info.append(f"min={attr['min']}")
                if 'max' in attr:
                    constraint_info.append(f"max={attr['max']}")
                
                constraint_str = f" [{', '.join(constraint_info)}]" if constraint_info else ""
                context_parts.append(f"  - {field_name} ({field_type}){constraint_str}: {field_desc}")
        
        if table_type and table_type.lower() == 'relationship' and full_schema:
            relations_info = self._build_relations_context(table_name, full_schema)
            if relations_info:
                context_parts.append("\nTable Relationships:")
                context_parts.append(relations_info)
        
        return "\n".join(context_parts)
    
    def _build_relations_context(self, table_name: str, full_schema: Dict[str, Any]) -> str:
        """æ„å»ºè¡¨å…³ç³»ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå‚è€ƒrelation_extractor.pyå®ç°ï¼‰
        
        ä»schemaçš„relationséƒ¨åˆ†æå–ä¸å½“å‰è¡¨ç›¸å…³çš„å…³ç³»ä¿¡æ¯ï¼Œ
        å¸®åŠ©LLMç†è§£è¯¥å…³ç³»è¡¨è¿æ¥äº†å“ªäº›å®ä½“è¡¨ã€‚
        """
        if not full_schema:
            return ""
        
        relations = full_schema.get('relations', [])
        if not relations:
            return ""
        
        relevant_relations = []
        for relation in relations:
            if not isinstance(relation, dict):
                continue
            
            from_info = relation.get('from', {})
            to_info = relation.get('to', {})
            
            if (from_info.get('table') == table_name or 
                to_info.get('table') == table_name):
                relevant_relations.append(relation)
        
        if not relevant_relations:
            return ""
        
        relation_lines = []
        for relation in relevant_relations:
            from_info = relation.get('from', {})
            to_info = relation.get('to', {})
            relation_type = relation.get('type', 'unknown')
            relation_id = relation.get('id', '')
            
            from_table = from_info.get('table', '')
            from_field = from_info.get('field', '')
            to_table = to_info.get('table', '')
            to_field = to_info.get('field', '')
            
            if from_table and from_field and to_table and to_field:
                relation_desc = f"  - {from_table}.{from_field} â†’ {to_table}.{to_field} ({relation_type})"
                if relation_id:
                    relation_desc += f" [ID: {relation_id}]"
                relation_lines.append(relation_desc)
        
        return "\n".join(relation_lines)
    
    
    
    def _identify_cells_from_violations(self, violations: List, 
                                       previous_snapshot) -> List[Dict[str, Any]]:
        """ä»violationsä¸­è¯†åˆ«éœ€è¦ä¿®å¤çš„cells"""
        cells_to_fix = []
        
        if not violations or not previous_snapshot:
            return cells_to_fix
        
        rows_map = {row.tuple_id: row for row in previous_snapshot.rows}
        
        for violation in violations:
            tuple_id = getattr(violation, 'tuple_id', '')
            field = getattr(violation, 'attr', '')
            constraint_type = getattr(violation, 'constraint_type', '').upper()
            description = getattr(violation, 'description', '')
            
            current_value = None
            if tuple_id in rows_map and field in rows_map[tuple_id].cells:
                current_value = rows_map[tuple_id].cells[field].value
            
            cells_to_fix.append({
                'tuple_id': tuple_id,
                'field': field,
                'current_value': current_value,
                'violation_id': getattr(violation, 'id', ''),
                'violation_desc': description,
                'constraint_type': constraint_type
            })
        
        return cells_to_fix
    
    def _build_cell_fix_prompt(self, cells_to_fix: List[Dict[str, Any]], 
                              table_def: Dict[str, Any],
                              previous_snapshot, nl_prompt: str) -> str:
        """æ„å»ºcellçº§ä¿®å¤çš„prompt"""
        table_name = table_def.get('name', 'unknown_table')
        
        rows_map = {}
        if previous_snapshot and previous_snapshot.rows:
            rows_map = {row.tuple_id: row for row in previous_snapshot.rows}
        
        cell_list = []
        for i, cell in enumerate(cells_to_fix[:20]):
            tuple_id = cell['tuple_id']
            field = cell['field']
            current_value = cell['current_value']
            issue = cell['violation_desc'][:100]
            
            row_context = ""
            if tuple_id in rows_map:
                row = rows_map[tuple_id]
                context_values = []
                for field_name, cell_data in row.cells.items():
                    if field_name != field:
                        context_values.append(f"{field_name}='{cell_data.value}'")
                if context_values:
                    row_context = f" | Row Context: {', '.join(context_values[:5])}"
            
            cell_list.append(
                f"  {i+1}. tuple_id: {tuple_id}, field: {field}, "
                f"current_value: {current_value}{row_context}\n"
                f"      Issue: {issue}"
            )
        
        if len(cells_to_fix) > 20:
            cell_list.append(f"  ... {len(cells_to_fix) - 20} more cells need to be fixed")
        
        attributes = table_def.get('attributes', [])
        field_definitions = []
        for attr in attributes:
            field_definitions.append(
                f"  - {attr['name']} ({attr.get('type', 'TEXT')}): {attr.get('description', 'N/A')}"
            )
        
        prompt = f"""ğŸ¯ Task: Fix problematic cell values in table [{table_name}]

ã€Field Definitionsã€‘
{chr(10).join(field_definitions)}

ã€Cells to Fixã€‘Total: {len(cells_to_fix)} cells
{chr(10).join(cell_list)}

ã€Task Requirementsã€‘
1. Carefully read the document content to find clues related to the cells above
2. Locate cells to fix based on tuple_id + field
3. Use "row context" information to accurately match the corresponding data
4. Extract accurate corrected values from the document
5. If the information is not found in the document, output null

Output strictly in the following JSON format:
{{
  "fixes": [
    {{
      "tuple_id": "Original tuple_id from the data",
      "field": "Field name",
      "new_value": "Corrected value extracted from document (or null)"
    }}
  ]
}}

ã€Important Notesã€‘
- Output ONLY JSON, do not add any other text
- tuple_id must exactly match those listed above
- field must exist in the field definitions
- Prevent mismatches: Use "row context" to accurately locate the data
"""
        
        if nl_prompt and nl_prompt.strip():
            prompt += f"\n\nã€Additional User Requirementsã€‘\n{nl_prompt}\n"
        
        return prompt
    
    def _build_batch_cell_fix_prompt(self, cells_to_fix: List[Dict[str, Any]], 
                                    table_def: Dict[str, Any],
                                    previous_snapshot, nl_prompt: str,
                                    document_count: int) -> str:
        """æ„å»ºæ‰¹é‡cellä¿®å¤çš„prompt"""
        table_name = table_def.get('name', 'unknown_table')
        
        rows_map = {}
        if previous_snapshot and previous_snapshot.rows:
            rows_map = {row.tuple_id: row for row in previous_snapshot.rows}
        
        cell_list = []
        for i, cell in enumerate(cells_to_fix[:50]):
            tuple_id = cell['tuple_id']
            field = cell['field']
            current_value = cell['current_value']
            issue = cell['violation_desc'][:100]
            
            row_context = ""
            if tuple_id in rows_map:
                row = rows_map[tuple_id]
                context_values = []
                for field_name, cell_data in row.cells.items():
                    if field_name != field:
                        context_values.append(f"{field_name}='{cell_data.value}'")
                if context_values:
                    row_context = f" | Row Context: {', '.join(context_values[:5])}"
            
            cell_list.append(
                f"  {i+1}. tuple_id: {tuple_id}, field: {field}, "
                f"current_value: {current_value}{row_context}\n"
                f"      Issue: {issue}"
            )
        
        if len(cells_to_fix) > 50:
            cell_list.append(f"  ... {len(cells_to_fix) - 50} more cells need to be fixed")
        
        attributes = table_def.get('attributes', [])
        field_definitions = []
        for attr in attributes:
            field_definitions.append(
                f"  - {attr['name']} ({attr.get('type', 'TEXT')}): {attr.get('description', 'N/A')}"
            )
        
        prompt = f"""ğŸ¯ Task: Batch fix problematic cell values in table [{table_name}]

ğŸ“Š Document Count: {document_count} documents will be analyzed together

ã€Field Definitionsã€‘
{chr(10).join(field_definitions)}

ã€Cells to Fixã€‘Total: {len(cells_to_fix)} cells
{chr(10).join(cell_list)}

ã€Task Requirementsã€‘
1. Carefully read the content of **ALL {document_count} documents**
2. Accurately locate cells to fix based on tuple_id + field
3. Use "row context" information to extract corresponding data from the correct document
4. If the information is not found in any document, output null

Output strictly in the following JSON format:
{{
  "fixes": [
    {{
      "tuple_id": "Original tuple_id from the data",
      "field": "Field name",
      "new_value": "Corrected value extracted from document (or null)"
    }}
  ]
}}

ã€Important Notesã€‘
- Output ONLY JSON
- tuple_id must exactly match
- field must exist in the field definitions
- Prevent mismatches: Must use "row context" to accurately locate the data
"""
        
        if nl_prompt and nl_prompt.strip():
            prompt += f"\n\nã€Additional User Requirementsã€‘\n{nl_prompt}\n"
        
        return prompt
    
    
    def _apply_cell_fixes(self, original_rows: List[TableRow], 
                         fixes_data: List[Dict[str, Any]],
                         doc_source: str,
                         table_name: str = None) -> tuple[List[TableRow], List]:
        """åœ¨cellçº§åˆ«åº”ç”¨ä¿®å¤"""
        rows_map = {row.tuple_id: row for row in original_rows}
        
        applied_count = 0
        skipped_count = 0
        fix_records = []
        
        for fix_data in fixes_data:
            tuple_id = fix_data.get('tuple_id', '')
            field = fix_data.get('field', '')
            new_value = fix_data.get('new_value')
            
            if new_value is None or new_value == 'null':
                skipped_count += 1
                continue
            
            if tuple_id not in rows_map:
                skipped_count += 1
                continue
            
            row = rows_map[tuple_id]
            old_value = None
            
            if field in row.cells:
                old_value = row.cells[field].value
                row.cells[field].value = new_value
                if hasattr(row.cells[field], 'evidences'):
                    row.cells[field].evidences.append(f"[Warm Start Fix] {doc_source}")
            else:
                old_value = None
                row.cells[field] = CellData(
                    value=new_value,
                    evidences=[f"[Warm Start Fix] {doc_source}"]
                )
            
            fix_record = Fix(
                id=IdGenerator.generate_fix_id(
                    table=table_name or 'unknown',
                    tuple_id=tuple_id,
                    attr=field,
                    fix_type='warm_start_extraction',
                    old_value=old_value
                ),
                table=table_name or 'unknown',
                tuple_id=tuple_id,
                attr=field,
                old=old_value,
                new=new_value,
                fix_type='warm_start_extraction',
                applied_by='BaseExtractor',
                timestamp=datetime.now().isoformat(),
                fix_success=True,
                failure_reason=''
            )
            fix_records.append(fix_record)
            
            applied_count += 1
        
        self.logger.info(f"âœ… åº”ç”¨ä¿®å¤ï¼š{applied_count} ä¸ªæˆåŠŸï¼Œ{skipped_count} ä¸ªè·³è¿‡")
        
        return list(rows_map.values()), fix_records
    
    
    def _apply_document_filter(self, text_contents: List[str], document_names: List[str], 
                               table_def: Dict[str, Any]) -> tuple:
        """æ ¹æ®è¡¨å®šä¹‰ä¸­çš„æ–‡æ¡£è¿‡æ»¤é…ç½®ï¼Œè¿‡æ»¤æ–‡æ¡£åˆ—è¡¨
        
        Args:
            text_contents: æ–‡æ¡£å†…å®¹åˆ—è¡¨
            document_names: æ–‡æ¡£åç§°åˆ—è¡¨
            table_def: è¡¨å®šä¹‰
            
        Returns:
            (è¿‡æ»¤åçš„text_contents, è¿‡æ»¤åçš„document_names)
        """
        doc_filter = None
        
        if 'relation_extraction' in table_def:
            relation_config = table_def['relation_extraction']
            if isinstance(relation_config, dict) and 'document_filter' in relation_config:
                doc_filter = relation_config['document_filter']
        
        if not doc_filter and 'document_filter' in table_def:
            doc_filter = table_def['document_filter']
        
        if not doc_filter:
            return text_contents, document_names
        
        mode = doc_filter.get('mode', 'include')  # include æˆ– exclude
        patterns = doc_filter.get('patterns', [])
        
        if not patterns:
            return text_contents, document_names
        
        self.logger.info(f"ğŸ“‹ åº”ç”¨æ–‡æ¡£è¿‡æ»¤ï¼šmode={mode}, patterns={patterns}")
        self.logger.info(f"   è¾“å…¥æ–‡æ¡£æ•°é‡: text_contents={len(text_contents)}, document_names={len(document_names)}")
        
        filtered_contents = []
        filtered_names = []
        
        for i, doc_name in enumerate(document_names):
            matches = any(pattern in doc_name for pattern in patterns)
            
            self.logger.debug(f"  æ–‡æ¡£: {doc_name}, åŒ¹é…: {matches}")
            for pattern in patterns:
                self.logger.debug(f"    æ£€æŸ¥pattern '{pattern}' in '{doc_name}': {pattern in doc_name}")
            
            should_keep = (mode == 'include' and matches) or (mode == 'exclude' and not matches)
            
            if should_keep:
                if i < len(text_contents):
                    filtered_contents.append(text_contents[i])
                    filtered_names.append(doc_name)
                    self.logger.info(f"  âœ… ä¿ç•™æ–‡æ¡£: {doc_name}")
                else:
                    self.logger.warning(f"  âš ï¸ æ–‡æ¡£ç´¢å¼•è¶Šç•Œ: i={i}, len(text_contents)={len(text_contents)}")
            else:
                self.logger.info(f"  âŠ— è¿‡æ»¤æ‰æ–‡æ¡£: {doc_name}")
        
        self.logger.info(f"ğŸ“‹ æ–‡æ¡£è¿‡æ»¤å®Œæˆï¼š{len(document_names)} -> {len(filtered_names)} ä¸ªæ–‡æ¡£")
        
        return filtered_contents, filtered_names
    
    def supports_format(self, file_path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒæŒ‡å®šæ ¼å¼çš„æ–‡ä»¶"""
        supported_extensions = {'.txt', '.md', '.csv', '.json'}
        file_ext = Path(file_path).suffix.lower()
        return file_ext in supported_extensions
